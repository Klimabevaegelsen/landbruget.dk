This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
dataflow/
  setup.py
scripts/
  load_property_owners_to_bq.py
  sync_agricultural_fields.py
  sync_bnbo_status.py
  sync_cadastral.py
  sync_chr_data.py
  sync_crops.py
  sync_herd_data.py
  sync_property_owners.py
  sync_water_projects.py
  sync_wetlands.py
  test_vetstat.py
src/
  sources/
    parsers/
      __init__.py
      agricultural_fields.py
      antibiotics.py
      bnbo_status.py
      cadastral.py
      chr_data.py
      chr_species.py
      herd_data.py
      property_owners.py
      water_projects.py
      wetlands.py
    static/
      crops/
        parser.py
      fertilizer/
        readme.md
      pesticides/
        parser.py
        README.md
      subsidies/
        readme.me
      wetlands/
        parser.py
    utils/
      geometry_validator.py
    base.py
  config.py
  main.py
tests/
  test_cadastral_db.py
  test_cadastral_sync.py
  test_cadastral.py
  test_db_connection.py
  test_herd_data.py
  test_water_projects_parser.py
  test_water_projects.py
  test_xml_structure.py
.dockerignore
cloudbuild.yaml
conftest.py
Dockerfile
Dockerfile.processing
Dockerfile.sync
pyproject.toml
README.md
requirements.txt
sync_app.py

================================================================
Files
================================================================

================
File: dataflow/setup.py
================
from setuptools import setup, find_packages

setup(
    name='landbrugsdata-processing',
    version='0.1.0',
    install_requires=[
        'apache-beam[gcp]',
        'geopandas',
        'pandas',
        'shapely',
    ],
    packages=find_packages(),
)

================
File: scripts/load_property_owners_to_bq.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
import json
import tempfile
from typing import Optional
from google.cloud import storage, bigquery

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

async def load_to_bigquery() -> Optional[int]:
    """Load latest property owners file from GCS to BigQuery"""
    try:
        # Initialize clients
        storage_client = storage.Client()
        bq_client = bigquery.Client()
        
        # Get latest file from GCS
        bucket = storage_client.bucket('landbrugsdata-raw-data')
        blobs = list(bucket.list_blobs(prefix='raw/property_owners_'))
        if not blobs:
            raise ValueError("No property owners files found in GCS")
            
        latest_blob = max(blobs, key=lambda b: b.time_created)
        gcs_uri = f"gs://{bucket.name}/{latest_blob.name}"
        logger.info(f"Found latest file: {latest_blob.name}")
        
        # Download a small sample to check structure
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.ndjson') as temp_file:
            sample_data = latest_blob.download_as_string(start=0, end=10000).decode('utf-8')
            sample_lines = sample_data.splitlines()[:5]  # Take first 5 lines
            
            logger.info("Sample data structure:")
            for line in sample_lines:
                data = json.loads(line)
                logger.info(f"Fields: {list(data.keys())}")
                break  # Just show first line's structure
        
        # Load to BigQuery
        logger.info("Loading to BigQuery...")
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
            autodetect=True,
            max_bad_records=1000,  # Allow more errors
            ignore_unknown_values=True  # Skip unknown fields
        )
        
        table_ref = "land_data.property_owners_raw"
        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_ref,
            job_config=job_config
        )
        
        # Wait and log any errors
        job = load_job.result()
        if job.errors:
            for error in job.errors:
                logger.error(f"Load error: {error}")
        
        # Get number of rows loaded
        table = bq_client.get_table(table_ref)
        logger.info(f"Loaded {table.num_rows:,} rows to {table_ref}")
        return table.num_rows
        
    except Exception as e:
        logger.error(f"Error loading to BigQuery: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(load_to_bigquery())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_agricultural_fields.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.agricultural_fields import AgriculturalFields
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync agricultural fields data to Cloud Storage"""
    load_dotenv()
    try:
        agricultural_fields = AgriculturalFields(SOURCES["agricultural_fields"])
        total_synced = await agricultural_fields.sync()
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_bnbo_status.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.bnbo_status import BNBOStatus
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync BNBO status data to Cloud Storage"""
    load_dotenv()
    try:
        bnbo_status = BNBOStatus(SOURCES["bnbo_status"])
        total_synced = await bnbo_status.sync()
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_cadastral.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.cadastral import Cadastral
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync cadastral data to Cloud Storage"""
    load_dotenv()
    try:
        cadastral = Cadastral(SOURCES["cadastral"])
        total_synced = await cadastral.sync()
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_chr_data.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv
import traceback

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.chr_data import CHRDataParser
from src.sources.parsers.chr_species import CHRSpeciesParser
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

def handle_signal(signum, frame):
    """Handle interrupt signals with detailed logging."""
    signal_name = signal.Signals(signum).name
    logger.warning(f"Received signal {signum} ({signal_name})")
    logger.warning("Stack trace at point of interrupt:")
    for line in traceback.format_stack():
        logger.warning(line.strip())
    logger.info("Starting graceful shutdown...")
    sys.exit(0)

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync CHR data to Cloud Storage"""
    load_dotenv()
    try:
        logger.info("Starting CHR data sync process...")
        
        # Register signal handlers with detailed logging
        for sig in [signal.SIGINT, signal.SIGTERM]:
            signal.signal(sig, handle_signal)
            logger.info(f"Registered handler for signal {sig} ({signal.Signals(sig).name})")
        
        logger.info("Fetching species data...")
        species_parser = CHRSpeciesParser(SOURCES["chr_data"])
        species_data = await species_parser.fetch()
        
        if species_data.empty:
            logger.error("Failed to fetch species data. Exiting.")
            return
            
        logger.info(f"Successfully fetched species data with {len(species_data)} records")
        
        logger.info("Initializing CHR data parser...")
        chr_data = CHRDataParser(SOURCES["chr_data"])
        
        # Process each species code
        unique_species = species_data['species_code'].unique()
        total_species = len(unique_species)
        logger.info(f"Found {total_species} unique species codes to process")
        
        # Track statistics
        processed_species = 0
        successful_species = 0
        failed_species = 0
        total_records = 0
        
        for i, species_code in enumerate(unique_species, 1):
            try:
                logger.info(f"Processing species code: {species_code} ({i}/{total_species})")
                result = await chr_data.process_species(species_code)
                if result is not None:
                    successful_species += 1
                    records = len(result)
                    total_records += records
                    logger.info(f"Successfully processed species code {species_code} ({records} records)")
                    logger.info(f"Progress: {i}/{total_species} species processed ({(i/total_species)*100:.1f}%)")
                else:
                    failed_species += 1
                    logger.warning(f"No results for species code {species_code}")
            except Exception as e:
                failed_species += 1
                logger.error(f"Error processing species code {species_code}: {str(e)}", exc_info=True)
                continue
            processed_species += 1
        
        # Log final statistics
        logger.info("\nSync process completed. Final statistics:")
        logger.info(f"Total species processed: {processed_species}/{total_species}")
        logger.info(f"Successful species: {successful_species}")
        logger.info(f"Failed species: {failed_species}")
        logger.info(f"Total records processed: {total_records:,}")
        
    except Exception as e:
        logger.error(f"Error during sync process: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_crops.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.static.crops.parser import CropCodes
from src.config import SOURCES

async def main() -> Optional[int]:
    """Sync crop codes data to Cloud Storage"""
    load_dotenv()
    try:
        crops = CropCodes(SOURCES["crops"])
        total_synced = await crops.sync()
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_herd_data.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.herd_data import HerdDataParser
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync herd data to Cloud Storage"""
    load_dotenv()
    try:
        herd_data = HerdDataParser(SOURCES["herd_data"])
        total_synced = await herd_data.sync()  # Now awaiting the async call
        if total_synced is not None:
            logger.info(f"Total records synced: {total_synced:,}")
        else:
            logger.warning("No records were synced")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_property_owners.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.property_owners import PropertyOwnersParser
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync property owners data to Cloud Storage"""
    load_dotenv()
    try:
        property_owners = PropertyOwnersParser(SOURCES["property_owners"])
        total_synced = await property_owners.sync()
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_water_projects.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.water_projects import WaterProjects
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync water projects data to Cloud Storage"""
    load_dotenv()
    try:
        water_projects = WaterProjects(SOURCES["water_projects"])
        total_synced = await water_projects.sync()  # No conn parameter
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/sync_wetlands.py
================
import asyncio
import os
from pathlib import Path
import sys
import logging
from typing import Optional
import signal
from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.wetlands import Wetlands
from src.config import SOURCES

shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    logger.info(f"Received signal {signum}. Starting graceful shutdown...")
    shutdown.set()

signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def main() -> Optional[int]:
    """Sync wetlands data to Cloud Storage"""
    load_dotenv()
    try:
        wetlands = Wetlands(SOURCES["wetlands"])
        total_synced = await wetlands.sync()  # No conn parameter
        logger.info(f"Total records synced: {total_synced:,}")
        return total_synced
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}")
        sys.exit(1)

================
File: scripts/test_vetstat.py
================
import asyncio
import os
import sys
from pathlib import Path
import logging
from datetime import date
from dateutil.relativedelta import relativedelta
import pandas as pd

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Add backend directory to path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.antibiotics import VetStatAntibioticsParser
from src.config import SOURCES

async def main():
    try:
        # Calculate period
        today = date.today()
        last_day_prev_month = today.replace(day=1) - relativedelta(days=1)
        first_day = (last_day_prev_month - relativedelta(months=11)).replace(day=1)
        
        logger.info(f"Testing VetStat parser for period: {first_day} to {last_day_prev_month}")
        
        # Initialize parser
        parser = VetStatAntibioticsParser(SOURCES["vetstat"])
        
        # Test with a single CHR number first
        chr_number = 16218  # Test CHR number
        results = parser.process_soap_request(
            chr_number=chr_number,
            periode_fra=first_day.isoformat(),
            periode_til=last_day_prev_month.isoformat()
        )
        
        if results:
            logger.info("Response received and parsed:")
            df = pd.DataFrame(results)
            logger.info("\nData summary:")
            logger.info(f"Total records: {len(df)}")
            logger.info("\nColumns:")
            for col in df.columns:
                non_null = df[col].count()
                logger.info(f"{col}: {non_null} non-null values")
            
            logger.info("\nSample data:")
            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', None)
            logger.info("\n" + str(df.head()))
        else:
            logger.error("No response received")
            
    except Exception as e:
        logger.error(f"Error during test: {str(e)}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())

================
File: src/sources/parsers/__init__.py
================
from .wetlands import Wetlands
from .cadastral import Cadastral
from .water_projects import WaterProjects
from .agricultural_fields import AgriculturalFields
from .chr_data import CHRDataParser
from .bnbo_status import BNBOStatus
from .antibiotics import VetStatAntibioticsParser

def get_source_handler(source_id: str, config: dict):
    """Get appropriate source handler based on source ID"""
    handlers = {
        'water_projects': WaterProjects,
        'wetlands': Wetlands,
        'cadastral': Cadastral,
        'agricultural_fields': AgriculturalFields,
        'chr_data': CHRDataParser,
        'bnbo_status': BNBOStatus,
        'antibiotics': VetStatAntibioticsParser
    }
    
    handler_class = handlers.get(source_id)
    if handler_class:
        return handler_class(config)
    return None

================
File: src/sources/parsers/agricultural_fields.py
================
import logging
import aiohttp
import geopandas as gpd
import asyncio
import xml.etree.ElementTree as ET
from ..base import GeospatialSource
import time
import os
import ssl
from ..utils.geometry_validator import validate_and_transform_geometries
import pandas as pd

logger = logging.getLogger(__name__)

class AgriculturalFields(GeospatialSource):
    """Danish Agricultural Fields WFS parser"""
    
    source_id = "agricultural_fields"
    
    COLUMN_MAPPING = {
        'Marknr': 'field_id',
        'IMK_areal': 'area_ha',
        'Journalnr': 'journal_number',
        'CVR': 'cvr_number',
        'Afgkode': 'crop_code',
        'Afgroede': 'crop_type',
        'GB': 'organic_farming',
        'GBanmeldt': 'reported_area_ha',
        'Markblok': 'block_id',
        'MB_NR': 'block_id',
        'BLOKAREAL': 'block_area_ha',
        'MARKBLOKTY': 'block_type'
    }
    
    def __init__(self, config):
        super().__init__(config)
        self.batch_size = 2000
        self.max_concurrent = 5
        self.storage_batch_size = 10000
        self.max_retries = 3
        
        self.timeout_config = aiohttp.ClientTimeout(
            total=1200,
            connect=60,
            sock_read=540
        )
        
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        self.ssl_context.options |= 0x4
        
        self.request_semaphore = asyncio.Semaphore(self.max_concurrent)
        self.start_time = None
        self.features_processed = 0
        self.bucket = self.storage_client.bucket(config['bucket'])
        logger.info(f"Initialized with batch_size={self.batch_size}, "
                   f"max_concurrent={self.max_concurrent}, "
                   f"storage_batch_size={self.storage_batch_size}")

    async def _get_total_count(self, session, endpoint):
        """Get total number of features for a specific endpoint"""
        params = {
            'f': 'json',
            'where': '1=1',
            'returnCountOnly': 'true'
        }
        
        try:
            url = self.config['urls'][endpoint]
            logger.info(f"Fetching total count from {url}")
            async with session.get(url, params=params, ssl=self.ssl_context) as response:
                if response.status == 200:
                    data = await response.json()
                    total = data.get('count', 0)
                    logger.info(f"Total {endpoint} features available: {total:,}")
                    return total
                else:
                    logger.error(f"Error getting count for {endpoint}: {response.status}")
                    response_text = await response.text()
                    logger.error(f"Response: {response_text[:500]}...")
                    return 0
        except Exception as e:
            logger.error(f"Error getting total count for {endpoint}: {str(e)}", exc_info=True)
            return 0

    async def _fetch_chunk(self, session, endpoint, start_index, retry_count=0):
        """Fetch a chunk of features with retry logic"""
        params = {
            'f': 'json',
            'where': '1=1',
            'returnGeometry': 'true',
            'outFields': '*',
            'resultOffset': str(start_index),
            'resultRecordCount': str(self.batch_size)
        }
        
        async with self.request_semaphore:
            try:
                chunk_start = time.time()
                url = self.config['urls'][endpoint]
                logger.debug(f"Fetching {endpoint} from URL: {url} (attempt {retry_count + 1})")
                async with session.get(url, params=params, ssl=self.ssl_context) as response:
                    if response.status == 200:
                        data = await response.json()
                        features = data.get('features', [])
                        
                        if not features:
                            logger.warning(f"No features returned at index {start_index}")
                            return None
                            
                        logger.debug(f"Creating GeoDataFrame from {len(features)} features")
                        
                        # Convert ArcGIS REST API features to GeoJSON format
                        geojson_features = []
                        for feature in features:
                            geojson_feature = {
                                'type': 'Feature',
                                'properties': feature['attributes'],
                                'geometry': {
                                    'type': 'Polygon',
                                    'coordinates': feature['geometry']['rings']
                                }
                            }
                            geojson_features.append(geojson_feature)
                        
                        gdf = gpd.GeoDataFrame.from_features(geojson_features)
                        gdf = gdf.rename(columns=self.COLUMN_MAPPING)
                        
                        # Set the CRS to EPSG:25832 (the coordinate system used by the API)
                        gdf.set_crs(epsg=25832, inplace=True)
                        
                        chunk_time = time.time() - chunk_start
                        logger.debug(f"Processed {len(features)} features in {chunk_time:.2f}s")
                        return gdf
                    else:
                        logger.error(f"Error response {response.status} at index {start_index}")
                        response_text = await response.text()
                        logger.error(f"Response: {response_text[:500]}...")
                        
                        if response.status >= 500 and retry_count < self.max_retries:
                            await asyncio.sleep(2 ** retry_count)
                            return await self._fetch_chunk(session, endpoint, start_index, retry_count + 1)
                        return None
                        
            except ssl.SSLError as e:
                logger.error(f"SSL Error at index {start_index}: {str(e)}")
                if retry_count < self.max_retries:
                    await asyncio.sleep(2 ** retry_count)
                    return await self._fetch_chunk(session, endpoint, start_index, retry_count + 1)
                return None
            except Exception as e:
                logger.error(f"Error fetching chunk at index {start_index}: {str(e)}")
                if retry_count < self.max_retries:
                    await asyncio.sleep(2 ** retry_count)
                    return await self._fetch_chunk(session, endpoint, start_index, retry_count + 1)
                return None

    async def sync(self):
        """Sync agricultural fields and blocks data"""
        logger.info("Starting agricultural data sync...")
        self.start_time = time.time()
        self.features_processed = 0
        total_processed = 0

        try:
            conn = aiohttp.TCPConnector(limit=self.max_concurrent, ssl=self.ssl_context)
            async with aiohttp.ClientSession(timeout=self.timeout_config, connector=conn) as session:
                # Process both fields and blocks
                for endpoint in ['fields', 'blocks']:
                    self.features_processed = 0
                    self.is_sync_complete = False
                    
                    total_features = await self._get_total_count(session, endpoint)
                    logger.info(f"Found {total_features:,} total {endpoint}")
                    
                    features_batch = []
                    
                    for start_index in range(0, total_features, self.batch_size):
                        chunk = await self._fetch_chunk(session, endpoint, start_index)
                        if chunk is not None:
                            features_batch.extend(chunk.to_dict('records'))
                            self.features_processed += len(chunk)
                            
                            is_last_batch = (start_index + self.batch_size) >= total_features
                            if len(features_batch) >= self.storage_batch_size or is_last_batch:
                                logger.info(f"Writing batch of {len(features_batch):,} {endpoint}")
                                self.is_sync_complete = is_last_batch
                                await self.write_to_storage(features_batch, f'agricultural_{endpoint}')
                                features_batch = []
                                
                                elapsed = time.time() - self.start_time
                                speed = self.features_processed / elapsed
                                remaining = total_features - self.features_processed
                                eta_minutes = (remaining / speed) / 60 if speed > 0 else 0
                                
                                logger.info(
                                    f"Progress: {self.features_processed:,}/{total_features:,} "
                                    f"({speed:.1f} features/second, ETA: {eta_minutes:.1f} minutes)"
                                )
                    
                    total_processed += self.features_processed
                    logger.info(f"Completed {endpoint} sync. Processed: {self.features_processed:,}")
                
                logger.info(f"All syncs completed. Total processed: {total_processed:,}")
                return total_processed
                
        except Exception as e:
            logger.error(f"Error in sync: {str(e)}")
            raise

    async def fetch(self):
        return await self.sync()

    async def write_to_storage(self, features, dataset):
        """Write features to GeoParquet in Cloud Storage"""
        if not features:
            return
        
        try:
            # Create GeoDataFrame from ArcGIS features
            gdf = gpd.GeoDataFrame(features, crs="EPSG:25832")
            gdf.columns = [col.replace('.', '_').replace('(', '_').replace(')', '_') for col in gdf.columns]
            
            # Validate and transform geometries
            gdf = validate_and_transform_geometries(gdf, dataset)
            
            # Handle working/final files
            temp_working = f"/tmp/{dataset}_working.parquet"
            working_blob = self.bucket.blob(f'raw/{dataset}/working.parquet')
            
            if working_blob.exists():
                working_blob.download_to_filename(temp_working)
                existing_gdf = gpd.read_parquet(temp_working)
                logger.info(f"Appending {len(gdf):,} features to existing {len(existing_gdf):,}")
                combined_gdf = pd.concat([existing_gdf, gdf], ignore_index=True)
            else:
                combined_gdf = gdf
                
            # Write working file
            combined_gdf.to_parquet(temp_working)
            working_blob.upload_from_filename(temp_working)
            logger.info(f"Updated working file now has {len(combined_gdf):,} features")
            
            # If sync complete, create final file
            if self.is_sync_complete:
                logger.info(f"Sync complete - writing final file with {len(combined_gdf):,} features")
                final_blob = self.bucket.blob(f'raw/{dataset}/current.parquet')
                final_blob.upload_from_filename(temp_working)
                working_blob.delete()
            
            # Cleanup
            os.remove(temp_working)
            
        except Exception as e:
            logger.error(f"Error writing to storage: {str(e)}")
            raise

================
File: src/sources/parsers/antibiotics.py
================
"""Parser for VetStat antibiotic data."""
import base64
import datetime
import hashlib
import secrets
import requests
import uuid
import logging
from typing import Dict, List, Any, Optional
from datetime import timedelta
from lxml import etree
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import Encoding
from cryptography.hazmat.primitives.serialization.pkcs12 import load_key_and_certificates
from copy import deepcopy
import pandas as pd
from google.cloud import secretmanager
from ..base import BaseSource
from .chr_species import CHRSpeciesParser
from dateutil.relativedelta import relativedelta

logger = logging.getLogger(__name__)

namespaces = {
    'soapenv': 'http://schemas.xmlsoap.org/soap/envelope/',
    'wsse': 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd',
    'wsu': 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd',
    'ds': 'http://www.w3.org/2000/09/xmldsig#',
    'ec': 'http://www.w3.org/2001/10/xml-exc-c14n#',
    'eks': 'http://vetstat.fvst.dk/ekstern',
    'glr': 'http://www.logica.com/glrchr'
}

class VetStatAntibioticsParser(BaseSource):
    """Parser for VetStat antibiotic data."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # Get credentials from Google Secret Manager
        client = secretmanager.SecretManagerServiceClient()
        project_id = "landbrugsdata-1"
        
        def get_secret(secret_id: str) -> str:
            name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
            response = client.access_secret_version(request={"name": name})
            return response.payload.data.decode("UTF-8") if secret_id != "vetstat-certificate" else response.payload.data
        
        # Get credentials
        self.username = get_secret('fvm_username')
        self.password = get_secret('fvm_password')
        self.p12_data = get_secret('vetstat-certificate')
        self.p12_password = get_secret('vetstat-certificate-password')
        
        logger.info("Initialized VetStat antibiotics parser")

    @property
    def source_id(self) -> str:
        """Return the source ID."""
        return "antibiotics"

    def compute_digest(self, element, inclusive_prefixes):
        """Canonicalize (C14N) the element and compute its SHA-256 digest in Base64."""
        c14n_bytes = etree.tostring(
            element,
            method="c14n",
            exclusive=True,
            inclusive_ns_prefixes=inclusive_prefixes,
            with_comments=False
        )
        sha256_hash = hashlib.sha256(c14n_bytes).digest()
        return base64.b64encode(sha256_hash).decode()

    def get_element_prefixes(self, element_type):
        """Get the appropriate namespace prefixes based on element type."""
        prefix_mappings = {
            'Body': ["ds", "ec", "eks", "glr", "wsse"],
            'Timestamp': ["wsse", "ds", "ec", "eks", "glr", "soapenv", "wsse"],
            'UsernameToken': ["ds", "ec", "eks", "glr", "soapenv", "wsse"],
            'BinarySecurityToken': [],
            'SecurityTokenReference': ["wsse", "ds", "ec", "eks", "glr", "soapenv"],
            'Signature': ["ds", "ec", "eks", "glr", "soapenv", "wsse", "wsu"],
            'SignedInfo': ["ds", "ec", "eks", "glr", "soapenv", "wsse", "wsu"],
        }
        return prefix_mappings.get(element_type, ["ds", "ec", "eks", "glr", "wsse"])

    def generate_uuid_id(self, prefix):
        """Generate a UUID-based ID with a specific prefix."""
        return f"{prefix}{uuid.uuid4().hex.upper()}"

    def replace_all_ids(self, root):
        """Replace all IDs in the document with new UUIDs and track the replacements."""
        id_mappings = {}
        
        # Find all elements with wsu:Id or Id attributes
        for element in root.xpath("//*[@wsu:Id or @Id]", namespaces=namespaces):
            # Handle wsu:Id
            wsu_id = element.get(f"{{{namespaces['wsu']}}}Id")
            if wsu_id:
                prefix = wsu_id.split('-')[0] + '-'
                new_id = self.generate_uuid_id(prefix)
                id_mappings[wsu_id] = new_id
                element.set(f"{{{namespaces['wsu']}}}Id", new_id)
            
            # Handle plain Id
            plain_id = element.get('Id')
            if plain_id:
                prefix = plain_id.split('-')[0] + '-'
                new_id = self.generate_uuid_id(prefix)
                id_mappings[plain_id] = new_id
                element.set('Id', new_id)
        
        # Update all references
        for ref in root.xpath("//ds:Reference|//wsse:Reference", namespaces=namespaces):
            uri = ref.get('URI')
            if uri and uri.lstrip('#') in id_mappings:
                ref.set('URI', f"#{id_mappings[uri.lstrip('#')]}")
        
        return id_mappings

    def update_security_elements(self, root, username, password, certificate):
        """Update all security-related elements in the XML."""
        now_utc = datetime.datetime.utcnow()
        expires_utc = now_utc + timedelta(hours=1)
        
        # Update BinarySecurityToken
        binary_token = root.find(".//wsse:BinarySecurityToken", namespaces)
        if binary_token is not None:
            binary_token.text = base64.b64encode(certificate.public_bytes(Encoding.DER)).decode()

        # Update Username and Password
        username_token = root.find(".//wsse:UsernameToken", namespaces)
        if username_token is not None:
            user_el = username_token.find("./wsse:Username", namespaces)
            pass_el = username_token.find("./wsse:Password", namespaces)
            if user_el is not None:
                user_el.text = username
            if pass_el is not None:
                pass_el.text = password

        # Update Nonce
        nonce_el = root.find(".//wsse:Nonce", namespaces)
        if nonce_el is not None:
            nonce_el.text = base64.b64encode(secrets.token_bytes(16)).decode()

        # Update Timestamps
        created_str = now_utc.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"
        expires_str = expires_utc.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"

        for created_el in root.xpath("//wsu:Created", namespaces=namespaces):
            created_el.text = created_str
        for expires_el in root.xpath("//wsu:Expires", namespaces=namespaces):
            expires_el.text = expires_str

    def update_references_and_digests(self, root):
        """Update all reference URIs and their corresponding digests."""
        references = root.xpath("//ds:Reference", namespaces=namespaces)
        
        for ref in references:
            uri = ref.get('URI')
            if uri:
                id_value = uri.lstrip('#')
                referenced_element = root.xpath(f"//*[@wsu:Id='{id_value}' or @Id='{id_value}']", namespaces=namespaces)
                
                if referenced_element:
                    element = referenced_element[0]
                    element_type = element.tag.split('}')[-1]
                    prefixes = self.get_element_prefixes(element_type)
                    new_digest = self.compute_digest(element, prefixes)
                    
                    digest_value = ref.find('./ds:DigestValue', namespaces=namespaces)
                    if digest_value is not None:
                        digest_value.text = new_digest

    def sign_document(self, root, private_key):
        """Sign the document with the provided private key."""
        signed_info = root.find(".//ds:SignedInfo", namespaces)
        if signed_info is not None:
            signed_info_c14n = etree.tostring(
                signed_info,
                method="c14n",
                exclusive=True,
                inclusive_ns_prefixes=["ds", "ec", "eks", "glr", "soapenv", "wsse", "wsu"],
                with_comments=False
            )
            
            signature = private_key.sign(
                signed_info_c14n,
                padding.PKCS1v15(),
                hashes.SHA1()
            )
            
            signature_value = root.find(".//ds:SignatureValue", namespaces=namespaces)
            if signature_value is not None:
                signature_value.text = base64.b64encode(signature).decode()

    def create_soap_envelope(self, username, password, certificate, chr_number, periode_fra, periode_til, species_code):
        """Create SOAP envelope with exact whitespace control."""
        
        # Generate IDs that will be referenced multiple times
        binary_token_id = self.generate_uuid_id("X509-")
        username_token_id = self.generate_uuid_id("UsernameToken-")
        timestamp_id = self.generate_uuid_id("TS-")
        signature_id = self.generate_uuid_id("SIG-")
        body_id = f"id-{uuid.uuid4().hex.upper()}"
        key_info_id = self.generate_uuid_id("KI-")
        str_id = self.generate_uuid_id("STR-")
        
        # Current timestamp and expiry
        now = datetime.datetime.utcnow()
        expires = now + timedelta(hours=1)
        created_str = now.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"
        expires_str = expires.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"
        
        # Store the XML parts in a list for better control
        xml_parts = []
        
        # Start building the XML with exact whitespace
        xml_parts.extend([
            '<soapenv:Envelope xmlns:ds="http://www.w3.org/2000/09/xmldsig#" ',
            'xmlns:ec="http://www.w3.org/2001/10/xml-exc-c14n#" ',
            'xmlns:eks="http://vetstat.fvst.dk/ekstern" ',
            'xmlns:glr="http://www.logica.com/glrchr" ',
            'xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" ',
            'xmlns:wsse="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd" ',
            'xmlns:wsu="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd">',
            '\n  <soapenv:Header>',
            '\n    <wsse:Security>',
            '\n      <wsse:BinarySecurityToken EncodingType="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary" ',
            'ValueType="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3" ',
            f'wsu:Id="{binary_token_id}">',
            base64.b64encode(certificate.public_bytes(Encoding.DER)).decode(),
            '</wsse:BinarySecurityToken>',
            f'\n      <wsse:UsernameToken wsu:Id="{username_token_id}">',
            f'\n        <wsse:Username>{username}</wsse:Username>',
            '\n        <wsse:Password Type="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0#PasswordText">',
            f'{password}</wsse:Password>',
            '\n        <wsse:Nonce EncodingType="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary">',
            f'{base64.b64encode(secrets.token_bytes(16)).decode()}</wsse:Nonce>',
            f'\n        <wsu:Created>{created_str}</wsu:Created>',
            '\n      </wsse:UsernameToken>',
            f'\n      <wsu:Timestamp wsu:Id="{timestamp_id}">',
            f'\n        <wsu:Created>{created_str}</wsu:Created>',
            f'\n        <wsu:Expires>{expires_str}</wsu:Expires>',
            '\n      </wsu:Timestamp>',
            f'\n      <ds:Signature Id="{signature_id}">',
            '\n        <ds:SignedInfo>',
            '\n          <ds:CanonicalizationMethod Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#">',
            '\n            <ec:InclusiveNamespaces PrefixList="ds ec eks glr soapenv wsse wsu"/>',
            '\n          </ds:CanonicalizationMethod>',
            '\n          <ds:SignatureMethod Algorithm="http://www.w3.org/2000/09/xmldsig#rsa-sha1"/>',
            f'\n          <ds:Reference URI="#{body_id}">',
            '\n            <ds:Transforms>',
            '\n              <ds:Transform Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#">',
            '\n                <ec:InclusiveNamespaces PrefixList="ds ec eks glr wsse"/>',
            '\n              </ds:Transform>',
            '\n            </ds:Transforms>',
            '\n            <ds:DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256"/>',
            '\n            <ds:DigestValue></ds:DigestValue>',
            '\n          </ds:Reference>',
            f'\n          <ds:Reference URI="#{timestamp_id}">',
            '\n            <ds:Transforms>',
            '\n              <ds:Transform Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#">',
            '\n                <ec:InclusiveNamespaces PrefixList="wsse ds ec eks glr soapenv wsse"/>',
            '\n              </ds:Transform>',
            '\n            </ds:Transforms>',
            '\n            <ds:DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256"/>',
            '\n            <ds:DigestValue></ds:DigestValue>',
            '\n          </ds:Reference>',
            f'\n          <ds:Reference URI="#{username_token_id}">',
            '\n            <ds:Transforms>',
            '\n              <ds:Transform Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#">',
            '\n                <ec:InclusiveNamespaces PrefixList="ds ec eks glr soapenv wsse"/>',
            '\n              </ds:Transform>',
            '\n            </ds:Transforms>',
            '\n            <ds:DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256"/>',
            '\n            <ds:DigestValue></ds:DigestValue>',
            '\n          </ds:Reference>',
            f'\n          <ds:Reference URI="#{binary_token_id}">',
            '\n            <ds:Transforms>',
            '\n              <ds:Transform Algorithm="http://www.w3.org/2001/10/xml-exc-c14n#">',
            '\n                <ec:InclusiveNamespaces PrefixList=""/>',
            '\n              </ds:Transform>',
            '\n            </ds:Transforms>',
            '\n            <ds:DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256"/>',
            '\n            <ds:DigestValue></ds:DigestValue>',
            '\n          </ds:Reference>',
            '\n        </ds:SignedInfo>',
            '\n        <ds:SignatureValue></ds:SignatureValue>',
            f'\n        <ds:KeyInfo Id="{key_info_id}">',
            f'\n          <wsse:SecurityTokenReference wsu:Id="{str_id}">',
            f'\n            <wsse:Reference URI="#{binary_token_id}" ',
            'ValueType="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-x509-token-profile-1.0#X509v3"/>',
            '\n          </wsse:SecurityTokenReference>',
            '\n        </ds:KeyInfo>',
            '\n      </ds:Signature>',
            '\n    </wsse:Security>',
            '\n  </soapenv:Header>',
            f'\n  <soapenv:Body wsu:Id="{body_id}">',
            '\n    <eks:VetStat_CHRHentAntibiotikaForbrugRequest>',
            '\n      <glr:GLRCHRWSInfoInbound>',
            '\n        <glr:KlientId>?</glr:KlientId>',
            f'\n        <glr:BrugerNavn>{username}</glr:BrugerNavn>',
            '\n        <glr:SessionId>?</glr:SessionId>',
            '\n        <glr:IPAdresse>?</glr:IPAdresse>',
            '\n        <glr:TrackID>?</glr:TrackID>',
            '\n      </glr:GLRCHRWSInfoInbound>',
            '\n      <eks:Request>',
            f'\n        <glr:DyreArtKode>{species_code}</glr:DyreArtKode>',
            f'\n        <eks:PeriodeFra>{periode_fra}</eks:PeriodeFra>',
            f'\n        <eks:PeriodeTil>{periode_til}</eks:PeriodeTil>',
            f'\n        <eks:CHRNummer>{chr_number}</eks:CHRNummer>',
            '\n      </eks:Request>',
            '\n    </eks:VetStat_CHRHentAntibiotikaForbrugRequest>',
            '\n  </soapenv:Body>',
            '\n</soapenv:Envelope>'
        ])
        
        # Join all parts and parse the result
        xml_string = ''.join(xml_parts)
        return etree.fromstring(xml_string.encode('utf-8'))

    def process_soap_request(self, chr_number: int, periode_fra: str, periode_til: str, species_code: int) -> Optional[Dict[str, Any]]:
        """Process the SOAP request with new UUIDs for all IDs."""
        try:
            # Load certificate and private key
            private_key, certificate, _ = load_key_and_certificates(self.p12_data, self.p12_password.encode())
            
            # Create envelope from scratch
            root = self.create_soap_envelope(self.username, self.password, certificate, chr_number, periode_fra, periode_til, species_code)
            
            # Update references and digests
            self.update_references_and_digests(root)
            
            # Sign the document
            self.sign_document(root, private_key)
            
            # Convert to string
            modified_xml = etree.tostring(root, pretty_print=True, encoding='unicode')
            
            # Send request
            response = requests.post(
                "https://vetstat.fvst.dk/vetstat/services/external/CHRWS",
                data=modified_xml,
                headers={
                    "Content-Type": "text/xml;charset=UTF-8",
                    "SOAPAction": "http://vetstat.fvst.dk/chr/hentAntibiotikaforbrug"
                }
            )
            
            if response.status_code != 200:
                logger.error(f"Error response from VetStat API: {response.status_code} - {response.text}")
                return None
                
            # Parse response
            response_root = etree.fromstring(response.content)
            
            # Add VetStat namespace
            namespaces['ns2'] = 'http://vetstat.fvst.dk/ekstern'
            
            # Extract data from response
            data_elements = response_root.findall('.//ns2:Data', namespaces)
            if not data_elements:
                logger.warning(f"No data found in response for CHR {chr_number}")
                return None
            
            results = []
            for data in data_elements:
                result = {
                    'chr_number': chr_number,
                    'year': self.safe_int(data.findtext('.//ns2:Aar', namespaces=namespaces)),
                    'month': self.safe_int(data.findtext('.//ns2:Maaned', namespaces=namespaces)),
                    'species_code': self.safe_int(data.findtext('.//DyreArtKode', namespaces=namespaces)),
                    'species_text': self.safe_str(data.findtext('.//DyreArtTekst', namespaces=namespaces)),
                    'age_group_code': self.safe_int(data.findtext('.//ns2:Aldersgruppekode', namespaces=namespaces)),
                    'age_group': self.safe_str(data.findtext('.//ns2:Aldersgruppe', namespaces=namespaces)),
                    'rolling_9m_avg': self.safe_float(data.findtext('.//ns2:Rul9MdrGns', namespaces=namespaces)),
                    'rolling_12m_avg': self.safe_float(data.findtext('.//ns2:Rul12MdrGns', namespaces=namespaces)),
                    'cvr_number': self.safe_str(data.findtext('.//ns2:CVRNummer', namespaces=namespaces)),
                    'municipality_code': self.safe_int(data.findtext('.//ns2:Kommunenr', namespaces=namespaces)),
                    'municipality_name': self.safe_str(data.findtext('.//ns2:Kommunenavn', namespaces=namespaces)),
                    'region_code': self.safe_int(data.findtext('.//ns2:Regionsnr', namespaces=namespaces)),
                    'region_name': self.safe_str(data.findtext('.//ns2:Regionsnavn', namespaces=namespaces)),
                    'animal_days': self.safe_int(data.findtext('.//ns2:Dyredage', namespaces=namespaces)),
                    'animal_doses': self.safe_float(data.findtext('.//ns2:Dyredoser', namespaces=namespaces)),
                    'add_per_100_animals_per_day': self.safe_float(data.findtext('.//ns2:ADDPer100DyrPerDag', namespaces=namespaces)),
                    'threshold_value': self.safe_float(data.findtext('.//ns2:Graensevaerdi', namespaces=namespaces)),
                    'period_from': periode_fra,
                    'period_to': periode_til
                }
                results.append({k: v for k, v in result.items() if v is not None})
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing SOAP request for CHR {chr_number}: {str(e)}", exc_info=True)
            return None

    def safe_int(self, value: Any) -> Optional[int]:
        """Safely convert value to int, return None if not possible."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return int(val_str) if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    def safe_str(self, value: Any) -> Optional[str]:
        """Safely convert value to string, return None if empty."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return val_str if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    def safe_float(self, value: Any) -> Optional[float]:
        """Safely convert value to float, return None if not possible."""
        if value is None:
            return None
        try:
            val_str = str(value).strip().replace(',', '.')
            return float(val_str) if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    async def process_species(self, species_code: int) -> Optional[pd.DataFrame]:
        """Process antibiotic data for a given species code."""
        try:
            # Get CHR numbers for this species code
            logger.info(f"Getting CHR numbers for species code {species_code}")
            species_parser = CHRSpeciesParser(self.config)
            chr_numbers_df = await species_parser.get_chr_numbers_async(species_code)
            
            if chr_numbers_df.empty:
                logger.warning(f"No CHR numbers found for species code {species_code}")
                return None
            
            # Calculate period for last 12 completed months
            today = datetime.date.today()
            last_day_prev_month = today.replace(day=1) - timedelta(days=1)
            first_day = (last_day_prev_month - relativedelta(months=11)).replace(day=1)
            
            logger.info(f"Fetching data for period: {first_day} to {last_day_prev_month}")
            
            # Process each CHR number
            results = []
            for chr_number in chr_numbers_df['chr_number'].unique():
                try:
                    result = self.process_soap_request(
                        chr_number=chr_number,
                        periode_fra=first_day.isoformat(),
                        periode_til=last_day_prev_month.isoformat(),
                        species_code=species_code
                    )
                    if result:
                        results.append(result)
                        
                except Exception as e:
                    logger.error(f"Error processing CHR number {chr_number}: {str(e)}", exc_info=True)
                    continue
            
            if results:
                df = pd.DataFrame(results)
                await self.store(df, f'antibiotics_{species_code}')
                return df
            
            return None
            
        except Exception as e:
            logger.error(f"Error processing species code {species_code}: {str(e)}", exc_info=True)
            return None

    async def fetch(self) -> pd.DataFrame:
        """Fetch antibiotic data from VetStat."""
        try:
            # First get species data
            species_parser = CHRSpeciesParser(self.config)
            species_data = await species_parser.get_species_usage_combinations_async()
            
            if species_data.empty:
                logger.error("Failed to fetch species data")
                return pd.DataFrame()
            
            # Process each species code
            results = []
            for species_code in species_data['species_code'].unique():
                logger.info(f"Processing species code: {species_code}")
                result = await self.process_species(species_code)
                if result is not None:
                    results.append(result)
            
            # Combine all results
            if results:
                return pd.concat(results, ignore_index=True)
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Error during fetch: {str(e)}")
            return pd.DataFrame()

    def fetch_sync(self) -> pd.DataFrame:
        """Synchronous version of fetch."""
        import asyncio
        return asyncio.run(self.fetch())

    async def sync(self) -> Optional[int]:
        """Full sync process: fetch and store"""
        try:
            df = await self.fetch()
            if await self.store(df):
                return len(df)
            return None
        except Exception as e:
            logger.error(f"Sync failed for {self.source_id}: {str(e)}")
            return None

================
File: src/sources/parsers/bnbo_status.py
================
from pathlib import Path
import asyncio
import os
import xml.etree.ElementTree as ET
from datetime import datetime
import logging
import aiohttp
from shapely.geometry import Polygon, MultiPolygon
from shapely import wkt, difference
from shapely.ops import unary_union
import geopandas as gpd
import pandas as pd
from google.cloud import storage
import time
import backoff
from aiohttp import ClientError, ClientTimeout
import ssl

from ..base import GeospatialSource
from ..utils.geometry_validator import validate_and_transform_geometries

logger = logging.getLogger(__name__)

def clean_value(value):
    """Clean string values"""
    if not isinstance(value, str):
        return value
    value = value.strip()
    return value if value else None

class BNBOStatus(GeospatialSource):
    source_id = "bnbo_status"
    
    def __init__(self, config):
        super().__init__(config)
        self.batch_size = 100
        self.max_concurrent = 3
        self.request_timeout = 300
        self.storage_batch_size = 5000
        
        self.request_timeout_config = ClientTimeout(
            total=self.request_timeout,
            connect=60,
            sock_read=300
        )
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 QGIS/33603/macOS 15.1'
        }
        
        self.request_semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # Define status mappings
        self.status_mapping = {
            'Frivillig aftale tilbudt (UDGET)': 'Action Required',
            'Gennemget, indsats ndvendig': 'Action Required',
            'Ikke gennemget (default vrdi)': 'Action Required',
            'Gennemget, indsats ikke ndvendig': 'Completed',
            'Indsats gennemfrt': 'Completed',
            'Ingen erhvervsmssig anvendelse af pesticider': 'Completed'
        }

    def _get_params(self, start_index=0):
        """Get WFS request parameters"""
        return {
            'SERVICE': 'WFS',
            'REQUEST': 'GetFeature',
            'VERSION': '2.0.0',
            'TYPENAMES': 'dai:status_bnbo',
            'STARTINDEX': str(start_index),
            'COUNT': str(self.batch_size),
            'SRSNAME': 'urn:ogc:def:crs:EPSG::25832'
        }

    def _parse_geometry(self, geom_elem):
        """Parse GML geometry into WKT and calculate area"""
        try:
            gml_ns = '{http://www.opengis.net/gml/3.2}'
            
            multi_surface = geom_elem.find(f'.//{gml_ns}MultiSurface')
            if multi_surface is None:
                logger.error("No MultiSurface element found")
                return None
            
            polygons = []
            for surface_member in multi_surface.findall(f'.//{gml_ns}surfaceMember'):
                polygon = surface_member.find(f'.//{gml_ns}Polygon')
                if polygon is None:
                    continue
                    
                pos_list = polygon.find(f'.//{gml_ns}posList')
                if pos_list is None or not pos_list.text:
                    continue
                    
                try:
                    coords = [float(x) for x in pos_list.text.strip().split()]
                    coords = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]
                    if len(coords) >= 4:
                        polygons.append(Polygon(coords))
                except Exception as e:
                    logger.error(f"Failed to parse coordinates: {str(e)}")
                    continue
            
            if not polygons:
                return None
            
            geom = MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]
            area_ha = geom.area / 10000  # Convert square meters to hectares
            
            return {
                'wkt': geom.wkt,
                'area_ha': area_ha
            }
            
        except Exception as e:
            logger.error(f"Error parsing geometry: {str(e)}")
            return None

    def _parse_feature(self, feature):
        """Parse a single feature into a dictionary"""
        try:
            namespace = feature.tag.split('}')[0].strip('{')
            
            geom_elem = feature.find(f'{{%s}}Shape' % namespace)
            if geom_elem is None:
                logger.warning("No geometry found in feature")
                return None

            geometry_data = self._parse_geometry(geom_elem)
            if geometry_data is None:
                logger.warning("Failed to parse geometry")
                return None

            data = {
                'geometry': geometry_data['wkt'],
                'area_ha': geometry_data['area_ha']
            }
            
            for elem in feature:
                if not elem.tag.endswith('Shape'):
                    key = elem.tag.split('}')[-1].lower()
                    if elem.text:
                        value = clean_value(elem.text)
                        if value is not None:
                            data[key] = value
            
            # Map the status to simplified categories
            if 'status_bnbo' in data:
                data['status_category'] = self.status_mapping.get(data['status_bnbo'], 'Unknown')
            
            return data
            
        except Exception as e:
            logger.error(f"Error parsing feature: {str(e)}", exc_info=True)
            return None

    @backoff.on_exception(
        backoff.expo,
        (ClientError, asyncio.TimeoutError),
        max_tries=3
    )
    async def _fetch_chunk(self, session, start_index):
        """Fetch a chunk of features with retries"""
        async with self.request_semaphore:
            params = self._get_params(start_index)
            
            try:
                async with session.get(
                    'https://arealeditering-dist-geo.miljoeportal.dk/geoserver/wfs',
                    params=params,
                    timeout=self.request_timeout_config
                ) as response:
                    if response.status != 200:
                        logger.error(f"Error fetching chunk. Status: {response.status}")
                        error_text = await response.text()
                        logger.error(f"Error response: {error_text[:500]}")
                        return None
                    
                    text = await response.text()
                    root = ET.fromstring(text)
                    
                    features = []
                    namespaces = {}
                    for elem in root.iter():
                        if '}' in elem.tag:
                            ns_url = elem.tag.split('}')[0].strip('{')
                            namespaces['ns'] = ns_url
                            break
                    
                    for member in root.findall('.//ns:member', namespaces=namespaces):
                        for feature in member:
                            parsed = self._parse_feature(feature)
                            if parsed and parsed.get('geometry'):
                                features.append(parsed)
                    
                    return features
                    
            except Exception as e:
                logger.error(f"Error fetching chunk: {str(e)}")
                raise

    async def write_to_storage(self, features, dataset):
        """Write features to GeoParquet in Cloud Storage with special dissolve logic"""
        if not features:
            return
        
        try:
            # Create DataFrame from features
            df = pd.DataFrame(features)
            geometries = [wkt.loads(f['geometry']) for f in features]
            gdf = gpd.GeoDataFrame(df, geometry=geometries, crs="EPSG:25832")
            
            # Handle working/final files
            temp_working = f"/tmp/{dataset}_working.parquet"
            working_blob = self.bucket.blob(f'raw/{dataset}/working.parquet')
            
            if working_blob.exists():
                working_blob.download_to_filename(temp_working)
                existing_gdf = gpd.read_parquet(temp_working)
                logger.info(f"Appending {len(gdf):,} features to existing {len(existing_gdf):,}")
                combined_gdf = pd.concat([existing_gdf, gdf], ignore_index=True)
            else:
                combined_gdf = gdf
            
            # Write working file
            combined_gdf.to_parquet(temp_working)
            working_blob.upload_from_filename(temp_working)
            logger.info(f"Updated working file now has {len(combined_gdf):,} features")
            
            # If sync complete, create final files
            if hasattr(self, 'is_sync_complete') and self.is_sync_complete:
                logger.info("Sync complete - writing final files")
                
                # Write regular final file
                final_blob = self.bucket.blob(f'raw/{dataset}/current.parquet')
                final_blob.upload_from_filename(temp_working)
                
                # Create dissolved version with special handling
                logger.info("Creating dissolved version...")
                try:
                    # Convert to WGS84 before processing
                    if combined_gdf.crs.to_epsg() != 4326:
                        combined_gdf = combined_gdf.to_crs("EPSG:4326")
                    
                    # Split into two categories
                    action_required = combined_gdf[combined_gdf['status_category'] == 'Action Required']
                    completed = combined_gdf[combined_gdf['status_category'] == 'Completed']
                    
                    # Dissolve each category
                    action_required_dissolved = None
                    if not action_required.empty:
                        action_required_dissolved = unary_union(action_required.geometry.values).buffer(0)
                    
                    completed_dissolved = None
                    if not completed.empty:
                        completed_dissolved = unary_union(completed.geometry.values).buffer(0)
                    
                    # Handle overlaps - remove completed areas that overlap with action required
                    if action_required_dissolved is not None and completed_dissolved is not None:
                        completed_dissolved = difference(completed_dissolved, action_required_dissolved)
                    
                    # Create final dissolved GeoDataFrame
                    dissolved_geometries = []
                    categories = []
                    
                    if action_required_dissolved is not None:
                        if action_required_dissolved.geom_type == 'MultiPolygon':
                            dissolved_geometries.extend(list(action_required_dissolved.geoms))
                            categories.extend(['Action Required'] * len(action_required_dissolved.geoms))
                        else:
                            dissolved_geometries.append(action_required_dissolved)
                            categories.append('Action Required')
                    
                    if completed_dissolved is not None:
                        if completed_dissolved.geom_type == 'MultiPolygon':
                            dissolved_geometries.extend(list(completed_dissolved.geoms))
                            categories.extend(['Completed'] * len(completed_dissolved.geoms))
                        else:
                            dissolved_geometries.append(completed_dissolved)
                            categories.append('Completed')
                    
                    dissolved_gdf = gpd.GeoDataFrame({
                        'status_category': categories,
                        'geometry': dissolved_geometries
                    }, crs="EPSG:4326")
                    
                    # Final validation
                    dissolved_gdf = validate_and_transform_geometries(dissolved_gdf, f"{dataset}_dissolved")
                    
                    # Write dissolved version
                    temp_dissolved = f"/tmp/{dataset}_dissolved.parquet"
                    dissolved_gdf.to_parquet(temp_dissolved)
                    dissolved_blob = self.bucket.blob(f'raw/{dataset}/dissolved_current.parquet')
                    dissolved_blob.upload_from_filename(temp_dissolved)
                    logger.info("Dissolved version created and saved")
                    
                except Exception as e:
                    logger.error(f"Error during dissolve operation: {str(e)}")
                    raise
                
                # Cleanup
                working_blob.delete()
                os.remove(temp_dissolved)
            
            # Cleanup working file
            if os.path.exists(temp_working):
                os.remove(temp_working)
            
        except Exception as e:
            logger.error(f"Error writing to storage: {str(e)}")
            raise

    async def sync(self):
        """Sync BNBO status data"""
        logger.info("Starting BNBO status sync...")
        self.is_sync_complete = False
        total_processed = 0
        features_batch = []
        
        try:
            # Create SSL context that doesn't verify certificates
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            connector = aiohttp.TCPConnector(ssl=ssl_context)
            async with aiohttp.ClientSession(headers=self.headers, connector=connector) as session:
                # Get total count
                params = self._get_params(0)
                async with session.get(
                    'https://arealeditering-dist-geo.miljoeportal.dk/geoserver/wfs',
                    params=params
                ) as response:
                    if response.status != 200:
                        logger.error(f"Failed initial request. Status: {response.status}")
                        return 0
                    
                    text = await response.text()
                    root = ET.fromstring(text)
                    total_features = int(root.get('numberMatched', '0'))
                    logger.info(f"Found {total_features:,} total features")
                    
                    # Process first batch
                    features = []
                    namespaces = {}
                    for elem in root.iter():
                        if '}' in elem.tag:
                            ns_url = elem.tag.split('}')[0].strip('{')
                            namespaces['ns'] = ns_url
                            break
                            
                    for member in root.findall('.//ns:member', namespaces=namespaces):
                        for feature in member:
                            parsed = self._parse_feature(feature)
                            if parsed and parsed.get('geometry'):
                                features.append(parsed)
                    
                    if features:
                        features_batch.extend(features)
                        total_processed += len(features)
                        logger.info(f"Processed {len(features):,} features")
                    
                    # Process remaining batches
                    for start_index in range(self.batch_size, total_features, self.batch_size):
                        logger.info(f"Fetching features {start_index:,}-{min(start_index + self.batch_size, total_features):,} of {total_features:,}")
                        chunk = await self._fetch_chunk(session, start_index)
                        if chunk:
                            features_batch.extend(chunk)
                            total_processed += len(chunk)
                            logger.info(f"Processed {len(chunk):,} features")
                        
                        # Write batch if it's large enough
                        if len(features_batch) >= self.storage_batch_size:
                            logger.info(f"Writing batch of {len(features_batch):,} features")
                            await self.write_to_storage(features_batch, 'bnbo_status')
                            features_batch = []

                # Write any remaining features as final batch
                if features_batch:
                    logger.info(f"Writing final batch of {len(features_batch):,} features")
                    self.is_sync_complete = True
                    await self.write_to_storage(features_batch, 'bnbo_status')
                
                logger.info(f"Sync completed. Total processed: {total_processed:,}")
                return total_processed
                
        except Exception as e:
            self.is_sync_complete = False
            logger.error(f"Error in sync: {str(e)}", exc_info=True)
            return total_processed

    async def fetch(self):
        """Implement abstract method - using sync() instead"""
        return await self.sync()

================
File: src/sources/parsers/cadastral.py
================
from pathlib import Path
import asyncio
import os
import xml.etree.ElementTree as ET
from datetime import datetime
import logging
import aiohttp
from shapely.geometry import Polygon, MultiPolygon
from shapely import wkt
import geopandas as gpd
from google.cloud import storage
import time
import backoff
from aiohttp import ClientError, ClientTimeout
from dotenv import load_dotenv
from tqdm import tqdm
import psutil
import pandas as pd

from ..base import GeospatialSource
from ..utils.geometry_validator import validate_and_transform_geometries

logger = logging.getLogger(__name__)

def clean_value(value):
    """Clean string values"""
    if not isinstance(value, str):
        return value
    value = value.strip()
    return value if value else None

class Cadastral(GeospatialSource):
    """Danish cadastral parcels from WFS"""
    
    source_id = "cadastral"
    
    def __init__(self, config):
        super().__init__(config)
        self.field_mapping = {
            'BFEnummer': ('bfe_number', int),
            'forretningshaendelse': ('business_event', str),
            'forretningsproces': ('business_process', str),
            'senesteSagLokalId': ('latest_case_id', str),
            'id_lokalId': ('id_local', str),
            'id_namespace': ('id_namespace', str),
            'registreringFra': ('registration_from', lambda x: datetime.fromisoformat(x.replace('Z', '+00:00'))),
            'virkningFra': ('effect_from', lambda x: datetime.fromisoformat(x.replace('Z', '+00:00'))),
            'virkningsaktoer': ('authority', str),
            'arbejderbolig': ('is_worker_housing', lambda x: x.lower() == 'true'),
            'erFaelleslod': ('is_common_lot', lambda x: x.lower() == 'true'),
            'hovedejendomOpdeltIEjerlejligheder': ('has_owner_apartments', lambda x: x.lower() == 'true'),
            'udskiltVej': ('is_separated_road', lambda x: x.lower() == 'true'),
            'landbrugsnotering': ('agricultural_notation', str)
        }
        
        load_dotenv()
        self.username = os.getenv('DATAFORDELER_USERNAME')
        self.password = os.getenv('DATAFORDELER_PASSWORD')
        if not self.username or not self.password:
            raise ValueError("Missing DATAFORDELER_USERNAME or DATAFORDELER_PASSWORD")
        
        self.page_size = int(os.getenv('CADASTRAL_PAGE_SIZE', '1000'))
        self.batch_size = int(os.getenv('CADASTRAL_BATCH_SIZE', '5000'))
        self.max_concurrent = int(os.getenv('CADASTRAL_MAX_CONCURRENT', '5'))
        self.request_timeout = int(os.getenv('CADASTRAL_REQUEST_TIMEOUT', '300'))
        self.total_timeout = int(os.getenv('CADASTRAL_TOTAL_TIMEOUT', '7200'))
        self.requests_per_second = int(os.getenv('CADASTRAL_REQUESTS_PER_SECOND', '2'))
        self.last_request_time = {}
        self.request_semaphore = asyncio.Semaphore(self.max_concurrent)
        
        self.request_timeout_config = aiohttp.ClientTimeout(
            total=self.request_timeout,
            connect=60,
            sock_read=300
        )
        
        self.total_timeout_config = aiohttp.ClientTimeout(
            total=self.total_timeout,
            connect=60,
            sock_read=300
        )
        
        self.timeout = aiohttp.ClientTimeout(total=self.request_timeout)
        
        self.namespaces = {
            'wfs': 'http://www.opengis.net/wfs/2.0',
            'mat': 'http://data.gov.dk/schemas/matrikel/1',
            'gml': 'http://www.opengis.net/gml/3.2'
        }

    def _get_base_params(self):
        """Get base WFS request parameters without pagination"""
        return {
            'username': self.username,
            'password': self.password,
            'SERVICE': 'WFS',
            'REQUEST': 'GetFeature',
            'VERSION': '2.0.0',
            'TYPENAMES': 'mat:SamletFastEjendom_Gaeldende',
            'SRSNAME': 'EPSG:25832'
        }

    def _get_params(self, start_index=0):
        """Get WFS request parameters with pagination"""
        params = self._get_base_params()
        params.update({
            'startIndex': str(start_index),
            'count': str(self.page_size)
        })
        return params

    def _parse_geometry(self, geom_elem):
        """Parse GML geometry to WKT"""
        try:
            pos_lists = geom_elem.findall('.//gml:posList', self.namespaces)
            if not pos_lists:
                return None

            polygons = []
            for pos_list in pos_lists:
                if not pos_list.text:
                    continue

                coords = [float(x) for x in pos_list.text.strip().split()]
                # Keep the original 3D coordinate handling - take x,y and skip z
                pairs = [(coords[i], coords[i+1]) 
                        for i in range(0, len(coords), 3)]

                if len(pairs) < 4:
                    logger.warning(f"Not enough coordinate pairs ({len(pairs)}) to form a polygon")
                    continue

                try:
                    # Check if the polygon is closed (first point equals last point)
                    if pairs[0] != pairs[-1]:
                        pairs.append(pairs[0])  # Close the polygon
                    
                    polygon = Polygon(pairs)
                    if polygon.is_valid:
                        polygons.append(polygon)
                    else:
                        # Try to fix invalid polygon
                        from shapely.ops import make_valid
                        fixed_polygon = make_valid(polygon)
                        if fixed_polygon.geom_type in ('Polygon', 'MultiPolygon'):
                            polygons.append(fixed_polygon)
                        else:
                            logger.warning(f"Could not create valid polygon, got {fixed_polygon.geom_type}")
                except Exception as e:
                    logger.warning(f"Error creating polygon: {str(e)}")
                    continue

            if not polygons:
                return None

            if len(polygons) == 1:
                final_geom = polygons[0]
            else:
                try:
                    final_geom = MultiPolygon(polygons)
                except Exception as e:
                    logger.warning(f"Error creating MultiPolygon: {str(e)}, falling back to first valid polygon")
                    final_geom = polygons[0]

            return wkt.dumps(final_geom)

        except Exception as e:
            logger.error(f"Error parsing geometry: {str(e)}")
            return None

    def _parse_feature(self, feature_elem):
        """Parse a single feature"""
        try:
            feature = {}
            
            # Add validation of the feature element
            if feature_elem is None:
                logger.warning("Received None feature element")
                return None
            
            # Parse all mapped fields
            for xml_field, (db_field, converter) in self.field_mapping.items():
                elem = feature_elem.find(f'.//mat:{xml_field}', self.namespaces)
                if elem is not None and elem.text:
                    try:
                        value = clean_value(elem.text)
                        if value is not None:
                            feature[db_field] = converter(value)
                    except (ValueError, TypeError) as e:
                        logger.warning(f"Error converting field {xml_field}: {str(e)}")
                        continue

            # Parse geometry
            geom_elem = feature_elem.find('.//mat:geometri/gml:MultiSurface', self.namespaces)
            if geom_elem is not None:
                geometry_wkt = self._parse_geometry(geom_elem)
                if geometry_wkt:
                    feature['geometry'] = geometry_wkt
                else:
                    logger.warning("Failed to parse geometry for feature")

            # Add validation of required fields
            if not feature.get('bfe_number'):
                logger.warning("Missing required field: bfe_number")
            if not feature.get('geometry'):
                logger.warning("Missing required field: geometry")

            return feature if feature.get('bfe_number') and feature.get('geometry') else None

        except Exception as e:
            logger.error(f"Error parsing feature: {str(e)}")
            return None

    async def _get_total_count(self, session):
        """Get total number of features from first page metadata"""
        params = self._get_base_params()
        params.update({
            'startIndex': '0',
            'count': '1'  # Just get one feature to check metadata
        })
        
        try:
            logger.info("Getting total count from first page metadata...")
            async with session.get(self.config['url'], params=params) as response:
                response.raise_for_status()
                text = await response.text()
                root = ET.fromstring(text)
                
                # Handle case where numberMatched might be '*'
                number_matched = root.get('numberMatched', '0')
                number_returned = root.get('numberReturned', '0')
                
                logger.info(f"WFS response metadata - numberMatched: {number_matched}, numberReturned: {number_returned}")
                
                if number_matched == '*':
                    # If server doesn't provide exact count, fetch a larger page to estimate
                    logger.warning("Server returned '*' for numberMatched, fetching sample to estimate...")
                    params['count'] = '1000'
                    async with session.get(self.config['url'], params=params) as sample_response:
                        sample_text = await sample_response.text()
                        sample_root = ET.fromstring(sample_text)
                        feature_count = len(sample_root.findall('.//mat:SamletFastEjendom_Gaeldende', self.namespaces))
                        # Estimate conservatively
                        return feature_count * 2000  # Adjust multiplier based on expected data size
                
                if not number_matched.isdigit():
                    raise ValueError(f"Invalid numberMatched value: {number_matched}")
                    
                total_available = int(number_matched)
                
                # Add sanity check for unreasonable numbers
                if total_available > 5000000:  # Adjust threshold as needed
                    logger.warning(f"Unusually high feature count: {total_available:,}. This may indicate an issue.")
                    
                return total_available
                
        except Exception as e:
            logger.error(f"Error getting total count: {str(e)}")
            raise

    async def _wait_for_rate_limit(self):
        """Ensure we don't exceed requests_per_second"""
        worker_id = id(asyncio.current_task())
        if worker_id in self.last_request_time:
            elapsed = time.time() - self.last_request_time[worker_id]
            if elapsed < 1.0 / self.requests_per_second:
                await asyncio.sleep(1.0 / self.requests_per_second - elapsed)
        self.last_request_time[worker_id] = time.time()

    @backoff.on_exception(
        backoff.expo,
        (ClientError, asyncio.TimeoutError),
        max_tries=3,
        max_time=60
    )
    async def _fetch_chunk(self, session, start_index, timeout=None):
        """Fetch a chunk of features with rate limiting and retries"""
        async with self.request_semaphore:
            await self._wait_for_rate_limit()
            
            params = self._get_params(start_index)
            
            try:
                logger.info(f"Fetching chunk at index {start_index}")
                async with session.get(
                    self.config['url'], 
                    params=params,
                    timeout=timeout or self.request_timeout_config
                ) as response:
                    if response.status == 429:  # Too Many Requests
                        retry_after = int(response.headers.get('Retry-After', 5))
                        logger.warning(f"Rate limited, waiting {retry_after} seconds")
                        await asyncio.sleep(retry_after)
                        raise ClientError("Rate limited")
                    
                    response.raise_for_status()
                    content = await response.text()
                    root = ET.fromstring(content)
                    
                    # Add validation of returned features count
                    number_returned = root.get('numberReturned', '0')
                    logger.info(f"WFS reports {number_returned} features returned in this chunk")
                    
                    features = []
                    feature_elements = root.findall('.//mat:SamletFastEjendom_Gaeldende', self.namespaces)
                    logger.info(f"Found {len(feature_elements)} feature elements in XML")
                    
                    for feature_elem in feature_elements:
                        feature = self._parse_feature(feature_elem)
                        if feature:
                            features.append(feature)
                    
                    valid_count = len(features)
                    logger.info(f"Chunk {start_index}: parsed {valid_count} valid features out of {len(feature_elements)} elements")
                    
                    # Validate that we're getting reasonable numbers
                    if valid_count == 0 and len(feature_elements) > 0:
                        logger.warning(f"No valid features parsed from {len(feature_elements)} elements - possible parsing issue")
                    elif valid_count < len(feature_elements) * 0.5:  # If we're losing more than 50% of features
                        logger.warning(f"Low feature parsing success rate: {valid_count}/{len(feature_elements)}")
                    
                    return features
                    
            except Exception as e:
                logger.error(f"Error fetching chunk at index {start_index}: {str(e)}")
                raise

    @backoff.on_exception(
        backoff.expo,
        Exception,  # Consider narrowing this to specific storage exceptions
        max_tries=3,
        max_time=300
    )
    async def write_to_storage(self, features, dataset):
        """Write features to GeoParquet in Cloud Storage"""
        if not features:
            return
            
        try:
            # Create DataFrame from WKT features
            df = pd.DataFrame([{k:v for k,v in f.items() if k != 'geometry'} for f in features])
            geometries = [wkt.loads(f['geometry']) for f in features]
            gdf = gpd.GeoDataFrame(df, geometry=geometries, crs="EPSG:25832")
            
            # Validate and transform geometries
            gdf = validate_and_transform_geometries(gdf, dataset)
            
            # Handle working/final files
            temp_working = f"/tmp/{dataset}_working.parquet"
            working_blob = self.bucket.blob(f'raw/{dataset}/working.parquet')
            
            if working_blob.exists():
                working_blob.download_to_filename(temp_working)
                existing_gdf = gpd.read_parquet(temp_working)
                logger.info(f"Appending {len(gdf):,} features to existing {len(existing_gdf):,}")
                combined_gdf = pd.concat([existing_gdf, gdf], ignore_index=True)
            else:
                combined_gdf = gdf
                
            # Write working file
            combined_gdf.to_parquet(temp_working)
            working_blob.upload_from_filename(temp_working)
            logger.info(f"Updated working file now has {len(combined_gdf):,} features")
            
            # If sync complete, create final file
            if self.is_sync_complete:
                logger.info(f"Sync complete - writing final file with {len(combined_gdf):,} features")
                final_blob = self.bucket.blob(f'raw/{dataset}/current.parquet')
                final_blob.upload_from_filename(temp_working)
                working_blob.delete()
            
            # Cleanup
            os.remove(temp_working)
            
        except Exception as e:
            logger.error(f"Error writing to storage: {str(e)}")
            raise

    async def sync(self):
        """Sync cadastral data to Cloud Storage"""
        logger.info("Starting cadastral sync...")
        self.is_sync_complete = False
        
        try:
            async with aiohttp.ClientSession(timeout=self.total_timeout_config) as session:
                total_features = await self._get_total_count(session)
                logger.info(f"Found {total_features:,} total features")
                
                features_batch = []
                total_processed = 0
                failed_chunks = []
                
                for start_index in range(0, total_features, self.page_size):
                    try:
                        chunk = await self._fetch_chunk(session, start_index)
                        if chunk:
                            features_batch.extend(chunk)
                            total_processed += len(chunk)
                            
                            # Log progress every 10,000 features
                            if total_processed % 10000 == 0:
                                logger.info(f"Progress: {total_processed:,}/{total_features:,} features ({(total_processed/total_features)*100:.1f}%)")
                            
                            # Write batch if it's large enough or it's the last batch
                            is_last_batch = (start_index + self.page_size) >= total_features
                            if len(features_batch) >= self.batch_size or is_last_batch:
                                logger.info(f"Writing batch of {len(features_batch):,} features (is_last_batch: {is_last_batch})")
                                self.is_sync_complete = is_last_batch
                                await self.write_to_storage(features_batch, 'cadastral')
                                features_batch = []
                                
                    except Exception as e:
                        logger.error(f"Error processing batch at {start_index}: {str(e)}")
                        failed_chunks.append(start_index)
                        continue
                
                if failed_chunks:
                    logger.error(f"Failed to process chunks starting at indices: {failed_chunks}")
                
                logger.info(f"Sync completed. Total processed: {total_processed:,} features")
                return total_processed
                
        except Exception as e:
            self.is_sync_complete = False
            logger.error(f"Error in sync: {str(e)}")
            raise

    async def fetch(self):
        """Implement abstract method - using sync() instead"""
        logger.info("Fetch method called - using sync() instead")
        return await self.sync()

================
File: src/sources/parsers/chr_data.py
================
"""Parser for CHR data using listOplysninger."""
import logging
from typing import Dict, List, Any, Optional
from zeep import Client
from zeep.transports import Transport
from requests import Session
from zeep.wsse.username import UsernameToken
import certifi
import pandas as pd
from datetime import datetime
from google.cloud import secretmanager
from pyproj import Transformer
from ..base import BaseSource
from .chr_species import CHRSpeciesParser

logger = logging.getLogger(__name__)

class CHRDataParser(BaseSource):
    """Parser for CHR data using listOplysninger."""
    
    WSDL_URLS = {
        'stamdata': 'https://ws.fvst.dk/service/CHR_stamdataWS?WSDL',
        'besaetning': 'https://ws.fvst.dk/service/CHR_besaetningWS?wsdl',
        'ejendom': 'https://ws.fvst.dk/service/CHR_ejendomWS?wsdl'
    }
    
    # Constants
    BATCH_SIZE = 100  # Further reduced batch size for better stability
    UPLOAD_THRESHOLD = 500  # Reduced threshold for batch uploads
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        try:
            # Get credentials from Google Secret Manager
            client = secretmanager.SecretManagerServiceClient()
            project_id = "landbrugsdata-1"
            
            def get_secret(secret_id: str) -> str:
                name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
                response = client.access_secret_version(request={"name": name})
                return response.payload.data.decode("UTF-8")
            
            # Get credentials
            self.username = get_secret('fvm_username')
            self.password = get_secret('fvm_password')
            
            # Initialize session with SSL verification
            session = Session()
            session.verify = certifi.where()
            self.transport = Transport(session=session)
            
            # Initialize clients for different services
            self.clients = {}
            for service_name, url in self.WSDL_URLS.items():
                logger.info(f"Initializing client for {service_name} service...")
                self.clients[service_name] = Client(
                    url,
                    transport=self.transport,
                    wsse=UsernameToken(self.username, self.password)
                )
                logger.info(f"Successfully initialized {service_name} client")
            
            logger.info("Successfully initialized CHR data parser with all services")
            
        except Exception as e:
            logger.error(f"Error initializing CHR data parser: {str(e)}", exc_info=True)
            raise

    @property
    def source_id(self) -> str:
        return "chr_data"

    async def process_chr_numbers(self, chr_numbers: List[int], species_code: int) -> pd.DataFrame:
        """Process a list of CHR numbers and return the data."""
        results = []
        total_chr_numbers = len(chr_numbers)
        logger.info(f"Starting to process {total_chr_numbers} CHR numbers for species {species_code}")
        
        try:
            # Track unique entities
            seen_properties = set()
            seen_owners = set()
            seen_users = set()
            seen_practices = set()
            
            # Data containers
            properties_data = []
            owners_data = []
            users_data = []
            practices_data = []
            herds_data = []
            
            # Process CHR numbers in smaller batches
            for i in range(0, total_chr_numbers, self.BATCH_SIZE):
                batch = chr_numbers[i:i + self.BATCH_SIZE]
                batch_num = i//self.BATCH_SIZE + 1
                total_batches = (total_chr_numbers + self.BATCH_SIZE - 1)//self.BATCH_SIZE
                logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} CHR numbers)")
                
                try:
                    # Fetch detailed data for all CHR numbers in this batch
                    logger.info(f"Fetching property details for batch {batch_num}...")
                    request = {
                        'GLRCHRWSInfoInbound': {
                            'BrugerNavn': self.username,
                            'KlientId': 'LandbrugsData',
                            'SessionId': '1',
                            'IPAdresse': '',
                            'TrackID': f'property_details_batch_{batch_num}'
                        },
                        'Request': {
                            'ChrNummer': batch,
                            'DyreArtKode': str(species_code)
                        }
                    }
                    
                    result = self.clients['ejendom'].service.listOplysninger(request)
                    logger.info(f"Received response for batch {batch_num}")
                    
                    if not hasattr(result, 'Response') or not result.Response or not hasattr(result.Response, 'EjendomsOplysningerListe'):
                        logger.warning(f"No property details found in response for batch {batch_num}")
                        continue
                    
                    properties = result.Response.EjendomsOplysningerListe.EjendomsOplysninger
                    if not isinstance(properties, list):
                        properties = [properties]
                    
                    logger.info(f"Processing {len(properties)} properties in batch {batch_num}")
                    
                    # Process each property and its related data
                    for prop_idx, prop in enumerate(properties, 1):
                        logger.info(f"Processing property {prop_idx}/{len(properties)} in batch {batch_num}")
                        chr_number = self.safe_int(getattr(prop, 'ChrNummer', None))
                        if not chr_number or chr_number in seen_properties:
                            continue
                        
                        # Process property details
                        property_data = self._process_property_data(prop, chr_number)
                        if property_data:
                            # Get additional veterinary events
                            additional_events = self._get_additional_veterinary_events(chr_number)
                            if additional_events:
                                if 'veterinary_events' not in property_data:
                                    property_data['veterinary_events'] = []
                                property_data['veterinary_events'].extend(additional_events)
                            
                            properties_data.append(property_data)
                            seen_properties.add(chr_number)
                        
                        # Process herds
                        if hasattr(prop, 'Besaetninger'):
                            herds = prop.Besaetninger.Besaetning
                            if not isinstance(herds, list):
                                herds = [herds]
                            
                            logger.info(f"Processing {len(herds)} herds for property {prop_idx}")
                            for herd_idx, herd in enumerate(herds, 1):
                                logger.info(f"Processing herd {herd_idx}/{len(herds)}")
                                # Get herd number once
                                herd_number = self.safe_int(getattr(herd, 'BesaetningsNummer', None))
                                
                                # Process herd data
                                herd_rows = self._process_herd_data(herd, chr_number)
                                herds_data.extend(herd_rows)
                                
                                # Process owner info
                                if hasattr(herd, 'Ejer'):
                                    owner_data = self._process_person_data(herd.Ejer, chr_number, 'owner', herd_number)
                                    if owner_data:
                                        owner_key = f"{owner_data['cvr_number'] or owner_data['cpr_number']}_{chr_number}_{herd_number}"
                                        if owner_key not in seen_owners:
                                            owners_data.append(owner_data)
                                            seen_owners.add(owner_key)
                                
                                # Process user info
                                if hasattr(herd, 'Bruger'):
                                    user_data = self._process_person_data(herd.Bruger, chr_number, 'user', herd_number)
                                    if user_data:
                                        user_key = f"{user_data['cvr_number'] or user_data['cpr_number']}_{chr_number}_{herd_number}"
                                        if user_key not in seen_users:
                                            users_data.append(user_data)
                                            seen_users.add(user_key)
                                
                                # Process veterinary practice info
                                if hasattr(herd, 'BesPraksis'):
                                    practice_data = self._process_practice_data(herd.BesPraksis, chr_number, herd_number)
                                    if practice_data:
                                        practice_key = f"{practice_data['practice_number']}_{chr_number}_{herd_number}"
                                        if practice_key not in seen_practices:
                                            practices_data.append(practice_data)
                                            seen_practices.add(practice_key)
                    
                    # Upload batch if we have enough data
                    if len(herds_data) >= self.UPLOAD_THRESHOLD or len(properties_data) >= self.UPLOAD_THRESHOLD:
                        logger.info(f"Upload threshold reached in batch {batch_num}, uploading data...")
                        await self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
                        results.append(('batch', {
                            'herds': len(herds_data),
                            'properties': len(properties_data),
                            'owners': len(owners_data),
                            'users': len(users_data),
                            'practices': len(practices_data)
                        }))
                        # Clear data containers
                        herds_data = []
                        properties_data = []
                        owners_data = []
                        users_data = []
                        practices_data = []
                        logger.info(f"Batch {batch_num} data uploaded successfully")
                    
                except Exception as e:
                    logger.error(f"Error processing batch {batch_num}: {str(e)}", exc_info=True)
                    continue
            
            # Upload any remaining data
            if any([herds_data, properties_data, owners_data, users_data, practices_data]):
                logger.info("Uploading remaining data...")
                await self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
                results.append(('final_batch', {
                    'herds': len(herds_data),
                    'properties': len(properties_data),
                    'owners': len(owners_data),
                    'users': len(users_data),
                    'practices': len(practices_data)
                }))
                logger.info("Remaining data uploaded successfully")
            
            logger.info(f"Completed processing all {total_chr_numbers} CHR numbers")
            return pd.DataFrame(results, columns=['batch', 'counts'])
            
        except Exception as e:
            logger.error(f"Error processing CHR numbers: {str(e)}", exc_info=True)
            return pd.DataFrame()

    async def _upload_batch_data(self, herds_data: list, properties_data: list, owners_data: list, users_data: list, practices_data: list) -> None:
        """Upload batch data to Cloud Storage."""
        try:
            if herds_data:
                logger.info(f"Converting and uploading {len(herds_data)} herd records...")
                herds_df = pd.DataFrame(herds_data)
                await self.store(herds_df, 'herds')
                logger.info(f"Uploaded {len(herds_data)} herd records")
            
            if properties_data:
                logger.info(f"Converting and uploading {len(properties_data)} property records...")
                properties_df = pd.DataFrame(properties_data)
                await self.store(properties_df, 'properties')
                logger.info(f"Uploaded {len(properties_data)} property records")
            
            if owners_data:
                logger.info(f"Converting and uploading {len(owners_data)} owner records...")
                owners_df = pd.DataFrame(owners_data)
                await self.store(owners_df, 'owners')
                logger.info(f"Uploaded {len(owners_data)} owner records")
            
            if users_data:
                logger.info(f"Converting and uploading {len(users_data)} user records...")
                users_df = pd.DataFrame(users_data)
                await self.store(users_df, 'users')
                logger.info(f"Uploaded {len(users_data)} user records")
            
            if practices_data:
                logger.info(f"Converting and uploading {len(practices_data)} practice records...")
                practices_df = pd.DataFrame(practices_data)
                await self.store(practices_df, 'practices')
                logger.info(f"Uploaded {len(practices_data)} practice records")
                
        except Exception as e:
            logger.error(f"Error uploading batch data: {str(e)}", exc_info=True)
            raise

    def _process_related_data(self, herd: Any, chr_number: int, owners_data: list, users_data: list, practices_data: list, 
                            seen_owners: set, seen_users: set, seen_practices: set) -> None:
        """Process owner, user, and practice data for a herd."""
        try:
            herd_number = self.safe_int(getattr(herd, 'BesaetningsNummer', None))
            
            # Process owner info
            if hasattr(herd, 'Ejer'):
                owner_data = self._process_person_data(herd.Ejer, chr_number, 'owner', herd_number)
                if owner_data:
                    owner_key = f"{owner_data['cvr_number'] or owner_data['cpr_number']}_{chr_number}_{herd_number}"
                    if owner_key not in seen_owners:
                        owners_data.append(owner_data)
                        seen_owners.add(owner_key)
            
            # Process user info
            if hasattr(herd, 'Bruger'):
                user_data = self._process_person_data(herd.Bruger, chr_number, 'user', herd_number)
                if user_data:
                    user_key = f"{user_data['cvr_number'] or user_data['cpr_number']}_{chr_number}_{herd_number}"
                    if user_key not in seen_users:
                        users_data.append(user_data)
                        seen_users.add(user_key)
            
            # Process veterinary practice info
            if hasattr(herd, 'BesPraksis'):
                practice_data = self._process_practice_data(herd.BesPraksis, chr_number, herd_number)
                if practice_data:
                    practice_key = f"{practice_data['practice_number']}_{chr_number}_{herd_number}"
                    if practice_key not in seen_practices:
                        practices_data.append(practice_data)
                        seen_practices.add(practice_key)
                        
        except Exception as e:
            logger.error(f"Error processing related data for CHR {chr_number}: {str(e)}", exc_info=True)

    def _get_additional_veterinary_events(self, chr_number: int) -> List[Dict[str, Any]]:
        """Get additional veterinary events from the ejendom endpoint."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': f'additional_events_{chr_number}'
                },
                'Request': {
                    'ChrNummer': [chr_number]
                }
            }
            
            result = self.clients['ejendom'].service.listOplysninger(request)
            
            if not hasattr(result, 'Response') or not result.Response or not hasattr(result.Response, 'EjendomsOplysningerListe'):
                return []
            
            properties = result.Response.EjendomsOplysningerListe.EjendomsOplysninger
            if not isinstance(properties, list):
                properties = [properties]
            
            all_events = []
            for prop in properties:
                if hasattr(prop, 'VeterinaereHaendelser'):
                    vet = prop.VeterinaereHaendelser
                    if hasattr(vet, 'VeterinaerHaendelse'):
                        events = vet.VeterinaerHaendelse
                        if not isinstance(events, list):
                            events = [events]
                        
                        for event in events:
                            event_data = {
                                'disease_code': self.safe_str(getattr(event, 'SygdomsKode', None)),
                                'disease_text': self.safe_str(getattr(event, 'SygdomsTekst', None)),
                                'status_code': self.safe_str(getattr(event, 'VeterinaerStatusKode', None)),
                                'status_text': self.safe_str(getattr(event, 'VeterinaerStatusTekst', None)),
                                'level_code': self.safe_str(getattr(event, 'SygdomsNiveauKode', None)),
                                'level_text': self.safe_str(getattr(event, 'SygdomsNiveauTekst', None)),
                                'status_date': self._parse_date(getattr(event, 'DatoVeterinaerStatus', None)),
                                'remarks': self.safe_str(getattr(event, 'VeterinaerHaendelseBemaerkning', None))
                            }
                            all_events.append({k: v for k, v in event_data.items() if v is not None})
            
            return all_events
            
        except Exception as e:
            logger.error(f"Error getting additional veterinary events for CHR {chr_number}: {str(e)}")
            return []

    def _process_property_data(self, prop: Any, chr_number: int) -> Dict[str, Any]:
        """Process property data from listOplysninger response."""
        property_details = {
            'chr_number': chr_number,
            'food_region_number': None,
            'food_region_name': None,
            'veterinary_dept_name': None,
            'veterinary_section_name': None
        }
        
        # Basic property info
        if hasattr(prop, 'Ejendom'):
            ejendom = prop.Ejendom
            property_details.update({
                'property_address': self.safe_str(getattr(ejendom, 'Adresse', None)),
                'property_city': self.safe_str(getattr(ejendom, 'ByNavn', None)),
                'property_postal_code': self.safe_str(getattr(ejendom, 'PostNummer', None)),
                'property_postal_district': self.safe_str(getattr(ejendom, 'PostDistrikt', None)),
                'property_municipality_code': self.safe_int(getattr(ejendom, 'KommuneNummer', None)),
                'property_municipality_name': self.safe_str(getattr(ejendom, 'KommuneNavn', None)),
                'property_created_date': self._parse_date(getattr(ejendom, 'DatoOpret', None)),
                'property_updated_date': self._parse_date(getattr(ejendom, 'DatoOpdatering', None))
            })
        
        # Food authority information
        if hasattr(prop, 'FVST'):
            fvst = prop.FVST
            property_details.update({
                'food_region_number': self.safe_int(getattr(fvst, 'FoedevareRegionsNummer', None)),
                'food_region_name': self.safe_str(getattr(fvst, 'FoedevareRegionsNavn', None)),
                'veterinary_dept_name': self.safe_str(getattr(fvst, 'VeterinaerAfdelingsNavn', None)),
                'veterinary_section_name': self.safe_str(getattr(fvst, 'VeterinaerSektionsNavn', None))
            })
        
        # Coordinates
        if hasattr(prop, 'StaldKoordinater'):
            coords = prop.StaldKoordinater
            x = self.safe_str(getattr(coords, 'StaldKoordinatX', None))
            y = self.safe_str(getattr(coords, 'StaldKoordinatY', None))
            if x and y:
                try:
                    x_float = float(x)
                    y_float = float(y)
                    property_details['stable_coordinates_utm32'] = {'x': x_float, 'y': y_float}
                    
                    # Convert UTM32 coordinates to WGS84 (latitude/longitude)
                    transformer = Transformer.from_crs("EPSG:25832", "EPSG:4326")
                    lat, lon = transformer.transform(x_float, y_float)
                    property_details['latitude'] = lat
                    property_details['longitude'] = lon
                except (ValueError, TypeError) as e:
                    logger.warning(f"Error converting coordinates for CHR {chr_number}: {str(e)}")
        
        # Veterinary events - combine from both sources
        events = []
        
        # Events from hentCHRStamoplysninger
        if hasattr(prop, 'VeterinaereHaendelser'):
            vet = prop.VeterinaereHaendelser
            property_details['veterinary_problems'] = self.safe_str(getattr(vet, 'VeterinaereProblemer', None))
            
            if hasattr(vet, 'VeterinaerHaendelse'):
                base_events = vet.VeterinaerHaendelse
                if not isinstance(base_events, list):
                    base_events = [base_events]
                
                for event in base_events:
                    event_data = {
                        'disease_code': self.safe_str(getattr(event, 'SygdomsKode', None)),
                        'disease_text': self.safe_str(getattr(event, 'SygdomsTekst', None)),
                        'status_code': self.safe_str(getattr(event, 'VeterinaerStatusKode', None)),
                        'status_text': self.safe_str(getattr(event, 'VeterinaerStatusTekst', None)),
                        'level_code': self.safe_str(getattr(event, 'SygdomsNiveauKode', None)),
                        'level_text': self.safe_str(getattr(event, 'SygdomsNiveauTekst', None)),
                        'status_date': self._parse_date(getattr(event, 'DatoVeterinaerStatus', None)),
                        'remarks': self.safe_str(getattr(event, 'VeterinaerHaendelseBemaerkning', None))
                    }
                    events.append({k: v for k, v in event_data.items() if v is not None})
        
        # Additional events from listOplysninger
        additional_events = self._get_additional_veterinary_events(chr_number)
        if additional_events:
            events.extend(additional_events)
        
        if events:
            property_details['veterinary_events'] = events
        
        # Cooperation agreements
        if hasattr(prop, 'SamdriftNaboaftaler'):
            if hasattr(prop.SamdriftNaboaftaler, 'Samdrift'):
                samdrift = prop.SamdriftNaboaftaler.Samdrift
                if not isinstance(samdrift, list):
                    samdrift = [samdrift]
                
                property_details['cooperation_agreements'] = []
                for agreement in samdrift:
                    agreement_data = {
                        'species_code': self.safe_int(getattr(agreement, 'DyreArtKode', None)),
                        'species_text': self.safe_str(getattr(agreement, 'DyreArtTekst', None))
                    }
                    
                    if hasattr(agreement, 'ChrNumre') and hasattr(agreement.ChrNumre, 'ChrNummer'):
                        chr_list = agreement.ChrNumre.ChrNummer
                        if not isinstance(chr_list, list):
                            chr_list = [chr_list]
                        agreement_data['chr_numbers'] = [self.safe_int(chr) for chr in chr_list if self.safe_int(chr)]
                    
                    property_details['cooperation_agreements'].append(agreement_data)
        
        return {k: v for k, v in property_details.items() if v is not None}

    def _process_herd_data(self, herd: Any, chr_number: int) -> List[Dict[str, Any]]:
        """Process herd data from listOplysninger response."""
        herd_rows = []
        
        # Process herd sizes
        if hasattr(herd, 'BesStr'):
            sizes = herd.BesStr
            if not isinstance(sizes, list):
                sizes = [sizes]
            
            for size in sizes:
                if not self.safe_str(getattr(size, 'BesaetningsStoerrelseTekst', '')).lower().startswith('i alt'):
                    herd_row = {
                        'herd_number': self.safe_int(getattr(herd, 'BesaetningsNummer', None)),
                        'chr_number': chr_number,
                        'species_code': self.safe_int(getattr(herd, 'DyreArtKode', None)),
                        'species_text': self.safe_str(getattr(herd, 'DyreArtTekst', None)),
                        'usage_code': self.safe_int(getattr(herd, 'BrugsArtKode', None)),
                        'usage_text': self.safe_str(getattr(herd, 'BrugsArtTekst', None)),
                        'business_type': self.safe_str(getattr(herd, 'VirksomhedsArtTekst', None)),
                        'business_type_code': self.safe_int(getattr(herd, 'VirksomhedsArtKode', None)),
                        'herd_type': self.safe_str(getattr(herd, 'BesaetningsTypeTekst', None)),
                        'herd_type_code': self.safe_int(getattr(herd, 'BesaetningsTypeKode', None)),
                        'trade_text': self.safe_str(getattr(herd, 'OmsaetningsTekst', None)),
                        'trade_code': self.safe_int(getattr(herd, 'OmsaetningsKode', None)),
                        'is_organic': int(getattr(herd, 'Oekologisk', 'Nej') == 'Ja'),
                        'herd_status': self.safe_str(getattr(herd, 'BesaetningsStatus', None)),
                        'creation_date': self._parse_date(getattr(herd, 'DatoOpret', None)),
                        'last_update': self._parse_date(getattr(herd, 'DatoOpdatering', None)),
                        'end_date': self._parse_date(getattr(herd, 'DatoOphoer', None)),
                        'last_size_update': self._parse_date(getattr(herd, 'BesStrDatoAjourfoert', None)),
                        'animal_type': self.safe_str(getattr(size, 'BesaetningsStoerrelseTekst', None)),
                        'count': self.safe_int(getattr(size, 'BesaetningsStoerrelse', None))
                    }
                    herd_rows.append({k: v for k, v in herd_row.items() if v is not None})
        
        return herd_rows

    def _process_person_data(self, person: Any, chr_number: int, role: str, herd_number: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """Process person (owner/user) data from listOplysninger response."""
        if not person:
            return None
            
        person_id = self.safe_str(getattr(person, 'CvrNummer', None)) or self.safe_str(getattr(person, 'CprNummer', None))
        if not person_id:
            return None
            
        return {
            'cvr_number': self.safe_str(getattr(person, 'CvrNummer', None)),
            'cpr_number': self.safe_str(getattr(person, 'CprNummer', None)),
            'chr_number': chr_number,
            'herd_number': herd_number,
            'role': role,
            'name': self.safe_str(getattr(person, 'Navn', None)),
            'address': self.safe_str(getattr(person, 'Adresse', None)),
            'city': self.safe_str(getattr(person, 'ByNavn', None)),
            'postal_code': self.safe_str(getattr(person, 'PostNummer', None)),
            'postal_district': self.safe_str(getattr(person, 'PostDistrikt', None)),
            'municipality_code': self.safe_int(getattr(person, 'KommuneNummer', None)),
            'municipality_name': self.safe_str(getattr(person, 'KommuneNavn', None)),
            'phone': self.safe_str(getattr(person, 'TelefonNummer', None)),
            'mobile': self.safe_str(getattr(person, 'MobilNummer', None)),
            'email': self.safe_str(getattr(person, 'Email', None)),
            'address_protection': int(getattr(person, 'Adressebeskyttelse', 'Nej') == 'Ja'),
            'advertising_protection': int(getattr(person, 'Reklamebeskyttelse', 'Nej') == 'Ja')
        }

    def _process_practice_data(self, practice: Any, chr_number: int, herd_number: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """Process veterinary practice data from listOplysninger response."""
        if not practice:
            return None
            
        practice_number = self.safe_int(getattr(practice, 'PraksisNr', None))
        if not practice_number:
            return None
            
        return {
            'practice_number': practice_number,
            'chr_number': chr_number,
            'herd_number': herd_number,
            'name': self.safe_str(getattr(practice, 'PraksisNavn', None)),
            'address': self.safe_str(getattr(practice, 'PraksisAdresse', None)),
            'city': self.safe_str(getattr(practice, 'PraksisByNavn', None)),
            'postal_code': self.safe_str(getattr(practice, 'PraksisPostNummer', None)),
            'postal_district': self.safe_str(getattr(practice, 'PraksisPostDistrikt', None)),
            'phone': self.safe_str(getattr(practice, 'PraksisTelefonNummer', None)),
            'mobile': self.safe_str(getattr(practice, 'PraksisMobilNummer', None)),
            'email': self.safe_str(getattr(practice, 'PraksisEmail', None))
        }

    def safe_str(self, value: Any) -> Optional[str]:
        """Safely convert value to string, return None if empty."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return val_str if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None
            
    def safe_int(self, value: Any) -> Optional[int]:
        """Safely convert value to int, return None if not possible."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return int(val_str) if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    def _parse_date(self, date_value: Any) -> Optional[str]:
        """Parse date value to ISO format string."""
        if not date_value:
            return None
            
        try:
            if isinstance(date_value, datetime):
                return date_value.date().isoformat()
                
            if isinstance(date_value, str):
                # The API returns dates in YYYY-MM-DD format
                return date_value
                
            return None
            
        except Exception as e:
            logger.error(f"Error parsing date {date_value}: {str(e)}")
            return None

    async def fetch(self) -> pd.DataFrame:
        """Fetch data from the CHR web services."""
        try:
            # First get species data
            species_parser = CHRSpeciesParser(self.config)
            species_data = await species_parser.fetch()
            
            if species_data.empty:
                logger.error("Failed to fetch species data")
                return pd.DataFrame()
            
            # Process each species code
            results = []
            for species_code in species_data['species_code'].unique():
                logger.info(f"Processing species code: {species_code}")
                result = await self.process_species(species_code)
                if result is not None:
                    results.append(result)
            
            # Combine all results
            if results:
                return pd.concat(results, ignore_index=True)
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Error during fetch: {str(e)}")
            return pd.DataFrame()

    def fetch_sync(self) -> pd.DataFrame:
        """Synchronous version of fetch."""
        import asyncio
        return asyncio.run(self.fetch())

    async def process_species(self, species_code: int) -> Optional[pd.DataFrame]:
        """Process all CHR numbers for a given species code."""
        try:
            # Get species/usage combinations for this species code
            logger.info(f"Getting species/usage combinations for species code {species_code}")
            species_parser = CHRSpeciesParser(self.config)
            combinations_df = await species_parser.get_species_usage_combinations_async()
            
            if combinations_df.empty:
                logger.warning(f"No species/usage combinations found for species code {species_code}")
                return None
            
            # Filter combinations for this species code
            species_combinations = combinations_df[combinations_df['species_code'] == species_code]
            if species_combinations.empty:
                logger.warning(f"No usage codes found for species code {species_code}")
                return None
            
            logger.info(f"Found {len(species_combinations)} usage codes for species {species_code}")
            
            # Process each usage code
            all_results = []
            for _, row in species_combinations.iterrows():
                usage_code = row['usage_code']
                try:
                    logger.info(f"Processing species code {species_code} with usage code {usage_code}")
                    
                    # Get CHR numbers for this species/usage combination
                    logger.info(f"Fetching CHR numbers for species {species_code}, usage {usage_code}...")
                    chr_numbers_df = await species_parser.get_chr_numbers_async(species_code, usage_code)
                    
                    if not chr_numbers_df.empty:
                        chr_count = len(chr_numbers_df)
                        logger.info(f"Processing {chr_count} CHR numbers for species code {species_code}, usage code {usage_code}")
                        
                        # Process in smaller batches
                        for i in range(0, chr_count, self.UPLOAD_THRESHOLD):
                            batch_df = chr_numbers_df.iloc[i:i + self.UPLOAD_THRESHOLD]
                            batch_num = i//self.UPLOAD_THRESHOLD + 1
                            total_batches = (chr_count + self.UPLOAD_THRESHOLD - 1)//self.UPLOAD_THRESHOLD
                            
                            logger.info(f"Starting batch {batch_num}/{total_batches} for species {species_code}, usage {usage_code}")
                            try:
                                logger.info(f"Processing {len(batch_df)} CHR numbers...")
                                result = await self.process_chr_numbers(batch_df['chr_number'].tolist(), species_code)
                                if result is not None:
                                    all_results.append(result)
                                    logger.info(f"Successfully processed and uploaded batch {batch_num}/{total_batches}")
                            except Exception as e:
                                logger.error(f"Error processing batch {batch_num}/{total_batches}: {str(e)}", exc_info=True)
                                continue
                    else:
                        logger.warning(f"No CHR numbers found for species {species_code}, usage {usage_code}")
                        
                except Exception as e:
                    logger.error(f"Error processing usage code {usage_code} for species {species_code}: {str(e)}", exc_info=True)
                    continue
            
            # Combine all results
            if all_results:
                logger.info(f"Combining results for species {species_code}...")
                final_df = pd.concat(all_results, ignore_index=True)
                logger.info(f"Successfully processed all data for species {species_code}")
                return final_df
            
            logger.warning(f"No results generated for species code {species_code}")
            return None
            
        except Exception as e:
            logger.error(f"Error processing species code {species_code}: {str(e)}", exc_info=True)
            return None

    async def sync(self) -> Optional[int]:
        """Full sync process: fetch and store"""
        try:
            df = await self.fetch()
            if await self.store(df):
                return len(df)
            return None
        except Exception as e:
            logger.error(f"Sync failed for {self.source_id}: {str(e)}")
            return None

================
File: src/sources/parsers/chr_species.py
================
"""Parser for CHR/species code combinations."""
import logging
from typing import Dict, List, Any, Optional
from zeep import Client
from zeep.transports import Transport
from requests import Session
from zeep.wsse.username import UsernameToken
import certifi
import pandas as pd
from datetime import datetime
from google.cloud import secretmanager
from ..base import BaseSource
import time
import asyncio

logger = logging.getLogger(__name__)

class CHRSpeciesParser(BaseSource):
    """Parser for CHR/species code combinations."""
    
    WSDL_URLS = {
        'stamdata': 'https://ws.fvst.dk/service/CHR_stamdataWS?WSDL',
        'besaetning': 'https://ws.fvst.dk/service/CHR_besaetningWS?wsdl',
        'ejendom': 'https://ws.fvst.dk/service/CHR_ejendomWS?wsdl'
    }
    
    # Constants
    BATCH_SIZE = 1000  # Maximum batch size for the API
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # Get credentials from Google Secret Manager
        client = secretmanager.SecretManagerServiceClient()
        project_id = "landbrugsdata-1"
        
        def get_secret(secret_id: str) -> str:
            name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
            response = client.access_secret_version(request={"name": name})
            return response.payload.data.decode("UTF-8")
        
        # Get credentials
        self.username = get_secret('fvm_username')
        self.password = get_secret('fvm_password')
        
        # Initialize session with SSL verification
        session = Session()
        session.verify = certifi.where()
        self.transport = Transport(session=session)
        
        # Initialize clients for different services
        self.clients = {}
        for service_name, url in self.WSDL_URLS.items():
            self.clients[service_name] = Client(
                url,
                transport=self.transport,
                wsse=UsernameToken(self.username, self.password)
            )
        
        logger.info("Initialized CHR species parser")

    @property
    def source_id(self) -> str:
        return "chr_species"

    def get_species_usage_combinations(self) -> pd.DataFrame:
        """Get all valid species and usage code combinations."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': 'species_usage_combinations'
                },
                'Request': {}
            }
            
            result = self.clients['stamdata'].service.ListDyrearterMedBrugsarter(request)
            
            if not hasattr(result, 'Response') or not result.Response:
                logger.error("No response from ListDyrearterMedBrugsarter")
                return pd.DataFrame()
            
            combinations = []
            for combo in result.Response:
                combinations.append({
                    'species_code': self.safe_int(getattr(combo, 'DyreArtKode', None)),
                    'species_text': self.safe_str(getattr(combo, 'DyreArtTekst', None)),
                    'usage_code': self.safe_int(getattr(combo, 'BrugsArtKode', None)),
                    'usage_text': self.safe_str(getattr(combo, 'BrugsArtTekst', None))
                })
            
            df = pd.DataFrame(combinations)
            self._upload_to_storage(df, 'species_usage_combinations')
            return df
            
        except Exception as e:
            logger.error(f"Error getting species usage combinations: {str(e)}")
            return pd.DataFrame()

    def get_herd_numbers(self, species_code: int, usage_code: Optional[int] = None) -> pd.DataFrame:
        """Get all herd numbers for a specific species code and optional usage code."""
        try:
            all_herds = set()  # Use a set to detect duplicates
            page = 1
            max_retries = 3
            bes_nr_fra = 0
            
            while True:
                logger.info(f"Fetching page {page} for species {species_code}, usage {usage_code} "
                          f"(starting from herd {bes_nr_fra}, current count: {len(all_herds)})")
                
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        request = {
                            'GLRCHRWSInfoInbound': {
                                'BrugerNavn': self.username,
                                'KlientId': 'LandbrugsData',
                                'SessionId': '1',
                                'IPAdresse': '',
                                'TrackID': f'list_herds_{species_code}_{usage_code}_{bes_nr_fra}'
                            },
                            'Request': {
                                'DyreArtKode': str(species_code),
                                'BrugsArtKode': str(usage_code) if usage_code is not None else None,
                                'BesNrFra': str(bes_nr_fra) if bes_nr_fra > 0 else None
                            }
                        }
                        
                        result = self.clients['besaetning'].service.listBesaetningerMedBrugsart(request)
                        break
                    except Exception as e:
                        retry_count += 1
                        if retry_count == max_retries:
                            logger.error(f"Failed to fetch page {page} after {max_retries} retries: {str(e)}")
                            return pd.DataFrame(
                                [{'herd_number': h, 'species_code': species_code, 'usage_code': usage_code} for h in sorted(list(all_herds))]
                            )
                        logger.warning(f"Retry {retry_count}/{max_retries} for page {page}: {str(e)}")
                        time.sleep(1)
                
                if not hasattr(result, 'Response'):
                    logger.warning(f"No Response attribute in result for page {page}")
                    break
                
                # Get metadata
                fra_bes_nr = getattr(result.Response, 'FraBesNr', None)
                til_bes_nr = getattr(result.Response, 'TilBesNr', None)
                api_antal = getattr(result.Response, 'antal', None)
                has_more = getattr(result.Response, 'FlereBesaetninger', False)
                
                logger.info(f"Response metadata: FraBesNr={fra_bes_nr}, TilBesNr={til_bes_nr}, "
                          f"antal={api_antal}, FlereBesaetninger={has_more}")
                
                if not hasattr(result.Response, 'BesaetningsnummerListe'):
                    logger.warning(f"No BesaetningsnummerListe in response for page {page}")
                    break
                
                if not hasattr(result.Response.BesaetningsnummerListe, 'BesNrListe'):
                    logger.warning(f"No BesNrListe in BesaetningsnummerListe for page {page}")
                    break
                
                # Get the list of herd numbers
                herd_numbers = result.Response.BesaetningsnummerListe.BesNrListe
                if not herd_numbers:
                    logger.info(f"No herds found on page {page}")
                    break
                
                # Convert to list if needed
                if not isinstance(herd_numbers, list):
                    herd_numbers = [herd_numbers]
                
                # Process numbers
                new_herds = 0
                duplicates = 0
                page_min = float('inf')
                page_max = 0
                
                for herd in herd_numbers:
                    if herd is not None:
                        try:
                            herd_int = int(herd)
                            if herd_int > 0:
                                page_min = min(page_min, herd_int)
                                page_max = max(page_max, herd_int)
                                if herd_int not in all_herds:
                                    all_herds.add(herd_int)
                                    new_herds += 1
                                else:
                                    duplicates += 1
                        except (ValueError, TypeError):
                            continue
                
                logger.info(f"Page {page} stats: {new_herds} new herds, {duplicates} duplicates")
                if new_herds > 0:
                    logger.info(f"Page range: {page_min} - {page_max}")
                
                # Check if we should continue
                if new_herds == 0:
                    logger.info("No new herds found, stopping pagination")
                    break
                
                if has_more and til_bes_nr is not None:
                    try:
                        next_bes_nr = int(til_bes_nr) + 1
                        if next_bes_nr <= bes_nr_fra:
                            logger.warning(f"Next starting point ({next_bes_nr}) is not greater than current ({bes_nr_fra})")
                            break
                        bes_nr_fra = next_bes_nr
                        page += 1
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid TilBesNr value: {til_bes_nr}")
                        break
                else:
                    logger.info("No more pages indicated by API")
                    break
                
                # Add small delay to avoid overwhelming the service
                time.sleep(0.2)
            
            total_herds = len(all_herds)
            logger.info(f"Found {total_herds} total herds across {page} pages")
            if all_herds:
                logger.info(f"Overall range: {min(all_herds)} - {max(all_herds)}")
            
            # Convert to DataFrame and upload
            df = pd.DataFrame([{'herd_number': h, 'species_code': species_code, 'usage_code': usage_code} for h in sorted(list(all_herds))])
            if not df.empty:
                self._upload_to_storage(df, f'herd_numbers_{species_code}_{usage_code}')
            return df
            
        except Exception as e:
            logger.error(f"Error getting herd numbers for species code {species_code}: {str(e)}")
            return pd.DataFrame()

    async def get_chr_numbers_async(self, species_code: int, usage_code: Optional[int] = None) -> pd.DataFrame:
        """Get all CHR numbers for a specific species code and optional usage code."""
        try:
            # First get all herd numbers
            herds_df = self.get_herd_numbers(species_code, usage_code)
            if herds_df.empty:
                return pd.DataFrame()
            
            total_herds = len(herds_df)
            logger.info(f"Processing {total_herds} herds for species {species_code}, usage {usage_code}")
            
            # Process herds in parallel using asyncio
            properties = []
            semaphore = asyncio.Semaphore(10)  # Limit concurrent requests
            
            async def process_herd(idx: int, herd_number: int):
                async with semaphore:
                    if idx % 100 == 0:
                        logger.info(f"Processing herd {idx}/{total_herds}")
                    
                    request = {
                        'GLRCHRWSInfoInbound': {
                            'BrugerNavn': self.username,
                            'KlientId': 'LandbrugsData',
                            'SessionId': '1',
                            'IPAdresse': '',
                            'TrackID': f'herd_details_{herd_number}'
                        },
                        'Request': {
                            'BesaetningsNummer': str(herd_number),
                            'DyreArtKode': str(species_code)
                        }
                    }
                    
                    try:
                        result = self.clients['besaetning'].service.hentStamoplysninger(request)
                        
                        if hasattr(result, 'Response') and result.Response:
                            response_data = result.Response[0].Besaetning
                            chr_number = self.safe_int(getattr(response_data, 'ChrNummer', None))
                            if chr_number:
                                return {
                                    'chr_number': chr_number,
                                    'species_code': species_code,
                                    'herd_number': herd_number
                                }
                    except Exception as e:
                        logger.error(f"Error processing herd {herd_number}: {str(e)}")
                    
                    await asyncio.sleep(0.05)  # Reduced delay
                    return None
            
            # Create tasks for all herds
            tasks = []
            for idx, herd_number in enumerate(herds_df['herd_number'].tolist(), 1):
                tasks.append(process_herd(idx, herd_number))
            
            # Wait for all tasks to complete
            results = await asyncio.gather(*tasks)
            properties = [r for r in results if r is not None]
            
            logger.info(f"Finished processing {total_herds} herds, found {len(properties)} CHR numbers")
            df = pd.DataFrame(properties)
            if not df.empty:
                self._upload_to_storage(df, f'chr_numbers_{species_code}')
            return df
            
        except Exception as e:
            logger.error(f"Error getting CHR numbers for species code {species_code}: {str(e)}")
            return pd.DataFrame()

    def get_chr_numbers(self, species_code: int, usage_code: Optional[int] = None) -> pd.DataFrame:
        """Synchronous wrapper for get_chr_numbers_async."""
        return asyncio.run(self.get_chr_numbers_async(species_code, usage_code))

    def safe_str(self, value: Any) -> Optional[str]:
        """Safely convert value to string, return None if empty."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return val_str if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None
            
    def safe_int(self, value: Any) -> Optional[int]:
        """Safely convert value to int, return None if not possible."""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return int(val_str) if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    async def get_species_usage_combinations_async(self) -> pd.DataFrame:
        """Get all species/usage combinations."""
        try:
            # Get all species codes
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': 'species_codes'
                }
            }
            
            result = self.clients['stamdata'].service.listDyreArt(request)
            
            if not hasattr(result, 'Response') or not result.Response:
                logger.error("No response from listDyreArt")
                return pd.DataFrame()
            
            species_list = result.Response.DyreArtListe.DyreArt
            if not isinstance(species_list, list):
                species_list = [species_list]
            
            # Process each species code
            combinations = []
            for species in species_list:
                species_code = self.safe_int(getattr(species, 'DyreArtKode', None))
                species_text = self.safe_str(getattr(species, 'DyreArtTekst', None))
                
                if not species_code:
                    continue
                
                # Get usage codes for this species
                usage_request = {
                    'GLRCHRWSInfoInbound': {
                        'BrugerNavn': self.username,
                        'KlientId': 'LandbrugsData',
                        'SessionId': '1',
                        'IPAdresse': '',
                        'TrackID': f'usage_codes_{species_code}'
                    },
                    'Request': {
                        'DyreArtKode': str(species_code)
                    }
                }
                
                usage_result = self.clients['stamdata'].service.listBrugsArt(usage_request)
                
                if hasattr(usage_result, 'Response') and usage_result.Response:
                    usage_list = usage_result.Response.BrugsArtListe.BrugsArt
                    if not isinstance(usage_list, list):
                        usage_list = [usage_list]
                    
                    for usage in usage_list:
                        usage_code = self.safe_int(getattr(usage, 'BrugsArtKode', None))
                        usage_text = self.safe_str(getattr(usage, 'BrugsArtTekst', None))
                        
                        if usage_code:
                            combinations.append({
                                'species_code': species_code,
                                'species_text': species_text,
                                'usage_code': usage_code,
                                'usage_text': usage_text
                            })
            
            df = pd.DataFrame(combinations)
            if not df.empty:
                await self.store(df, 'species_usage_combinations')
            return df
            
        except Exception as e:
            logger.error(f"Error getting species/usage combinations: {str(e)}")
            return pd.DataFrame()

    async def get_herd_numbers_async(self, species_code: int, usage_code: Optional[int] = None) -> pd.DataFrame:
        """Get all herd numbers for a given species code and optional usage code."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': f'herd_numbers_{species_code}_{usage_code}'
                },
                'Request': {
                    'DyreArtKode': str(species_code)
                }
            }
            
            if usage_code:
                request['Request']['BrugsArtKode'] = str(usage_code)
            
            result = self.clients['besaetning'].service.listBesaetningsNummer(request)
            
            if not hasattr(result, 'Response') or not result.Response:
                logger.error(f"No response for species {species_code}, usage {usage_code}")
                return pd.DataFrame()
            
            herds = result.Response.BesaetningsNummerListe.BesaetningsNummer
            if not isinstance(herds, list):
                herds = [herds]
            
            df = pd.DataFrame([{
                'herd_number': self.safe_int(getattr(herd, 'BesaetningsNummer', None)),
                'species_code': species_code,
                'usage_code': usage_code
            } for herd in herds])
            
            if not df.empty:
                await self.store(df, f'herd_numbers_{species_code}_{usage_code}')
            return df
            
        except Exception as e:
            logger.error(f"Error getting herd numbers for species {species_code}, usage {usage_code}: {str(e)}")
            return pd.DataFrame()

    async def fetch(self) -> pd.DataFrame:
        """Fetch data from the CHR web services."""
        try:
            # Get all species/usage combinations
            logger.info("Getting species/usage combinations")
            combinations_df = await self.get_species_usage_combinations_async()
            
            if combinations_df.empty:
                logger.error("Failed to get species/usage combinations")
                return pd.DataFrame()
            
            # Upload the combinations
            self._upload_to_storage(combinations_df, 'species_usage_combinations')
            
            return combinations_df
            
        except Exception as e:
            logger.error(f"Error during fetch: {str(e)}")
            return pd.DataFrame()

    async def sync(self) -> Optional[int]:
        """Full sync process: fetch and store"""
        try:
            df = await self.fetch()
            if await self.store(df):
                return len(df)
            return None
        except Exception as e:
            logger.error(f"Sync failed for {self.source_id}: {str(e)}")
            return None

    def fetch_sync(self) -> pd.DataFrame:
        """Synchronous version of fetch."""
        import asyncio
        return asyncio.run(self.fetch())

================
File: src/sources/parsers/herd_data.py
================
import logging
from typing import Dict, List, Any, Optional
from zeep import Client
from zeep.transports import Transport
from requests import Session
from zeep.wsse.username import UsernameToken
import certifi
from datetime import datetime
import pandas as pd
from ..base import BaseSource
import time
import os
from google.cloud import secretmanager
from pyproj import Transformer
import asyncio

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
logging.getLogger('zeep').setLevel(logging.WARNING)

class HerdDataParser(BaseSource):
    """Parser for CHR herd data"""
    
    WSDL_URLS = {
        'stamdata': 'https://ws.fvst.dk/service/CHR_stamdataWS?WSDL',
        'besaetning': 'https://ws.fvst.dk/service/CHR_besaetningWS?wsdl',
        'ejendom': 'https://ws.fvst.dk/service/CHR_ejendomWS?wsdl',
        'ejer': 'https://ws.fvst.dk/service/CHR_ejerWS?wsdl',
    }
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # Get credentials from Google Secret Manager
        client = secretmanager.SecretManagerServiceClient()
        project_id = "landbrugsdata-1"
        
        def get_secret(secret_id: str) -> str:
            name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
            response = client.access_secret_version(request={"name": name})
            return response.payload.data.decode("UTF-8")
        
        # Get credentials
        self.username = get_secret('fvm_username')
        self.password = get_secret('fvm_password')
        
        # Initialize session with SSL verification
        session = Session()
        session.verify = certifi.where()
        self.transport = Transport(session=session)
        
        # Initialize clients for different services
        self.clients = {}
        for service_name, url in self.WSDL_URLS.items():
            self.clients[service_name] = Client(
                url,
                transport=self.transport,
                wsse=UsernameToken(self.username, self.password)
            )
        
        logger.info("Initialized CHR web service clients")

    @property
    def source_id(self) -> str:
        return "herd_data"

    async def fetch(self) -> pd.DataFrame:
        """Fetch data from the CHR web services"""
        return self.fetch_sync()  # Call the sync version

    def fetch_sync(self) -> pd.DataFrame:
        """Synchronous version of fetch"""
        results = []
        total_combinations = 0
        current_combination = 0
        
        try:
            # Track unique entities to avoid duplicates
            seen_properties = set()
            seen_owners = set()
            seen_users = set()
            seen_practices = set()
            
            # Data containers
            properties_data = []
            owners_data = []
            users_data = []
            practices_data = []
            herds_data = []
            
            # 1. Fetch stamdata (reference data)
            logger.info("Fetching stamdata (reference data)...")
            combinations = self.get_species_usage_combinations()
            if not combinations:
                logger.error("No species/usage combinations found")
                return pd.DataFrame()
                
            total_combinations = len(combinations)
            logger.info(f"Found {total_combinations} species/usage combinations to process")
            
            stamdata_df = pd.DataFrame(combinations)
            stamdata_df.columns = [c.lower() for c in stamdata_df.columns]
            self._upload_to_storage(stamdata_df, 'stamdata')
            results.append(('stamdata', len(stamdata_df)))
            
            # 2. Fetch all other data
            logger.info("Fetching herd and related data...")
            batch_size = 1000
            processed = 0
            
            for combo in combinations:
                current_combination += 1
                species_code = combo['species_code']
                usage_code = combo['usage_code']
                
                logger.info(f"Processing combination {current_combination}/{total_combinations}: "
                          f"species={species_code} ({combo['species_text']}), "
                          f"usage={usage_code} ({combo['usage_text']})")
                
                try:
                    # For testing, limit to 100 herds per combination
                    herds = self.get_herds_for_combination(species_code, usage_code, limit=100)
                    if not herds:
                        logger.info(f"No herds found for species {species_code}, usage {usage_code}")
                        continue
                        
                    total_herds = len(herds)
                    logger.info(f"Found {total_herds} herds for species {species_code}, usage {usage_code}")
                    
                    for herd_idx, herd_number in enumerate(herds, 1):
                        try:
                            logger.info(f"[{current_combination}/{total_combinations}] "
                                      f"Processing herd {herd_idx}/{total_herds}: {herd_number} "
                                      f"(species={species_code}, usage={usage_code})")
                            
                            details = self.get_herd_details(herd_number, species_code)
                            if not details:
                                logger.warning(f"No details found for herd {herd_number}")
                                continue
                            
                            chr_number = details.get('chr_number')
                            if not chr_number:
                                continue
                                
                            # Extract base herd data with animal counts
                            if 'herd_sizes' in details and details['herd_sizes']:
                                for size in details['herd_sizes']:
                                    if not size['type'].lower().startswith('i alt'):  # Skip total rows
                                        herd_row = {
                                            # Primary key
                                            'herd_number': details.get('herd_number'),
                                            'chr_number': chr_number,
                                            
                                            # Herd details
                                            'species_code': details.get('species_code'),
                                            'species_text': details.get('species_text'),
                                            'usage_code': details.get('usage_code'),
                                            'usage_text': details.get('usage_text'),
                                            'business_type_code': details.get('business_type_code'),
                                            'business_type': details.get('business_type'),
                                            'herd_type_code': details.get('herd_type_code'),
                                            'herd_type': details.get('herd_type'),
                                            'trade_code': details.get('trade_code'),
                                            'trade_text': details.get('trade_text'),
                                            'is_organic': details.get('is_organic'),
                                            'herd_status': details.get('herd_status'),
                                            
                                            # Dates
                                            'creation_date': details.get('creation_date'),
                                            'last_update': details.get('last_update'),
                                            'end_date': details.get('end_date'),
                                            'last_size_update': details.get('last_size_update'),
                                            'fetched_at': details.get('fetched_at'),
                                            
                                            # Animal counts
                                            'animal_type': size['type'],
                                            'count': size['size']
                                        }
                                        herds_data.append(herd_row)
                            
                            # Handle property details
                            if chr_number not in seen_properties:
                                property_details = self.get_property_details(chr_number)
                                if property_details:
                                    # Convert coordinates
                                    coords = property_details.get('stable_coordinates_utm32', {})
                                    if coords:
                                        lat, lon = self._convert_coordinates(coords)
                                        if lat and lon:
                                            property_details['latitude'] = lat
                                            property_details['longitude'] = lon
                                    
                                    # Add primary key
                                    property_details['chr_number'] = chr_number
                                    properties_data.append(property_details)
                                    seen_properties.add(chr_number)
                            
                            # Handle owner info
                            if 'owner' in details and details['owner']:
                                owner = details['owner']
                                owner_id = owner.get('cvr_number') or owner.get('cpr_number')
                                if owner_id:
                                    owner_key = f"{owner_id}_{chr_number}"
                                    if owner_key not in seen_owners:
                                        owners_data.append({
                                            # Primary keys
                                            'cvr_number': owner.get('cvr_number'),
                                            'cpr_number': owner.get('cpr_number'),
                                            'chr_number': chr_number,  # Link back to property
                                            
                                            # Contact info
                                            'name': owner.get('name'),
                                            'address': owner.get('address'),
                                            'city': owner.get('city'),
                                            'postal_code': owner.get('postal_code'),
                                            'postal_district': owner.get('postal_district'),
                                            'municipality_code': owner.get('municipality_code'),
                                            'municipality_name': owner.get('municipality_name'),
                                            'country': owner.get('country'),
                                            'phone': owner.get('phone'),
                                            'mobile': owner.get('mobile'),
                                            'email': owner.get('email'),
                                            
                                            # Privacy flags
                                            'address_protection': owner.get('address_protection', 0),
                                            'advertising_protection': owner.get('advertising_protection', 0)
                                        })
                                        seen_owners.add(owner_key)
                            
                            # Handle user info
                            if 'user' in details and details['user']:
                                user = details['user']
                                user_id = user.get('cvr_number') or user.get('cpr_number')
                                if user_id:
                                    user_key = f"{user_id}_{chr_number}"
                                    if user_key not in seen_users:
                                        users_data.append({
                                            # Primary keys
                                            'cvr_number': user.get('cvr_number'),
                                            'cpr_number': user.get('cpr_number'),
                                            'chr_number': chr_number,  # Link back to property
                                            
                                            # Contact info
                                            'name': user.get('name'),
                                            'address': user.get('address'),
                                            'city': user.get('city'),
                                            'postal_code': user.get('postal_code'),
                                            'postal_district': user.get('postal_district'),
                                            'municipality_code': user.get('municipality_code'),
                                            'municipality_name': user.get('municipality_name'),
                                            'country': user.get('country'),
                                            'phone': user.get('phone'),
                                            'mobile': user.get('mobile'),
                                            'email': user.get('email'),
                                            
                                            # Privacy flags
                                            'address_protection': user.get('address_protection', 0),
                                            'advertising_protection': user.get('advertising_protection', 0)
                                        })
                                        seen_users.add(user_key)
                            
                            # Handle veterinary practice info
                            if 'veterinary_practice' in details and details['veterinary_practice']:
                                practice = details['veterinary_practice']
                                practice_id = practice.get('number')
                                if practice_id:
                                    practice_key = f"{practice_id}_{chr_number}"
                                    if practice_key not in seen_practices:
                                        practices_data.append({
                                            # Primary key
                                            'number': practice_id,
                                            'chr_number': chr_number,  # Link back to property
                                            
                                            # Practice details
                                            'name': practice.get('name'),
                                            'address': practice.get('address'),
                                            'city': practice.get('city'),
                                            'postal_code': practice.get('postal_code'),
                                            'postal_district': practice.get('postal_district'),
                                            'phone': practice.get('phone'),
                                            'mobile': practice.get('mobile'),
                                            'email': practice.get('email')
                                        })
                                        seen_practices.add(practice_key)
                            
                            processed += 1
                            
                            # Upload batches
                            if processed % batch_size == 0:
                                self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
                                herds_data = []
                                properties_data = []
                                owners_data = []
                                users_data = []
                                practices_data = []
                            
                        except Exception as e:
                            logger.error(f"Error processing herd {herd_number}: {str(e)}")
                            continue
                
                except Exception as e:
                    logger.error(f"Error processing combination {current_combination}/{total_combinations}: {str(e)}")
                    continue
            
            # Upload remaining data
            if any([herds_data, properties_data, owners_data, users_data, practices_data]):
                self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
            
            return pd.DataFrame(results, columns=['table_name', 'record_count'])
            
        except Exception as e:
            logger.error(f"Error in herd data sync: {str(e)}", exc_info=True)
            raise

    async def sync(self) -> Optional[int]:
        """Sync herd data to Cloud Storage"""
        try:
            logger.info("Starting herd data sync...")
            
            # Get all valid species/usage combinations
            combinations = self.get_species_usage_combinations()
            if not combinations:
                logger.warning("No species/usage combinations found")
                return 0
            
            logger.info(f"Found {len(combinations)} species/usage combinations to process")
            
            # Track all unique entities across all combinations
            all_herds = set()
            seen_properties = set()
            seen_owners = set()
            seen_users = set()
            seen_practices = set()
            
            # Data containers
            herds_data = []      # Basic herd info with animal counts
            properties_data = [] # Property details
            owners_data = []     # Owner information
            users_data = []      # User information
            practices_data = []  # Veterinary practice information
            
            # Process combinations in chunks
            chunk_size = 5  # Process 5 combinations at a time
            for i in range(0, len(combinations), chunk_size):
                chunk = combinations[i:i + chunk_size]
                
                # Process each combination in parallel
                tasks = []
                for combo in chunk:
                    species_code = combo['species_code']
                    usage_code = combo['usage_code']
                    
                    # Get herds for this combination
                    herd_numbers = self.get_herds_for_combination(species_code, usage_code)
                    if not herd_numbers:
                        continue
                    
                    # Process herds in parallel
                    for herd_number in herd_numbers:
                        if herd_number in all_herds:
                            continue
                        
                        # Create task for processing this herd
                        task = asyncio.create_task(self._process_herd(
                            herd_number, 
                            species_code, 
                            all_herds,
                            seen_properties,
                            seen_owners,
                            seen_users,
                            seen_practices,
                            herds_data,
                            properties_data,
                            owners_data,
                            users_data,
                            practices_data
                        ))
                        tasks.append(task)
                        
                        # Process in batches of 50 herds
                        if len(tasks) >= 50:
                            await asyncio.gather(*tasks)
                            tasks = []
                            
                            # Upload data if we have enough
                            if len(herds_data) >= 5000:
                                self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
                                herds_data = []
                                properties_data = []
                                owners_data = []
                                users_data = []
                                practices_data = []
                
                # Process any remaining tasks
                if tasks:
                    await asyncio.gather(*tasks)
            
            # Upload any remaining data
            if any([herds_data, properties_data, owners_data, users_data, practices_data]):
                self._upload_batch_data(herds_data, properties_data, owners_data, users_data, practices_data)
            
            logger.info("Herd data sync completed successfully")
            return len(all_herds)
            
        except Exception as e:
            logger.error(f"Error in herd data sync: {str(e)}")
            raise

    async def _process_herd(self, herd_number: int, species_code: int,
                          all_herds: set, seen_properties: set, seen_owners: set,
                          seen_users: set, seen_practices: set,
                          herds_data: list, properties_data: list,
                          owners_data: list, users_data: list, practices_data: list) -> None:
        """Process a single herd asynchronously."""
        try:
            # Get herd details
            details = self.get_herd_details(herd_number, species_code)
            if not details:
                return
            
            chr_number = details.get('chr_number')
            if not chr_number:
                return
            
            # Extract base herd data with animal counts
            if 'herd_sizes' in details and details['herd_sizes']:
                for size in details['herd_sizes']:
                    if not size['type'].lower().startswith('i alt'):
                        herd_row = {
                            'herd_number': details.get('herd_number'),
                            'chr_number': chr_number,
                            'species_code': details.get('species_code'),
                            'species_text': details.get('species_text'),
                            'usage_code': details.get('usage_code'),
                            'usage_text': details.get('usage_text'),
                            'business_type_code': details.get('business_type_code'),
                            'business_type': details.get('business_type'),
                            'herd_type_code': details.get('herd_type_code'),
                            'herd_type': details.get('herd_type'),
                            'trade_code': details.get('trade_code'),
                            'trade_text': details.get('trade_text'),
                            'is_organic': details.get('is_organic'),
                            'herd_status': details.get('herd_status'),
                            'creation_date': details.get('creation_date'),
                            'last_update': details.get('last_update'),
                            'end_date': details.get('end_date'),
                            'last_size_update': details.get('last_size_update'),
                            'fetched_at': details.get('fetched_at'),
                            'animal_type': size['type'],
                            'count': size['size']
                        }
                        herds_data.append(herd_row)
            
            # Handle property details
            if chr_number not in seen_properties:
                property_details = self.get_property_details(chr_number)
                if property_details:
                    properties_data.append(property_details)
                    seen_properties.add(chr_number)
            
            # Handle owner info
            if 'owner' in details and details['owner']:
                owner = details['owner']
                owner_id = owner.get('cvr_number') or owner.get('cpr_number')
                if owner_id:
                    owner_key = f"{owner_id}_{chr_number}"
                    if owner_key not in seen_owners:
                        owners_data.append({
                            'cvr_number': owner.get('cvr_number'),
                            'cpr_number': owner.get('cpr_number'),
                            'chr_number': chr_number,
                            'name': owner.get('name'),
                            'address': owner.get('address'),
                            'city': owner.get('city'),
                            'postal_code': owner.get('postal_code'),
                            'postal_district': owner.get('postal_district'),
                            'municipality_code': owner.get('municipality_code'),
                            'municipality_name': owner.get('municipality_name'),
                            'country': owner.get('country'),
                            'phone': owner.get('phone'),
                            'mobile': owner.get('mobile'),
                            'email': owner.get('email'),
                            'address_protection': owner.get('address_protection', 0),
                            'advertising_protection': owner.get('advertising_protection', 0)
                        })
                        seen_owners.add(owner_key)
            
            # Handle user info
            if 'user' in details and details['user']:
                user = details['user']
                user_id = user.get('cvr_number') or user.get('cpr_number')
                if user_id:
                    user_key = f"{user_id}_{chr_number}"
                    if user_key not in seen_users:
                        users_data.append({
                            'cvr_number': user.get('cvr_number'),
                            'cpr_number': user.get('cpr_number'),
                            'chr_number': chr_number,
                            'name': user.get('name'),
                            'address': user.get('address'),
                            'city': user.get('city'),
                            'postal_code': user.get('postal_code'),
                            'postal_district': user.get('postal_district'),
                            'municipality_code': user.get('municipality_code'),
                            'municipality_name': user.get('municipality_name'),
                            'country': user.get('country'),
                            'phone': user.get('phone'),
                            'mobile': user.get('mobile'),
                            'email': user.get('email'),
                            'address_protection': user.get('address_protection', 0),
                            'advertising_protection': user.get('advertising_protection', 0)
                        })
                        seen_users.add(user_key)
            
            # Handle veterinary practice info
            if 'veterinary_practice' in details and details['veterinary_practice']:
                practice = details['veterinary_practice']
                practice_id = practice.get('number')
                if practice_id:
                    practice_key = f"{practice_id}_{chr_number}"
                    if practice_key not in seen_practices:
                        practices_data.append({
                            'number': practice_id,
                            'chr_number': chr_number,
                            'name': practice.get('name'),
                            'address': practice.get('address'),
                            'city': practice.get('city'),
                            'postal_code': practice.get('postal_code'),
                            'postal_district': practice.get('postal_district'),
                            'phone': practice.get('phone'),
                            'mobile': practice.get('mobile'),
                            'email': practice.get('email')
                        })
                        seen_practices.add(practice_key)
            
            # Mark herd as processed
            all_herds.add(herd_number)
            
        except Exception as e:
            logger.error(f"Error processing herd {herd_number}: {str(e)}")
            return

    def get_species_usage_combinations(self) -> List[Dict[str, int]]:
        """Get valid species and usage type combinations."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': 'list_combinations'
                },
                'Request': {}
            }
            
            logger.debug("Sending request to list species/usage combinations")
            result = self.clients['stamdata'].service.ListDyrearterMedBrugsarter(request)
            
            # Debug: Print raw response structure
            logger.info(f"Raw response attributes: {dir(result)}")
            if hasattr(result, 'Response'):
                logger.info(f"Response attributes: {dir(result.Response)}")
                # Log the first few combinations to understand structure
                for i, combo in enumerate(result.Response[:5]):
                    logger.info(f"Sample combination {i}: species={getattr(combo, 'DyreArtKode', None)}, "
                              f"species_text={getattr(combo, 'DyreArtTekst', None)}, "
                              f"usage={getattr(combo, 'BrugsArtKode', None)}, "
                              f"usage_text={getattr(combo, 'BrugsArtTekst', None)}")
            
            combinations = []
            if hasattr(result, 'Response') and result.Response:
                # The combinations are in the Response array
                for combo in result.Response:
                    species_code = getattr(combo, 'DyreArtKode', None)
                    species_text = getattr(combo, 'DyreArtTekst', '')
                    usage_code = getattr(combo, 'BrugsArtKode', None)
                    usage_text = getattr(combo, 'BrugsArtTekst', '')
                    
                    if species_code is not None and usage_code is not None:
                        try:
                            species_int = int(species_code)
                            usage_int = int(usage_code)
                            combinations.append({
                                'species_code': species_int,
                                'species_text': species_text,
                                'usage_code': usage_int,
                                'usage_text': usage_text
                            })
                            logger.debug(f"Added combination: species={species_int} ({species_text}), "
                                       f"usage={usage_int} ({usage_text})")
                        except (ValueError, TypeError) as e:
                            logger.warning(f"Invalid combination found: species={species_code}, usage={usage_code}: {e}")
                    else:
                        logger.warning(f"Incomplete combination found: species={species_code}, usage={usage_code}")
                        
            logger.info(f"Found {len(combinations)} valid species/usage combinations")
            return combinations
            
        except Exception as e:
            logger.error(f"Error getting species/usage combinations: {str(e)}")
            return []

    def safe_str(self, value: Any) -> Optional[str]:
        """Safely convert value to string, return None if empty"""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return val_str if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None
            
    def safe_int(self, value: Any) -> Optional[int]:
        """Safely convert value to int, return None if not possible"""
        if value is None:
            return None
        try:
            val_str = str(value).strip()
            return int(val_str) if val_str else None
        except (ValueError, TypeError, AttributeError):
            return None

    def get_herd_details(self, herd_number: int, species_code: int) -> Dict[str, Any]:
        """Get detailed information for a specific herd."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': f'herd_details_{herd_number}'
                },
                'Request': {
                    'BesaetningsNummer': str(herd_number),
                    'DyreArtKode': str(species_code)
                }
            }
            
            logger.debug(f"Fetching details for herd {herd_number}")
            result = self.clients['besaetning'].service.hentStamoplysninger(request)
            
            if not hasattr(result, 'Response') or not result.Response:
                logger.warning(f"No data found for herd {herd_number}")
                return {}
            
            # Response is a list with one item containing a Besaetning object
            response_data = result.Response[0].Besaetning
            
            # Parse all available fields from the response
            details = {
                # Basic identification
                'herd_number': herd_number,  # We already know this is valid
                'chr_number': self.safe_int(getattr(response_data, 'ChrNummer', None)),
                
                # Species and usage information
                'species_code': species_code,  # We already know this is valid
                'species_text': self.safe_str(getattr(response_data, 'DyreArtTekst', None)),
                'usage_code': self.safe_int(getattr(response_data, 'BrugsArtKode', None)),
                'usage_text': self.safe_str(getattr(response_data, 'BrugsArtTekst', None)),
                
                # Business information
                'business_type_code': self.safe_int(getattr(response_data, 'VirksomhedsArtKode', None)),
                'business_type': self.safe_str(getattr(response_data, 'VirksomhedsArtTekst', None)),
                'herd_type_code': self.safe_int(getattr(response_data, 'BesaetningsTypeKode', None)),
                'herd_type': self.safe_str(getattr(response_data, 'BesaetningsTypeTekst', None)),
                'trade_code': self.safe_int(getattr(response_data, 'OmsaetningsKode', None)),
                'trade_text': self.safe_str(getattr(response_data, 'OmsaetningsTekst', None)),
                'is_organic': int(getattr(response_data, 'Oekologisk', 'Nej') == 'Ja'),
                'herd_status': self.safe_str(getattr(response_data, 'BesaetningsStatus', None)),
                
                # Dates
                'creation_date': self._parse_date(getattr(response_data, 'DatoOpret', None)),
                'last_update': self._parse_date(getattr(response_data, 'DatoOpdatering', None)),
                'end_date': self._parse_date(getattr(response_data, 'DatoOphoer', None)),
                'last_size_update': self._parse_date(getattr(response_data, 'BesStrDatoAjourfoert', None)),
                
                # Related entities
                'owner': self._parse_person_info(getattr(response_data, 'Ejer', None)),
                'user': self._parse_person_info(getattr(response_data, 'Bruger', None)),
                'veterinary_practice': self._parse_practice_info(getattr(response_data, 'BesPraksis', None)),
                'delivery_declarations': self._parse_delivery_declarations(getattr(response_data, 'LeveringsErklaeringer', None)),
                
                'fetched_at': datetime.now().isoformat()
            }
            
            # Parse herd sizes if available
            if hasattr(response_data, 'BesStr'):
                details['herd_sizes'] = []
                for size_info in response_data.BesStr:
                    size_type = self.safe_str(getattr(size_info, 'BesaetningsStoerrelseTekst', None))
                    size_value = getattr(size_info, 'BesaetningsStoerrelse', None)
                    if size_type and size_value is not None:
                        size_int = self.safe_int(size_value)
                        if size_int is not None and not size_type.lower().startswith('i alt'):
                            details['herd_sizes'].append({
                                'type': size_type,
                                'size': size_int
                            })
            
            # Remove None values
            return {k: v for k, v in details.items() if v is not None}
            
        except Exception as e:
            logger.error(f"Error getting herd details for {herd_number}: {str(e)}")
            return {}

    def _parse_date(self, date_value: Any) -> Optional[str]:
        """Parse date value to ISO format string"""
        if not date_value:
            return None
            
        try:
            if isinstance(date_value, datetime):
                return date_value.date().isoformat()
                
            if isinstance(date_value, str):
                # The API returns dates in YYYY-MM-DD format
                return date_value
                
            return None
            
        except Exception as e:
            logger.error(f"Error parsing date {date_value}: {str(e)}")
            return None

    def _parse_person_info(self, person_data: Any) -> Optional[Dict[str, Any]]:
        """Parse person (owner/user) information."""
        if not person_data:
            return None
            
        info = {
            'cpr_number': self.safe_str(getattr(person_data, 'CprNummer', None)),
            'cvr_number': self.safe_str(getattr(person_data, 'CvrNummer', None)),
            'name': self.safe_str(getattr(person_data, 'Navn', None)),
            'address': self.safe_str(getattr(person_data, 'Adresse', None)),
            'city': self.safe_str(getattr(person_data, 'ByNavn', None)),
            'postal_code': self.safe_str(getattr(person_data, 'PostNummer', None)),
            'postal_district': self.safe_str(getattr(person_data, 'PostDistrikt', None)),
            'municipality_code': self.safe_int(getattr(person_data, 'KommuneNummer', None)),
            'municipality_name': self.safe_str(getattr(person_data, 'KommuneNavn', None)),
            'country': self.safe_str(getattr(person_data, 'Land', None)),
            'phone': self.safe_str(getattr(person_data, 'TelefonNummer', None)),
            'mobile': self.safe_str(getattr(person_data, 'MobilNummer', None)),
            'email': self.safe_str(getattr(person_data, 'Email', None)),
            'address_protection': int(getattr(person_data, 'Adressebeskyttelse', 'Nej') == 'Ja'),
            'advertising_protection': int(getattr(person_data, 'Reklamebeskyttelse', 'Nej') == 'Ja')
        }
        
        return {k: v for k, v in info.items() if v is not None}
    
    def _parse_practice_info(self, practice_data: Any) -> Optional[Dict[str, Any]]:
        """Parse veterinary practice information."""
        if not practice_data:
            return None
            
        info = {
            'number': self.safe_int(getattr(practice_data, 'PraksisNr', None)),
            'name': self.safe_str(getattr(practice_data, 'PraksisNavn', None)),
            'address': self.safe_str(getattr(practice_data, 'PraksisAdresse', None)),
            'city': self.safe_str(getattr(practice_data, 'PraksisByNavn', None)),
            'postal_code': self.safe_str(getattr(practice_data, 'PraksisPostNummer', None)),
            'postal_district': self.safe_str(getattr(practice_data, 'PraksisPostDistrikt', None)),
            'phone': self.safe_str(getattr(practice_data, 'PraksisTelefonNummer', None)),
            'mobile': self.safe_str(getattr(practice_data, 'PraksisMobilNummer', None)),
            'email': self.safe_str(getattr(practice_data, 'PraksisEmail', None))
        }
        
        return {k: v for k, v in info.items() if v is not None}
    
    def _parse_delivery_declarations(self, declarations_data: Any) -> Optional[List[Dict[str, Any]]]:
        """Parse delivery declarations information."""
        if not declarations_data or not hasattr(declarations_data, 'LeveringsErklaering'):
            return None
            
        declarations = []
        for decl in declarations_data.LeveringsErklaering:
            info = {
                'sender_chr_number': self.safe_int(getattr(decl, 'ChrNummerAfsender', None)),
                'sender_herd_number': self.safe_int(getattr(decl, 'BesaetningsNummerAfsender', None)),
                'receiver_chr_number': self.safe_int(getattr(decl, 'ChrNummerModtager', None)),
                'receiver_herd_number': self.safe_int(getattr(decl, 'BesaetningsNummerModtager', None)),
                'own_production': int(getattr(decl, 'EgenProduktion', 'Nej') == 'Ja'),
                'breeding_only': int(getattr(decl, 'KunAvlsdyr', 'Nej') == 'Ja'),
                'group_delivery': int(getattr(decl, 'IndgaarIGruppevisLevering', 'Nej') == 'Ja'),
                'start_date': self._parse_date(getattr(decl, 'DatoStart', None)),
                'end_date': self._parse_date(getattr(decl, 'DatoSlut', None))
            }
            declarations.append({k: v for k, v in info.items() if v is not None})
            
        return declarations if declarations else None

    def get_herds_for_combination(self, species_code: int, usage_code: int, limit: Optional[int] = None) -> List[int]:
        """Get herd numbers for a species/usage combination with pagination."""
        try:
            all_herds = set()  # Use a set to detect duplicates
            page = 1
            max_retries = 3
            bes_nr_fra = 0
            
            while True:
                logger.info(f"Fetching page {page} for species {species_code}, usage {usage_code} "
                          f"(starting from herd {bes_nr_fra}, current count: {len(all_herds)})")
                
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        request = {
                            'GLRCHRWSInfoInbound': {
                                'BrugerNavn': self.username,
                                'KlientId': 'LandbrugsData',
                                'SessionId': '1',
                                'IPAdresse': '',
                                'TrackID': f'list_herds_{species_code}_{usage_code}_{bes_nr_fra}'
                            },
                            'Request': {
                                'DyreArtKode': str(species_code),
                                'BrugsArtKode': str(usage_code),
                                'BesNrFra': str(bes_nr_fra)
                            }
                        }
                        
                        result = self.clients['besaetning'].service.listBesaetningerMedBrugsart(request)
                        break
                    except Exception as e:
                        retry_count += 1
                        if retry_count == max_retries:
                            logger.error(f"Failed to fetch page {page} after {max_retries} retries: {str(e)}")
                            return sorted(list(all_herds))
                        logger.warning(f"Retry {retry_count}/{max_retries} for page {page}: {str(e)}")
                        time.sleep(1)
                
                if not hasattr(result, 'Response'):
                    logger.warning(f"No Response attribute in result for page {page}")
                    break
                
                # Get metadata
                fra_bes_nr = getattr(result.Response, 'FraBesNr', None)
                til_bes_nr = getattr(result.Response, 'TilBesNr', None)
                api_antal = getattr(result.Response, 'antal', None)
                has_more = getattr(result.Response, 'FlereBesaetninger', False)
                
                logger.info(f"Response metadata: FraBesNr={fra_bes_nr}, TilBesNr={til_bes_nr}, "
                          f"antal={api_antal}, FlereBesaetninger={has_more}")
                
                if not hasattr(result.Response, 'BesaetningsnummerListe'):
                    logger.warning(f"No BesaetningsnummerListe in response for page {page}")
                    break
                
                if not hasattr(result.Response.BesaetningsnummerListe, 'BesNrListe'):
                    logger.warning(f"No BesNrListe in BesaetningsnummerListe for page {page}")
                    break
                
                # Get the list of herd numbers
                herd_numbers = result.Response.BesaetningsnummerListe.BesNrListe
                if not herd_numbers:
                    logger.info(f"No herds found on page {page}")
                    break
                
                # Convert to list if needed
                if not isinstance(herd_numbers, list):
                    herd_numbers = list(herd_numbers)
                
                # Process numbers
                new_herds = 0
                duplicates = 0
                page_min = float('inf')
                page_max = 0
                
                for herd in herd_numbers:
                    if herd is not None:
                        try:
                            herd_int = int(herd)
                            if herd_int > 0:
                                page_min = min(page_min, herd_int)
                                page_max = max(page_max, herd_int)
                                if herd_int not in all_herds:
                                    all_herds.add(herd_int)
                                    new_herds += 1
                                else:
                                    duplicates += 1
                        except (ValueError, TypeError):
                            continue
                
                logger.info(f"Page {page} stats: {new_herds} new herds, {duplicates} duplicates")
                if new_herds > 0:
                    logger.info(f"Page range: {page_min} - {page_max}")
                
                # Check if we should continue
                if new_herds == 0:
                    logger.info("No new herds found, stopping pagination")
                    break
                
                if has_more and til_bes_nr is not None:
                    try:
                        next_bes_nr = int(til_bes_nr) + 1
                        if next_bes_nr <= bes_nr_fra:
                            logger.warning(f"Next starting point ({next_bes_nr}) is not greater than current ({bes_nr_fra})")
                            break
                        bes_nr_fra = next_bes_nr
                        page += 1
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid TilBesNr value: {til_bes_nr}")
                        break
                else:
                    logger.info("No more pages indicated by API")
                    break
                
                # Add small delay to avoid overwhelming the service
                time.sleep(0.2)
            
            total_herds = len(all_herds)
            logger.info(f"Found {total_herds} total herds across {page} pages")
            if all_herds:
                logger.info(f"Overall range: {min(all_herds)} - {max(all_herds)}")
            
            return sorted(list(all_herds))
            
        except Exception as e:
            logger.error(f"Error listing herds for species {species_code}, usage {usage_code}: {str(e)}")
            return []

    async def analyze_all_combinations(self):
        """Analyze all species/usage combinations to find those with exactly 10000 herds."""
        try:
            # First get all valid combinations
            combinations = self.get_species_usage_combinations()
            logger.info(f"Found {len(combinations)} valid species/usage combinations")
            
            # Track combinations with exactly 10000 herds
            suspicious_combinations = []
            all_results = []
            
            # Process combinations in chunks to avoid overwhelming the API
            chunk_size = 5  # Process 5 combinations at a time
            for i in range(0, len(combinations), chunk_size):
                chunk = combinations[i:i + chunk_size]
                
                # Process each combination in the chunk
                for combo in chunk:
                    species_code = combo['species_code']
                    usage_code = combo['usage_code']
                    species_text = combo['species_text']
                    usage_text = combo['usage_text']
                    
                    logger.info(f"\nTesting combination {i+1}/{len(combinations)}: "
                              f"species {species_code} ({species_text}), "
                              f"usage {usage_code} ({usage_text})")
                    
                    herds = self.get_herds_for_combination(species_code, usage_code)
                    count = len(herds)
                    
                    result = {
                        'species_code': species_code,
                        'species_text': species_text,
                        'usage_code': usage_code,
                        'usage_text': usage_text,
                        'herd_count': count
                    }
                    
                    if herds:
                        result.update({
                            'min_herd': min(herds),
                            'max_herd': max(herds)
                        })
                    
                    all_results.append(result)
                    
                    if count == 10000:
                        logger.warning(f"Found combination with exactly 10000 herds!")
                        suspicious_combinations.append(result)
                    else:
                        logger.info(f"Found {count} herds")
                    
                    # Add a small delay to avoid overwhelming the service
                    time.sleep(0.1)  # Reduced from 0.2s to 0.1s
            
            # Log summary
            logger.info("\n=== Summary ===")
            logger.info(f"Tested {len(combinations)} combinations")
            logger.info(f"Found {len(suspicious_combinations)} combinations with exactly 10000 herds:")
            
            for combo in suspicious_combinations:
                logger.info(f"\nSpecies {combo['species_code']} ({combo['species_text']}), "
                          f"Usage {combo['usage_code']} ({combo['usage_text']})")
                logger.info(f"Herd range: {combo.get('min_herd', 'N/A')} - {combo.get('max_herd', 'N/A')}")
            
            # Also log distribution of herd counts
            counts = [r['herd_count'] for r in all_results]
            if counts:
                logger.info("\nHerd count distribution:")
                logger.info(f"Min count: {min(counts)}")
                logger.info(f"Max count: {max(counts)}")
                logger.info(f"Number of empty combinations (0 herds): {counts.count(0)}")
                logger.info(f"Number of combinations with exactly 10000 herds: {counts.count(10000)}")
                
                # Count ranges
                ranges = [(0, 100), (100, 1000), (1000, 5000), (5000, 9999), (10000, 10000), (10001, float('inf'))]
                for start, end in ranges:
                    count = len([c for c in counts if start <= c <= end])
                    logger.info(f"Combinations with {start}-{end} herds: {count}")
            
            return suspicious_combinations
            
        except Exception as e:
            logger.error(f"Error analyzing combinations: {str(e)}")
            return []

    def _parse_herd_details(self, response: Any) -> Dict[str, Any]:
        """Parse the herd details response into a structured dictionary."""
        return {
            'herd_number': getattr(response, 'BesaetningsNummer', None),
            'species_code': getattr(response, 'DyreArtKode', None),
            'usage_code': getattr(response, 'BrugsArtKode', None),
            'status': getattr(response, 'Status', None),
            'start_date': getattr(response, 'StartDato', None),
            'end_date': getattr(response, 'SlutDato', None),
            # Add more fields as needed
        }

    def get_all_herds(self) -> List[Dict[str, Any]]:
        """Get all herds by iterating through species/usage combinations."""
        all_herds = []
        
        # Get valid species/usage combinations first
        combinations = self.get_species_usage_combinations()
        
        for combo in combinations:
            try:
                response = self.herd_client.service.listBesaetninger(
                    arg0={
                        'GLRCHRWSInfoInbound': self._get_base_request_info(),
                        'Request': {
                            'DyreArtKode': combo['species_code'],
                            'BrugsArtKode': combo['usage_code']
                        }
                    }
                )
                
                # Parse response and add herds
                batch_herds = self._parse_herd_list_response(response)
                all_herds.extend(batch_herds)
                
                # Add small delay to avoid overwhelming the service
                time.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"Error fetching herds for species {combo['species_code']}, usage {combo['usage_code']}: {e}")
                continue
                    
        return all_herds

    def _parse_herd_list_response(self, response: Any) -> List[Dict[str, Any]]:
        """Parse the herd list response into structured data."""
        herds = []
        
        response_data = getattr(response, 'return', None)
        if not response_data or not hasattr(response_data, 'Response'):
            return herds
        
        for herd in response_data.Response:
            herds.append({
                'herd_number': getattr(herd, 'BesaetningsNummer', None),
                'species_code': getattr(herd, 'DyreArtKode', None),
                'usage_code': getattr(herd, 'BrugsArtKode', None),
                'chk_number': getattr(herd, 'CHKNummer', None),
                'status': getattr(herd, 'Status', None)
            })
        
        return herds

    def get_property_details(self, chr_number: int) -> Dict[str, Any]:
        """Get detailed property information for a CHR number."""
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'BrugerNavn': self.username,
                    'KlientId': 'LandbrugsData',
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': f'property_details_{chr_number}'
                },
                'Request': {
                    'ChrNummer': chr_number
                }
            }
            
            logger.debug(f"Fetching property details for CHR {chr_number}")
            result = self.clients['ejendom'].service.hentCHRStamoplysninger(request)
            
            if not hasattr(result, 'Response') or not result.Response:
                logger.warning(f"No property data found for CHR {chr_number}")
                return {}
            
            # Response is a list with one item
            response_data = result.Response[0]
            
            # Parse property information
            details = {
                'chr_number': int(chr_number),
                
                # Property information
                'property_address': str(getattr(response_data.Ejendom, 'Adresse', '')).strip() or None,
                'property_city': str(getattr(response_data.Ejendom, 'ByNavn', '')).strip() or None,
                'property_postal_code': str(getattr(response_data.Ejendom, 'PostNummer', '')).strip() or None,
                'property_postal_district': str(getattr(response_data.Ejendom, 'PostDistrikt', '')).strip() or None,
                'property_municipality_code': int(getattr(response_data.Ejendom, 'KommuneNummer', 0)) or None,
                'property_municipality_name': str(getattr(response_data.Ejendom, 'KommuneNavn', '')).strip() or None,
                'property_created_date': self._parse_date(getattr(response_data.Ejendom, 'DatoOpret', None)),
                'property_updated_date': self._parse_date(getattr(response_data.Ejendom, 'DatoOpdatering', None)),
                
                # Food authority information
                'food_region_number': int(getattr(response_data.FVST, 'FoedevareRegionsNummer', 0)) or None,
                'food_region_name': str(getattr(response_data.FVST, 'FoedevareRegionsNavn', '')).strip() or None,
                'veterinary_dept_name': str(getattr(response_data.FVST, 'VeterinaerAfdelingsNavn', '')).strip() or None,
                'veterinary_section_name': str(getattr(response_data.FVST, 'VeterinaerSektionsNavn', '')).strip() or None,
            }
            
            # Add coordinates if available
            if hasattr(response_data, 'StaldKoordinater'):
                coords = response_data.StaldKoordinater
                x = getattr(coords, 'StaldKoordinatX', None)
                y = getattr(coords, 'StaldKoordinatY', None)
                
                if x is not None and y is not None:
                    try:
                        x = float(x)
                        y = float(y)
                        
                        # Store original UTM32 coordinates
                        details['stable_coordinates_utm32'] = {
                            'x': x,
                            'y': y
                        }
                        
                        # Convert to WGS84 if coordinates seem valid
                        if 400000 <= x <= 900000 and 6000000 <= y <= 6500000:  # Valid range for Denmark
                            from pyproj import Transformer
                            transformer = Transformer.from_crs("EPSG:25832", "EPSG:4326")
                            lat, lon = transformer.transform(x, y)
                            
                            # Validate converted coordinates
                            if 54 <= lat <= 58 and 8 <= lon <= 13:  # Valid range for Denmark
                                details['latitude'] = lat
                                details['longitude'] = lon
                            else:
                                logger.warning(f"Converted coordinates outside Denmark for CHR {chr_number}: {lat}, {lon}")
                        else:
                            logger.warning(f"UTM32 coordinates outside valid range for CHR {chr_number}: {x}, {y}")
                            
                    except Exception as e:
                        logger.error(f"Error processing coordinates for CHR {chr_number}: {str(e)}")
            
            # Remove None values
            return {k: v for k, v in details.items() if v is not None}
            
        except Exception as e:
            logger.error(f"Error getting property details for CHR {chr_number}: {str(e)}")
            return {}

    def _prepare_dataframe_for_bigquery(self, df: pd.DataFrame, nested_columns: List[str] = None) -> pd.DataFrame:
        """Prepare a DataFrame for BigQuery by:
        1. Flattening nested dictionaries
        2. Converting datetime objects to ISO format strings
        3. Converting booleans to integers
        4. Ensuring column names are BigQuery-compatible
        """
        # Make a copy to avoid modifying the original
        df = df.copy()
        
        # Flatten nested dictionaries into columns
        if nested_columns:
            for col in nested_columns:
                if col in df.columns:
                    nested_df = pd.json_normalize(df[col].dropna())
                    if not nested_df.empty:
                        # Prefix the new columns with the original column name
                        nested_df.columns = [f"{col}_{c}" for c in nested_df.columns]
                        # Drop the original column and join the new columns
                        df = df.drop(columns=[col]).join(nested_df)
        
        # Convert datetime objects to ISO format strings
        date_columns = df.select_dtypes(include=['datetime64']).columns
        for col in date_columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d')
        
        # Convert boolean columns to integers
        bool_columns = df.select_dtypes(include=['bool']).columns
        for col in bool_columns:
            df[col] = df[col].astype(int)
        
        # Ensure column names are BigQuery compatible (no dots, spaces)
        df.columns = [c.replace('.', '_').replace(' ', '_').lower() for c in df.columns]
        
        return df

    def _convert_coordinates(self, coords: dict) -> tuple[float, float]:
        """Convert UTM32 coordinates to WGS84 with validation"""
        try:
            if not coords or 'x' not in coords or 'y' not in coords:
                return None, None
                
            x = float(coords['x'])
            y = float(coords['y'])
            
            # Valid range for Denmark in UTM32
            if not (400000 <= x <= 900000 and 6000000 <= y <= 6500000):
                logger.warning(f"Coordinates outside UTM32 range for Denmark: {x}, {y}")
                return None, None
                
            transformer = Transformer.from_crs("EPSG:25832", "EPSG:4326")
            lat, lon = transformer.transform(x, y)
            
            # Validate converted coordinates (Denmark bounds)
            if not (54 <= lat <= 58 and 8 <= lon <= 13):
                logger.warning(f"Converted coordinates outside Denmark: {lat}, {lon}")
                return None, None
                
            return lat, lon
            
        except Exception as e:
            logger.error(f"Error converting coordinates: {str(e)}")
            return None, None

    def _fetch_herds_for_combination(self, species_code: int, usage_code: int) -> List[Dict[str, Any]]:
        """Fetch all herds for a specific species/usage combination."""
        all_herds = []
        
        try:
            request = {
                'GLRCHRWSInfoInbound': {
                    'KlientId': 'LandbrugsData',
                    'BrugerNavn': self.username,
                    'SessionId': '1',
                    'IPAdresse': '',
                    'TrackID': f'fetch_herds_{species_code}_{usage_code}'
                },
                'Request': {
                    'DyreArtKode': species_code,
                    'BrugsArtKode': usage_code
                }
            }
            
            response = self.herd_client.service.listBesaetninger(arg0=request)
            
            # Parse response and extract herds
            if hasattr(response, '_return') and hasattr(response._return, 'Response'):
                for herd in response._return.Response:
                    herd_data = {
                        'herd_number': getattr(herd, 'BesaetningsNummer', None),
                        'species_code': getattr(herd, 'DyreArtKode', None),
                        'usage_code': getattr(herd, 'BrugsArtKode', None),
                        'chk_number': getattr(herd, 'CHKNummer', None),
                        'status': getattr(herd, 'Status', None)
                    }
                    all_herds.append(herd_data)
            
            # Add small delay to avoid overwhelming the service
            time.sleep(0.1)
            
            return all_herds
            
        except Exception as e:
            logger.error(f"Error fetching herds for species {species_code}, usage {usage_code}: {e}")
            return []

    def _upload_batch_data(self, herds_data: list, properties_data: list, owners_data: list, users_data: list, practices_data: list) -> None:
        """Upload batches of data to storage."""
        try:
            # Convert lists to DataFrames
            if herds_data:
                herds_df = pd.DataFrame(herds_data)
                self._upload_to_storage(herds_df, 'herds')
            
            if properties_data:
                properties_df = pd.DataFrame(properties_data)
                self._upload_to_storage(properties_df, 'properties')
            
            if owners_data:
                owners_df = pd.DataFrame(owners_data)
                self._upload_to_storage(owners_df, 'owners')
            
            if users_data:
                users_df = pd.DataFrame(users_data)
                self._upload_to_storage(users_df, 'users')
            
            if practices_data:
                practices_df = pd.DataFrame(practices_data)
                self._upload_to_storage(practices_df, 'practices')
                
        except Exception as e:
            logger.error(f"Error uploading batch data: {str(e)}")
            raise

    async def process_herd(self, herd_number: int, species_code: int) -> Dict[str, Any]:
        """Process a single herd, fetching details and property information."""
        try:
            herd_details = await self.get_herd_details(herd_number, species_code)
            if not herd_details:
                return {}
            
            chr_number = herd_details.get('chr_number')
            if not chr_number:
                return herd_details
            
            property_details = await self.get_property_details(chr_number)
            if not property_details:
                return herd_details
            
            return {**herd_details, **property_details}
            
        except Exception as e:
            logger.error(f"Error processing herd {herd_number}: {str(e)}")
            return {}

================
File: src/sources/parsers/property_owners.py
================
import logging
import json
import zipfile
import ijson
from typing import Generator, Any, Dict, Optional, Tuple
from ..base import BaseSource
from google.cloud import storage
from pathlib import Path
import os
import pandas as pd
import asyncio
from datetime import datetime
import tempfile
import paramiko
from google.cloud import secretmanager
from google.cloud import bigquery

logger = logging.getLogger(__name__)

class PropertyOwnersParser(Source):
    """Parser for property owners data from Datafordeler"""
    
    source_id = "property_owners"
    
    def __init__(self, config):
        super().__init__(config)
        self.batch_size = 1000
        self.bucket_name = config.get('bucket', 'landbrugsdata-raw-data')
        self.raw_folder = "raw"
        self._get_sftp_credentials()
        self.bq_client = bigquery.Client()
        self.dataset_id = 'land_data'
        self.table_id = 'property_owners_raw'
        
    def _get_sftp_credentials(self):
        """Get SFTP credentials from Secret Manager."""
        try:
            secret_client = secretmanager.SecretManagerServiceClient()
            
            # Get host
            host_secret = "projects/677523729299/secrets/datafordeler-sftp-host/versions/latest"
            host_response = secret_client.access_secret_version(name=host_secret)
            self.sftp_host = host_response.payload.data.decode('UTF-8')
            
            # Get username
            username_secret = "projects/677523729299/secrets/datafordeler-sftp-username/versions/latest"
            username_response = secret_client.access_secret_version(name=username_secret)
            self.sftp_username = username_response.payload.data.decode('UTF-8')
            
        except Exception as e:
            logger.error(f"Failed to get SFTP credentials from Secret Manager: {str(e)}")
            raise
        
    def _get_ssh_key(self) -> str:
        """Get SSH private key from Secret Manager."""
        try:
            secret_client = secretmanager.SecretManagerServiceClient()
            secret_name = "projects/677523729299/secrets/ssh-brugerdatafordeleren/versions/3"
            
            response = secret_client.access_secret_version(name=secret_name)
            return response.payload.data.decode('UTF-8')
            
        except Exception as e:
            logger.error(f"Failed to get SSH key from Secret Manager: {str(e)}")
            raise
        
    def _get_sftp_client(self) -> paramiko.SFTPClient:
        """Create and return an SFTP client connection."""
        try:
            # Initialize SSH client
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            # Get private key from Secret Manager
            private_key_data = self._get_ssh_key()
            
            # Create a temporary file for the key
            with tempfile.NamedTemporaryFile(mode='w', delete=False) as key_file:
                key_file.write(private_key_data)
                key_file.flush()
                
                try:
                    # Load private key
                    private_key = paramiko.RSAKey.from_private_key_file(key_file.name)
                    
                    # Connect to SFTP server
                    ssh.connect(
                        hostname=self.sftp_host,
                        port=22,
                        username=self.sftp_username,
                        pkey=private_key,
                    )
                    
                    return ssh.open_sftp()
                    
                finally:
                    # Clean up the temporary key file
                    os.unlink(key_file.name)
                    
        except Exception as e:
            logger.error(f"Failed to connect to SFTP server: {str(e)}")
            raise

    def _find_latest_file(self, sftp: paramiko.SFTPClient) -> Tuple[str, datetime]:
        """
        Find the latest .zip file in the SFTP server.
        
        Returns:
            Tuple of (filename, modification_time)
        """
        latest_file = None
        latest_time = None
        
        try:
            # List all files in the root directory
            for entry in sftp.listdir_attr():
                # Check if it's a file and ends with .zip
                if not entry.longname.startswith('d') and entry.filename.endswith('.zip'):
                    mod_time = datetime.fromtimestamp(entry.st_mtime)
                    if latest_time is None or mod_time > latest_time:
                        latest_file = entry.filename
                        latest_time = mod_time
            
            if latest_file is None:
                raise FileNotFoundError("No .zip files found on SFTP server")
                
            logger.info(f"Found latest file: {latest_file} (modified: {latest_time})")
            return latest_file, latest_time
            
        except Exception as e:
            logger.error(f"Error listing SFTP directory: {str(e)}")
            raise
    
    def _get_gcs_blob(self, filename: str) -> storage.Blob:
        """Get a GCS blob for the given filename."""
        bucket = self.storage_client.bucket(self.bucket_name)
        blob_name = f"{self.raw_folder}/{filename}"
        return bucket.blob(blob_name)
    
    def build_nested_dict(self, d: dict, keys: list, value: Any) -> dict:
        """
        Helper function to build nested dictionary structure.
        
        Args:
            d: Dictionary to build into
            keys: List of keys representing the path
            value: Value to set at the final key
        """
        current = d
        for key in keys[:-1]:
            current = current.setdefault(key, {})
        current[keys[-1]] = value
        return d

    def stream_and_transform(self, input_path: str, output_path: str) -> int:
        """
        Stream the zipped JSON file and transform it to newline-delimited JSON.
        Maintains nested structure of the original data.
        """
        logger.info(f"Starting transformation of {input_path}")
        
        tmp_output = f"{output_path}.tmp"
        records_processed = 0
        
        try:
            with zipfile.ZipFile(input_path, 'r') as zip_file:
                json_files = [f for f in zip_file.namelist() if f.endswith('.json')]
                if not json_files:
                    raise ValueError("No JSON file found in ZIP archive")
                
                json_file = json_files[0]
                logger.info(f"Processing JSON file from ZIP: {json_file}")
                
                with zip_file.open(json_file) as json_data, open(tmp_output, 'w') as out_file:
                    parser = ijson.parse(json_data)
                    current_object = {"type": None, "properties": {}}
                    current_path = []
                    in_properties = False
                    
                    for prefix, event, value in parser:
                        if prefix.startswith('features.item'):
                            parts = prefix.split('.')
                            
                            if len(parts) == 2 and event == 'start_map':
                                current_object = {"type": None, "properties": {}}
                                current_path = []
                                in_properties = False
                            elif len(parts) == 2 and event == 'end_map':
                                if current_object:  # Only write non-empty objects
                                    json.dump(current_object, out_file)
                                    out_file.write('\n')
                                    records_processed += 1
                            elif parts[-1] == 'type' and len(parts) == 3:
                                current_object['type'] = value
                            elif parts[-1] == 'properties' and event == 'start_map':
                                in_properties = True
                            elif parts[-1] == 'properties' and event == 'end_map':
                                in_properties = False
                            elif in_properties and event in ('string', 'number', 'boolean', 'null'):
                                # Get the path after 'properties'
                                property_path = parts[parts.index('properties') + 1:]
                                # Build nested structure under properties
                                self.build_nested_dict(current_object['properties'], property_path, value)
            
            os.rename(tmp_output, output_path)
            logger.info(f"Successfully transformed {input_path} to {output_path} ({records_processed} records)")
            return records_processed
            
        except Exception as e:
            if os.path.exists(tmp_output):
                os.remove(tmp_output)
            logger.error(f"Error transforming file: {str(e)}")
            raise
    
    def upload_to_gcs(self, local_path: str, gcs_filename: str) -> str:
        """
        Upload a file to Google Cloud Storage.
        
        Args:
            local_path: Path to the local file
            gcs_filename: Desired filename in GCS
            
        Returns:
            GCS URI of the uploaded file
        """
        blob = self._get_gcs_blob(gcs_filename)
        blob.upload_from_filename(local_path)
        
        gcs_uri = f"gs://{self.bucket_name}/{self.raw_folder}/{gcs_filename}"
        logger.info(f"File uploaded to {gcs_uri}")
        return gcs_uri
    
    async def fetch(self) -> pd.DataFrame:
        """
        Fetch the latest data from GCS and return as DataFrame.
        Required by Source base class.
        """
        # List blobs in the raw folder
        blobs = list(self.storage_client.list_blobs(
            self.bucket_name,
            prefix=f"{self.raw_folder}/",
            delimiter='/'
        ))
        
        if not blobs:
            raise ValueError("No data found in GCS bucket")
        
        # Get the latest blob (by name or creation time)
        latest_blob = max(blobs, key=lambda b: b.time_created)
        
        # Create a temporary file to store the data
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.ndjson') as temp_file:
            latest_blob.download_to_filename(temp_file.name)
            df = pd.read_json(temp_file.name, lines=True)
            return df
    
    async def sync(self) -> Optional[int]:
        """Sync data from SFTP to GCS and BigQuery"""
        sftp_client = None
        temp_input = None
        temp_output = None
        records_processed = None
        
        try:
            # Connect to SFTP and download latest file
            sftp_client = self._get_sftp_client()
            latest_file, mod_time = self._find_latest_file(sftp_client)
            
            # Create temporary files
            temp_input = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
            temp_output = tempfile.NamedTemporaryFile(delete=False, suffix='.ndjson')
            temp_input.close()
            temp_output.close()
            
            # Download and transform
            logger.info(f"Downloading {latest_file}...")
            sftp_client.get(latest_file, temp_input.name)
            records_processed = self.stream_and_transform(temp_input.name, temp_output.name)
            
            # Upload to GCS
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            original_name = Path(latest_file).stem
            gcs_filename = f"property_owners_{original_name}_{timestamp}.ndjson"
            gcs_uri = self.upload_to_gcs(temp_output.name, gcs_filename)
            
            # Load to BigQuery from GCS
            logger.info("Loading data to BigQuery...")
            job_config = bigquery.LoadJobConfig(
                source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
                autodetect=True  # Let BigQuery detect the schema
            )
            
            table_ref = f"{self.dataset_id}.{self.table_id}"
            load_job = self.bq_client.load_table_from_uri(
                gcs_uri,
                table_ref,
                job_config=job_config
            )
            load_job.result()  # Wait for job to complete
            
            logger.info(f"Loaded {records_processed} records to BigQuery table {table_ref}")
            return records_processed
            
        finally:
            # Clean up
            if sftp_client:
                sftp_client.close()
            
            for temp_file in [temp_input, temp_output]:
                if temp_file and os.path.exists(temp_file.name):
                    os.remove(temp_file.name)
    
    def process_file(self, input_path: str, base_filename: str) -> str:
        """
        Process the input file and upload to GCS.
        
        Args:
            input_path: Path to the input ZIP file
            base_filename: Base name for the output file
            
        Returns:
            GCS URI of the processed file
        """
        # Create output filename with timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f"{Path(base_filename).stem}_{timestamp}.ndjson"
        output_path = f"/tmp/{output_filename}"
        
        try:
            # Transform the file to newline-delimited JSON
            self.stream_and_transform(input_path, output_path)
            
            # Upload to GCS
            gcs_uri = self.upload_to_gcs(output_path, output_filename)
            
            # Clean up local files
            os.remove(output_path)
            
            return gcs_uri
            
        except Exception as e:
            logger.error(f"Failed to process file: {str(e)}")
            # Clean up any remaining temporary files
            if os.path.exists(output_path):
                os.remove(output_path)
            raise

================
File: src/sources/parsers/water_projects.py
================
from pathlib import Path
import asyncio
import os
import xml.etree.ElementTree as ET
from datetime import datetime
import logging
import aiohttp
from shapely.geometry import Polygon, MultiPolygon
from shapely import wkt
from shapely.ops import unary_union
import geopandas as gpd
import pandas as pd
from google.cloud import storage
import time
import backoff
from aiohttp import ClientError, ClientTimeout
from dotenv import load_dotenv
from tqdm import tqdm
import ssl
from shapely.validation import explain_validity
from shapely.geometry.polygon import orient

from ..base import GeospatialSource
from ..utils.geometry_validator import validate_and_transform_geometries

logger = logging.getLogger(__name__)

def clean_value(value):
    """Clean string values"""
    if not isinstance(value, str):
        return value
    value = value.strip()
    return value if value else None

class WaterProjects(GeospatialSource):
    source_id = "water_projects"
    
    def __init__(self, config):
        super().__init__(config)
        self.batch_size = 100
        self.max_concurrent = 3
        self.request_timeout = 300
        self.storage_batch_size = 5000  # Size for storage batches
        
        self.request_timeout_config = ClientTimeout(
            total=self.request_timeout,
            connect=60,
            sock_read=300
        )
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 QGIS/33603/macOS 15.1'
        }
        
        self.layers = [
            "N2000_projekter:Hydrologi_E",
            #"N2000_projekter:Hydrologi_F",
            "Ovrige_projekter:Vandloebsrestaurering_E",
            "Ovrige_projekter:Vandloebsrestaurering_F",
            "Vandprojekter:Fosfor_E_samlet",
            "Vandprojekter:Fosfor_F_samlet",
            "Vandprojekter:Kvaelstof_E_samlet",
            "Vandprojekter:Kvaelstof_F_samlet",
            "Vandprojekter:Lavbund_E_samlet",
            "Vandprojekter:Lavbund_F_samlet",
            "Vandprojekter:Private_vaadomraader",
            "Vandprojekter:Restaurering_af_aadale_2024",
            "vandprojekter:kla_projektforslag",
            "vandprojekter:kla_projektomraader",
            "Klima_lavbund_demarkation___offentlige_projekter:0"
        ]
        
        self.request_semaphore = asyncio.Semaphore(self.max_concurrent)
        
        self.url_mapping = {
            'vandprojekter:kla_projektforslag': 'https://wfs2-miljoegis.mim.dk/vandprojekter/wfs',
            'vandprojekter:kla_projektomraader': 'https://wfs2-miljoegis.mim.dk/vandprojekter/wfs',
            'Klima_lavbund_demarkation___offentlige_projekter:0': 'https://gis.nst.dk/server/rest/services/autonom/Klima_lavbund_demarkation___offentlige_projekter/FeatureServer'
        }
        
        self.service_types = {
            'Klima_lavbund_demarkation___offentlige_projekter:0': 'arcgis'
        }

    def _get_params(self, layer, start_index=0):
        """Get WFS request parameters"""
        return {
            'SERVICE': 'WFS',
            'REQUEST': 'GetFeature',
            'VERSION': '2.0.0',
            'TYPENAMES': layer,
            'STARTINDEX': str(start_index),
            'COUNT': str(self.batch_size),
            'SRSNAME': 'urn:ogc:def:crs:EPSG::25832'
        }

    def _parse_geometry(self, geom_elem):
        """Parse GML geometry into WKT and calculate area"""
        try:
            gml_ns = '{http://www.opengis.net/gml/3.2}'
            
            multi_surface = geom_elem.find(f'.//{gml_ns}MultiSurface')
            if multi_surface is None:
                logger.error("No MultiSurface element found")
                return None
            
            polygons = []
            for surface_member in multi_surface.findall(f'.//{gml_ns}surfaceMember'):
                polygon = surface_member.find(f'.//{gml_ns}Polygon')
                if polygon is None:
                    continue
                    
                pos_list = polygon.find(f'.//{gml_ns}posList')
                if pos_list is None or not pos_list.text:
                    continue
                    
                try:
                    coords = [float(x) for x in pos_list.text.strip().split()]
                    coords = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]
                    if len(coords) >= 4:  # Ensure we have enough coordinates
                        polygons.append(Polygon(coords))
                except Exception as e:
                    logger.error(f"Failed to parse coordinates: {str(e)}")
                    continue
            
            if not polygons:
                return None
            
            geom = MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]
            area_ha = geom.area / 10000  # Convert square meters to hectares
            
            return {
                'wkt': geom.wkt,
                'area_ha': area_ha
            }
            
        except Exception as e:
            logger.error(f"Error parsing geometry: {str(e)}")
            return None

    def _parse_feature(self, feature, layer_name):
        """Parse a single feature into a dictionary"""
        try:
            namespace = feature.tag.split('}')[0].strip('{')
            
            geom_elem = feature.find(f'{{%s}}the_geom' % namespace) or feature.find(f'{{%s}}wkb_geometry' % namespace)
            if geom_elem is None:
                logger.warning(f"No geometry found in feature for layer {layer_name}")
                return None

            geometry_data = self._parse_geometry(geom_elem)
            if geometry_data is None:
                logger.warning(f"Failed to parse geometry for layer {layer_name}")
                return None

            data = {
                'layer_name': layer_name,
                'geometry': geometry_data['wkt'],
                'area_ha': geometry_data['area_ha']
            }
            
            for elem in feature:
                if not elem.tag.endswith(('the_geom', 'wkb_geometry')):
                    key = elem.tag.split('}')[-1].lower()
                    if elem.text:
                        value = clean_value(elem.text)
                        if value is not None:
                            # Convert specific fields
                            try:
                                if key in ['area', 'budget']:
                                    value = float(''.join(c for c in value if c.isdigit() or c == '.'))
                                elif key in ['startaar', 'tilsagnsaa', 'slutaar']:
                                    value = int(value)
                                elif key in ['startdato', 'slutdato']:
                                    value = pd.to_datetime(value, dayfirst=True)
                            except (ValueError, TypeError):
                                logger.warning(f"Failed to convert {key} value: {value}")
                                value = None
                            data[key] = value
            
            return data
            
        except Exception as e:
            logger.error(f"Error parsing feature in layer {layer_name}: {str(e)}", exc_info=True)
            return None

    @backoff.on_exception(
        backoff.expo,
        (ClientError, asyncio.TimeoutError),
        max_tries=3
    )
    async def _fetch_chunk(self, session, layer, start_index):
        """Fetch a chunk of features with retries"""
        async with self.request_semaphore:
            params = self._get_params(layer, start_index)
            url = self.url_mapping.get(layer, self.config['url'])
            
            try:
                async with session.get(
                    url, 
                    params=params,
                    timeout=self.request_timeout_config
                ) as response:
                    if response.status != 200:
                        logger.error(f"Error fetching chunk. Status: {response.status}")
                        error_text = await response.text()
                        logger.error(f"Error response: {error_text[:500]}")
                        return None
                    
                    text = await response.text()
                    root = ET.fromstring(text)
                    
                    features = []
                    namespaces = {}
                    for elem in root.iter():
                        if '}' in elem.tag:
                            ns_url = elem.tag.split('}')[0].strip('{')
                            namespaces['ns'] = ns_url
                            break
                    
                    for member in root.findall('.//ns:member', namespaces=namespaces):
                        for feature in member:
                            parsed = self._parse_feature(feature, layer)
                            if parsed and parsed.get('geometry'):
                                features.append(parsed)
                    
                    return features
                    
            except Exception as e:
                logger.error(f"Error fetching chunk: {str(e)}")
                raise

    async def write_to_storage(self, features, dataset):
        """Write features to GeoParquet in Cloud Storage"""
        if not features:
            return
        
        try:
            # Create DataFrame from features
            df = pd.DataFrame(features)
            geometries = [wkt.loads(f['geometry']) for f in features]
            gdf = gpd.GeoDataFrame(df, geometry=geometries, crs="EPSG:25832")
            
            # Handle working/final files
            temp_working = f"/tmp/{dataset}_working.parquet"
            working_blob = self.bucket.blob(f'raw/{dataset}/working.parquet')
            
            if working_blob.exists():
                working_blob.download_to_filename(temp_working)
                existing_gdf = gpd.read_parquet(temp_working)
                logger.info(f"Appending {len(gdf):,} features to existing {len(existing_gdf):,}")
                combined_gdf = pd.concat([existing_gdf, gdf], ignore_index=True)
            else:
                combined_gdf = gdf
            
            # Write working file
            combined_gdf.to_parquet(temp_working)
            working_blob.upload_from_filename(temp_working)
            logger.info(f"Updated working file now has {len(combined_gdf):,} features")
            
            # If sync complete, create final files
            if hasattr(self, 'is_sync_complete') and self.is_sync_complete:
                logger.info(f"Sync complete - writing final files")
                
                # Write regular final file
                final_blob = self.bucket.blob(f'raw/{dataset}/current.parquet')
                final_blob.upload_from_filename(temp_working)
                
                # Create dissolved version
                logger.info("Creating dissolved version...")
                try:
                    logger.info(f"Initial CRS: {combined_gdf.crs}")
                    
                    # Convert to WGS84 before dissolve
                    if combined_gdf.crs.to_epsg() != 4326:
                        logger.info("Converting to WGS84...")
                        combined_gdf = combined_gdf.to_crs("EPSG:4326")
                    
                    # Single dissolve operation in WGS84
                    logger.info("Dissolving in WGS84...")
                    dissolved = unary_union(combined_gdf.geometry.values)
                    logger.info(f"Dissolved geometry type: {dissolved.geom_type}")
                    
                    if dissolved.geom_type == 'MultiPolygon':
                        logger.info(f"Got MultiPolygon with {len(dissolved.geoms)} parts")
                        # Clean each geometry with buffer(0)
                        cleaned_geoms = [geom.buffer(0) for geom in dissolved.geoms]
                        dissolved_gdf = gpd.GeoDataFrame(geometry=cleaned_geoms, crs="EPSG:4326")
                    else:
                        # Clean single geometry with buffer(0)
                        cleaned = dissolved.buffer(0)
                        dissolved_gdf = gpd.GeoDataFrame(geometry=[cleaned], crs="EPSG:4326")
                    
                    # Detailed geometry inspection after dissolve
                    if dissolved.geom_type == 'MultiPolygon':
                        logger.info(f"Post-dissolve parts: {len(dissolved.geoms)}")
                        logger.info(f"Post-dissolve validity: {dissolved.is_valid}")
                        logger.info(f"Post-dissolve simplicity: {dissolved.is_simple}")
                        
                        # Inspect each part in detail
                        for i, part in enumerate(dissolved.geoms):
                            if not part.is_valid or not part.is_simple:
                                logger.error(f"Invalid part {i}:")
                                logger.error(f"- Validity explanation: {explain_validity(part)}")
                                logger.error(f"- Number of exterior points: {len(list(part.exterior.coords))}")
                                logger.error(f"- Number of interior rings: {len(part.interiors)}")
                                # Log coordinates of problematic part
                                logger.error(f"- Exterior coordinates: {list(part.exterior.coords)}")
                                for j, interior in enumerate(part.interiors):
                                    logger.error(f"- Interior ring {j} coordinates: {list(interior.coords)}")
                    else:
                        if not dissolved.is_valid or not dissolved.is_simple:
                            logger.error("Invalid single polygon:")
                            logger.error(f"- Validity explanation: {explain_validity(dissolved)}")
                            logger.error(f"- Number of exterior points: {len(list(dissolved.exterior.coords))}")
                            logger.error(f"- Number of interior rings: {len(dissolved.interiors)}")
                            logger.error(f"- Exterior coordinates: {list(dissolved.exterior.coords)}")
                            for j, interior in enumerate(dissolved.interiors):
                                logger.error(f"- Interior ring {j} coordinates: {list(interior.coords)}")

                    # Final validation will handle BigQuery compatibility
                    dissolved_gdf = validate_and_transform_geometries(dissolved_gdf, f"{dataset}_dissolved")
                    
                    # Write dissolved version
                    temp_dissolved = f"/tmp/{dataset}_dissolved.parquet"
                    dissolved_gdf.to_parquet(temp_dissolved)
                    dissolved_blob = self.bucket.blob(f'raw/{dataset}/dissolved_current.parquet')
                    dissolved_blob.upload_from_filename(temp_dissolved)
                    logger.info("Dissolved version created and saved")
                    
                except Exception as e:
                    logger.error(f"Error during dissolve operation: {str(e)}")
                    raise
                
                # Cleanup
                working_blob.delete()
                os.remove(temp_dissolved)
            
            # Cleanup working file
            if os.path.exists(temp_working):
                os.remove(temp_working)
            
        except Exception as e:
            logger.error(f"Error writing to storage: {str(e)}")
            raise

    async def _fetch_arcgis_features(self, session, layer, url):
        """Fetch features from ArcGIS REST service"""
        try:
            # Extract layer ID from the full layer string
            layer_id = layer.split(':')[1]  # This will get "0" from "Klima_lavbund_demarkation___offentlige_projekter:0"
            
            params = {
                'f': 'json',
                'where': '1=1',
                'outFields': '*',
                'geometryPrecision': '6',
                'outSR': '25832',
                'returnGeometry': 'true'
            }

            # Create SSL context
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE

            async with session.get(f"{url}/{layer_id}/query", params=params, ssl=ssl_context) as response:
                if response.status != 200:
                    logger.error(f"Error fetching ArcGIS features. Status: {response.status}")
                    error_text = await response.text()
                    logger.error(f"Error response: {error_text[:500]}")
                    return None

                data = await response.json()
                features = []
                
                for feature in data.get('features', []):
                    try:
                        attrs = feature.get('attributes', {})
                        geom = feature.get('geometry', {})
                        
                        if 'rings' not in geom:
                            continue
                            
                        # Convert geometry
                        polygons = []
                        for ring in geom['rings']:
                            coords = [(x, y) for x, y in ring]
                            polygons.append(Polygon(coords))
                        
                        multi_poly = MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]
                        area_ha = multi_poly.area / 10000
                        
                        # Convert timestamps
                        start_date = datetime.fromtimestamp(attrs.get('projektstart')/1000) if attrs.get('projektstart') else None
                        end_date = datetime.fromtimestamp(attrs.get('projektslut')/1000) if attrs.get('projektslut') else None
                        
                        processed_feature = {
                            'layer_name': layer,
                            'geometry': multi_poly.wkt,
                            'area_ha': area_ha,
                            'projektnavn': attrs.get('projektnavn'),
                            'enhedskontakt': attrs.get('enhedskontakt'),
                            'startdato': start_date,
                            'slutdato': end_date,
                            'status': attrs.get('status'),
                            'object_id': attrs.get('OBJECTID'),
                            'global_id': attrs.get('GlobalID')
                        }
                        
                        features.append(processed_feature)
                        
                    except Exception as e:
                        logger.error(f"Error processing feature: {str(e)}")
                        continue
                        
                return features
                
        except Exception as e:
            logger.error(f"Error in _fetch_arcgis_features: {str(e)}")
            return None

    async def sync(self):
        """Sync all water project layers"""
        logger.info("Starting water projects sync...")
        self.is_sync_complete = False
        total_processed = 0
        features_batch = []
        
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                for layer in self.layers:
                    logger.info(f"\nProcessing layer: {layer}")
                    try:
                        service_type = self.service_types.get(layer, 'wfs')
                        base_url = self.url_mapping.get(layer, self.config['url'])

                        if service_type == 'arcgis':
                            features = await self._fetch_arcgis_features(session, layer, base_url)
                            if features:
                                features_batch.extend(features)
                                total_processed += len(features)
                                logger.info(f"Layer {layer}: processed {len(features):,} features")
                            continue

                        # Existing WFS handling code
                        params = self._get_params(layer, 0)
                        async with session.get(base_url, params=params) as response:
                            if response.status != 200:
                                logger.error(f"Failed to fetch {layer}. Status: {response.status}")
                                error_text = await response.text()
                                logger.error(f"Error response: {error_text[:500]}")
                                continue
                            
                            text = await response.text()
                            root = ET.fromstring(text)
                            total_features = int(root.get('numberMatched', '0'))
                            logger.info(f"Layer {layer}: found {total_features:,} total features")
                            
                            # Process first batch
                            features = []
                            namespaces = {}
                            for elem in root.iter():
                                if '}' in elem.tag:
                                    ns_url = elem.tag.split('}')[0].strip('{')
                                    namespaces['ns'] = ns_url
                                    break
                                    
                            for member in root.findall('.//ns:member', namespaces=namespaces):
                                for feature in member:
                                    parsed = self._parse_feature(feature, layer)
                                    if parsed and parsed.get('geometry'):
                                        features.append(parsed)
                            
                            if features:
                                features_batch.extend(features)
                                total_processed += len(features)
                                logger.info(f"Layer {layer}: processed {len(features):,} features")
                            
                            # Process remaining batches
                            for start_index in range(self.batch_size, total_features, self.batch_size):
                                logger.info(f"Layer {layer}: fetching features {start_index:,}-{min(start_index + self.batch_size, total_features):,} of {total_features:,}")
                                chunk = await self._fetch_chunk(session, layer, start_index)
                                if chunk:
                                    features_batch.extend(chunk)
                                    total_processed += len(chunk)
                                    logger.info(f"Layer {layer}: processed {len(chunk):,} features")
                            
                            # Write batch if it's large enough
                            if len(features_batch) >= self.storage_batch_size:
                                logger.info(f"Writing batch of {len(features_batch):,} features")
                                await self.write_to_storage(features_batch, 'water_projects')
                                features_batch = []
                            
                    except Exception as e:
                        logger.error(f"Error processing layer {layer}: {str(e)}", exc_info=True)
                        continue

                # Write any remaining features as final batch
                if features_batch:
                    logger.info(f"Writing final batch of {len(features_batch):,} features")
                    self.is_sync_complete = True
                    await self.write_to_storage(features_batch, 'water_projects')
                
                logger.info(f"Sync completed. Total processed: {total_processed:,}")
                return total_processed
                
        except Exception as e:
            self.is_sync_complete = False
            logger.error(f"Error in sync: {str(e)}", exc_info=True)
            return total_processed

    async def fetch(self):
        """Implement abstract method - using sync() instead"""
        return await self.sync() 

def is_clockwise(coords):
    """Check if a ring is clockwise oriented"""
    # Implementation of shoelace formula
    area = 0
    for i in range(len(coords)-1):
        j = (i + 1) % len(coords)
        area += coords[i][0] * coords[j][1]
        area -= coords[j][0] * coords[i][1]
    return area > 0

================
File: src/sources/parsers/wetlands.py
================
from pathlib import Path
import asyncio
import xml.etree.ElementTree as ET
import logging
import aiohttp
from shapely.geometry import Polygon, MultiPolygon
from aiohttp import ClientError
from ..base import GeospatialSource
import pandas as pd
import geopandas as gpd
import os
from ..utils.geometry_validator import validate_and_transform_geometries
import time
import psutil
from collections import Counter
from shapely.ops import unary_union

logger = logging.getLogger(__name__)

class Wetlands(GeospatialSource):
    source_id = "wetlands"
    
    def __init__(self, config):
        super().__init__(config)
        self.batch_size = 100000
        self.max_concurrent = 5
        self.request_timeout = 300
        
        self.namespaces = {
            'wfs': 'http://www.opengis.net/wfs/2.0',
            'natur': 'http://wfs2-miljoegis.mim.dk/natur',
            'gml': 'http://www.opengis.net/gml/3.2'
        }
        
        self.request_semaphore = asyncio.Semaphore(self.max_concurrent)

    def analyze_geometry(self, geom):
        """Analyze a single geometry for grid characteristics"""
        bounds = geom.bounds
        width = bounds[2] - bounds[0]
        height = bounds[3] - bounds[1]
        area = width * height
        
        # Check grid alignment
        vertices = list(geom.exterior.coords)
        is_grid_aligned = all(
            abs(round(coord / 10) * 10 - coord) < 0.01
            for vertex in vertices
            for coord in vertex
        )
        
        return {
            'width': width,
            'height': height,
            'area': area,
            'grid_aligned': is_grid_aligned,
            'vertices': len(vertices)
        }

    def log_geometry_statistics(self, gdf):
        """Analyze and log statistics about the geometries"""
        stats = []
        for geom in gdf.geometry:
            stats.append(self.analyze_geometry(geom))
        
        # Convert to DataFrame for easy analysis
        stats_df = pd.DataFrame(stats)
        
        # Unique dimensions
        dimensions = Counter(zip(stats_df['width'], stats_df['height']))
        
        logger.info("Geometry Statistics:")
        logger.info(f"Total features: {len(stats_df)}")
        logger.info("\nUnique dimensions (width x height, count):")
        for (width, height), count in dimensions.most_common():
            logger.info(f"{width:.1f}m x {height:.1f}m: {count} features")
        
        logger.info(f"\nNon-grid-aligned features: {sum(~stats_df['grid_aligned'])}")
        logger.info(f"Average vertices per feature: {stats_df['vertices'].mean():.1f}")
        logger.info(f"Total area covered: {stats_df['area'].sum() / 1_000_000:.2f} km")

    def _get_params(self, start_index=0):
        """Get WFS request parameters"""
        return {
            'SERVICE': 'WFS',
            'REQUEST': 'GetFeature',
            'VERSION': '2.0.0',
            'TYPENAMES': self.config['layer'],
            'SRSNAME': 'EPSG:25832',
            'count': str(self.batch_size),
            'startIndex': str(start_index)
        }

    def _parse_geometry(self, geom_elem):
        """Parse GML geometry into Shapely geometry"""
        try:
            coords = geom_elem.find('.//gml:posList', self.namespaces).text.split()
            coords = [(float(coords[i]), float(coords[i + 1])) 
                     for i in range(0, len(coords), 2)]
            poly = Polygon(coords)
            
            # Ensure the polygon is valid
            if not poly.is_valid:
                poly = poly.buffer(0)
            return poly
        except Exception as e:
            logger.error(f"Error parsing geometry: {str(e)}")
            return None

    def _parse_feature(self, feature):
        """Parse a single feature into GeoJSON-like dictionary"""
        try:
            geom = self._parse_geometry(
                feature.find('.//gml:Polygon', self.namespaces)
            )
            
            if not geom:
                return None

            return {
                'type': 'Feature',
                'geometry': geom.__geo_interface__,
                'properties': {
                    'id': feature.get('{http://www.opengis.net/gml/3.2}id'),
                    'gridcode': int(feature.find('natur:gridcode', self.namespaces).text),
                    'toerv_pct': feature.find('natur:toerv_pct', self.namespaces).text
                }
            }
        except Exception as e:
            logger.error(f"Error parsing feature: {str(e)}")
            return None

    async def sync(self):
        """Sync wetlands data to Cloud Storage"""
        logger.info("Starting wetlands sync...")
        self.is_sync_complete = False
        
        # Clean up any existing working files
        working_blob = self.bucket.blob('raw/wetlands/working.parquet')
        if working_blob.exists():
            working_blob.delete()
        
        async with aiohttp.ClientSession() as session:
            # Get total count
            params = self._get_params(0)
            async with session.get(self.config['url'], params=params) as response:
                text = await response.text()
                root = ET.fromstring(text)
                total_features = int(root.get('numberMatched', '0'))
                logger.info(f"Total available features: {total_features:,}")
                
                features = [
                    self._parse_feature(f) 
                    for f in root.findall('.//natur:kulstof2022', self.namespaces)
                ]
                features = [f for f in features if f]
                
                if features:
                    await self.write_to_storage(features, 'wetlands')
                logger.info(f"Wrote first batch: {len(features)} features")
                
                total_processed = len(features)
                for start_index in range(self.batch_size, total_features, self.batch_size):
                    try:
                        chunk = await self._fetch_chunk(session, start_index)
                        if chunk:
                            self.is_sync_complete = (start_index + self.batch_size) >= total_features
                            await self.write_to_storage(chunk, 'wetlands')
                            total_processed += len(chunk)
                            logger.info(f"Progress: {total_processed:,}/{total_features:,}")
                    except Exception as e:
                        logger.error(f"Error processing batch at {start_index}: {str(e)}")
                        continue
        
        logger.info(f"Sync completed. Total processed: {total_processed:,}")
        return total_processed

    async def write_to_storage(self, features, dataset):
        """Write features to GeoParquet in Cloud Storage"""
        if not features:
            return
        
        try:
            # Create DataFrame
            df = pd.DataFrame([f['properties'] for f in features])
            geometries = [Polygon(f['geometry']['coordinates'][0]) for f in features]
            
            # Create GeoDataFrame
            gdf = gpd.GeoDataFrame(df, geometry=geometries, crs="EPSG:25832")
            
            # Handle working/final files
            temp_working = f"/tmp/{dataset}_working.parquet"
            working_blob = self.bucket.blob(f'raw/{dataset}/working.parquet')
            
            if working_blob.exists():
                working_blob.download_to_filename(temp_working)
                existing_gdf = gpd.read_parquet(temp_working)
                logger.info(f"Appending {len(gdf):,} features to existing {len(existing_gdf):,}")
                combined_gdf = pd.concat([existing_gdf, gdf], ignore_index=True)
            else:
                combined_gdf = gdf
            
            # Write working file
            combined_gdf.to_parquet(temp_working)
            working_blob.upload_from_filename(temp_working)
            logger.info(f"Updated working file now has {len(combined_gdf):,} features")
            
            # If sync complete, create final files
            if hasattr(self, 'is_sync_complete') and self.is_sync_complete:
                logger.info("Sync complete - analyzing input geometries...")
                self.log_geometry_statistics(combined_gdf)
                
                logger.info(f"Starting merge of {len(combined_gdf):,} features...")
                start_time = time.time()
                
                # Create spatial index for efficient neighbor finding
                logger.info("Creating spatial index...")
                combined_gdf['idx'] = range(len(combined_gdf))
                spatial_index = combined_gdf.sindex
                
                # Function to check if two polygons share an edge
                def shares_edge(geom1, geom2):
                    intersection = geom1.intersection(geom2)
                    return (intersection.geom_type == 'LineString' and 
                           intersection.length >= 10)  # At least one grid cell length
                
                # Find and merge adjacent polygons
                logger.info("Finding and merging adjacent polygons...")
                merged = set()  # Keep track of merged polygons
                merged_polygons = []
                
                for idx, row in combined_gdf.iterrows():
                    if idx in merged:
                        continue
                    
                    # Find potential neighbors using spatial index
                    bounds = row.geometry.bounds
                    possible_matches_idx = list(spatial_index.intersection(bounds))
                    possible_matches = combined_gdf.iloc[possible_matches_idx]
                    
                    # Start with current polygon
                    current_group = [row.geometry]
                    merged.add(idx)
                    
                    # Check each potential neighbor
                    for match_idx, match_row in possible_matches.iterrows():
                        if match_idx != idx and match_idx not in merged:
                            if shares_edge(row.geometry, match_row.geometry):
                                current_group.append(match_row.geometry)
                                merged.add(match_idx)
                    
                    # Merge the group if we found any adjacent polygons
                    if len(current_group) > 1:
                        merged_poly = unary_union(current_group)
                    else:
                        merged_poly = current_group[0]
                    
                    merged_polygons.append(merged_poly)
                    
                    if len(merged_polygons) % 1000 == 0:
                        logger.info(f"Processed {len(merged_polygons)} groups")
                
                # Create new GeoDataFrame with merged polygons
                dissolved_gdf = gpd.GeoDataFrame(
                    geometry=merged_polygons,
                    crs=combined_gdf.crs
                )
                
                # Add wetland_id
                dissolved_gdf['wetland_id'] = range(1, len(dissolved_gdf) + 1)
                
                logger.info(f"Created {len(dissolved_gdf):,} merged polygons")
                logger.info(f"Reduced from {len(combined_gdf):,} grid cells")
                logger.info(f"Processing took {time.time() - start_time:.2f} seconds")
                
                # Log statistics for the dissolved geometries
                logger.info("\nAnalyzing dissolved geometries:")
                self.log_geometry_statistics(dissolved_gdf)
                
                # Transform and validate final geometries
                logger.info("Transforming geometries to BigQuery-compatible CRS...")
                dissolved_gdf = validate_and_transform_geometries(dissolved_gdf, 'wetlands')
                
                # Write dissolved version
                temp_dissolved = f"/tmp/{dataset}_dissolved.parquet"
                dissolved_gdf.to_parquet(temp_dissolved)
                dissolved_blob = self.bucket.blob(f'raw/{dataset}/dissolved_current.parquet')
                dissolved_blob.upload_from_filename(temp_dissolved)
                
                # Cleanup
                working_blob.delete()
                os.remove(temp_dissolved)
            
            # Cleanup working file
            if os.path.exists(temp_working):
                os.remove(temp_working)
            
        except Exception as e:
            logger.error(f"Error writing to storage: {str(e)}")
            raise

    async def fetch(self):
        """Not implemented - using sync() directly"""
        raise NotImplementedError("This source uses sync() directly")

    async def _fetch_chunk(self, session, start_index):
        """Fetch a chunk of features starting at the given index"""
        try:
            async with self.request_semaphore:
                params = self._get_params(start_index)
                async with session.get(
                    self.config['url'], 
                    params=params, 
                    timeout=self.request_timeout
                ) as response:
                    text = await response.text()
                    root = ET.fromstring(text)
                    
                    features = [
                        self._parse_feature(f) 
                        for f in root.findall('.//natur:kulstof2022', self.namespaces)
                    ]
                    return [f for f in features if f]
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching batch at {start_index}")
            return None
        except Exception as e:
            logger.error(f"Error fetching batch at {start_index}: {str(e)}")
            return None

================
File: src/sources/static/crops/parser.py
================
import pandas as pd
import pdfplumber
from pathlib import Path
from ....base import Source
import os

class CropCodes(Source):
    """Danish Agricultural Crop Codes parser"""
    
    async def fetch(self) -> pd.DataFrame:
        data_path = Path(__file__).parent / 'bilag-1-afgroedekoder-2023.pdf'
        if not data_path.exists():
            raise FileNotFoundError(f"Crop codes data not found at {data_path}")
            
        # Initialize lists to store the data
        codes = []
        names = []
        categories = []
        
        current_code = None
        current_name = None
        current_category = None
        
        with pdfplumber.open(data_path) as pdf:
            # Skip first page header
            start_processing = False
            
            # Iterate through all pages
            for page in pdf.pages:
                text = page.extract_text()
                lines = text.split('\n')
                
                for line in lines:
                    # Skip until we find the header
                    if not start_processing:
                        if "Afgrdekode Navn Engangskompensationskategori" in line:
                            start_processing = True
                        continue
                    
                    # Skip footer
                    if "Miljstyrelsen" in line or "* Stjernemarkering" in line:
                        continue
                        
                    line = line.strip()
                    if not line:
                        continue
                    
                    # Try to parse as new entry
                    parts = line.split(' ', 1)
                    if parts[0].isdigit():
                        # Process previous entry if exists
                        if current_code is not None and current_name is not None and current_category is not None:
                            codes.append(current_code)
                            names.append(current_name.strip())
                            categories.append(current_category.strip())
                        
                        # Start new entry
                        current_code = int(parts[0])
                        if len(parts) > 1:
                            rest = parts[1].strip()
                            # Look for category at the end
                            words = rest.split()
                            if words:
                                last_word = words[-1]
                                if any(cat in last_word for cat in ["Omdrift", "Natur", "grs"]):
                                    current_category = last_word
                                    current_name = ' '.join(words[:-1])
                                else:
                                    current_category = None
                                    current_name = rest
                    else:
                        # Continuation of previous entry
                        words = line.split()
                        if words:
                            last_word = words[-1]
                            if any(cat in last_word for cat in ["Omdrift", "Natur", "grs"]):
                                current_category = last_word
                                if current_name:
                                    current_name += " " + ' '.join(words[:-1])
                                else:
                                    current_name = ' '.join(words[:-1])
                            else:
                                if current_name:
                                    current_name += " " + line
                                else:
                                    current_name = line
            
            # Process last entry
            if current_code is not None and current_name is not None and current_category is not None:
                codes.append(current_code)
                names.append(current_name.strip())
                categories.append(current_category.strip())
        
        # Create DataFrame
        df = pd.DataFrame({
            'Afgrdekode': codes,
            'Navn': names,
            'Engangskompensationskategori': categories
        })
        
        # Sort by crop code
        df = df.sort_values('Afgrdekode').reset_index(drop=True)
        
        # Save with proper encoding and quoting
        if 'test_parser' in str(data_path):
            df.to_csv('crop_codes.csv', 
                     index=False, 
                     encoding='utf-8-sig',
                     quoting=1,  # Quote strings
                     quotechar='"',
                     sep=',')
            
            # Print some stats
            print(f"\nFound {len(df)} crop codes")
            print("\nFirst few entries:")
            print(df.head())
            print("\nLast few entries:")
            print(df.tail())
        
        return df

    async def sync(self):
        """Sync the crop codes data"""
        df = await self.fetch()
        
        # Write to temporary local parquet file
        temp_file = "/tmp/crop_codes_current.parquet"
        df.to_parquet(temp_file)
        
        # Upload to storage
        blob = self.bucket.blob(f'raw/crops/current.parquet')
        blob.upload_from_filename(temp_file)
        
        # Cleanup
        os.remove(temp_file)
        
        return len(df)

================
File: src/sources/static/fertilizer/readme.md
================
# Agricultural Nitrogen Field-Level Analysis

## Purpose
Distributes farm-level nitrogen quotas and consumption patterns to individual fields while maintaining compliance with agricultural regulations. Translates farm-level fertilizer data to field-level estimates based on each farm's quota adjustments and consumption patterns.

## Input Data Structure

### Farm-Level File
Farm-wide nitrogen data with columns:
- CVR: Unique farm identifier
- F_504_1: Base nitrogen quota
- F_505_1: Additional nitrogen quota
- F_512: Corrected quota (when regulatory adjustments apply)
- F_901: Total nitrogen consumption
- F_610: Animal fertilizer usage
- F_706_1: Mineral fertilizer usage
- F_193: Other organic fertilizer usage

Note: Negative values in fertilizer columns represent transfers or sales to other farms.

### Field-Level File
Individual field data:
- CVR: Farm identifier
- Marknummer: Field ID number
- Areal: Total field area
- Harmoni Areal: Area eligible for fertilizer application
- N Kvote Mark: Field's base nitrogen quota

## Analysis Steps

### 1. Data Preparation

#### Farm-Level Data (df1)
- Convert all quota and fertilizer columns to numeric values
- Calculate original quota (F_504_1 + F_505_1)
- Identify final quota (use F_512 if available, otherwise use original)
- Calculate quota adjustment factor = final quota / original quota
- Calculate fertilizer type shares from total consumption
- Handle negative values:
  * Negative fertilizer values reflect transfers/sales
  * These affect farm's total available nitrogen

#### Field-Level Data (df2)
- Remove header rows
- Convert numeric columns: CVR, Areal, Harmoni Areal, N Kvote Mark
- Verify field areas and quotas

### 2. Farm-Level Calculations

#### Quota Processing
1. Original quota = F_504_1 + F_505_1
2. Final quota = F_512 (if available) or original quota
3. Adjustment factor = final quota / original quota
   * Default to 1 if original quota is zero

#### Consumption Analysis
1. Farm performance = total consumption / final quota
2. Fertilizer distribution:
   * Animal share = F_610 / total consumption
   * Mineral share = F_706_1 / total consumption
   * Organic share = F_193 / total consumption

### 3. Field-Level Distribution

For each field:

1. Harmonized Area Check
   * If zero: field ineligible for fertilizer application
   * If non-zero: proceed with calculations

2. Quota Adjustment
   * Adjusted quota = N Kvote Mark  farm's adjustment factor

3. Consumption Calculation
   * Expected consumption = adjusted quota  farm performance
   * Consumption percentage = (consumption / adjusted quota)  100

4. Fertilizer Type Distribution
   * Apply farm's fertilizer shares to field consumption
   * Maintains farm's fertilizer mix at field level

### 4. Validation Checks

#### Farm Level
- Complete quota information (original or corrected)
- Fertilizer shares sum to 100%
- Transfer/sale amounts (negative values) verification
- Total consumption versus quota alignment

#### Field Level
- Harmonized area  total area
- Field quota presence
- Consumption percentage reasonability
- CVR matches in both datasets

### 5. Output Documentation
Each field record includes:
- Original field data
- Adjusted quotas
- Calculated consumption
- Fertilizer type breakdowns
- Special cases:
  * Zero harmonized area
  * Missing quota data
  * High consumption percentages

## Key Assumptions
1. Farm-level patterns apply uniformly to fields
2. Field consumption follows farm performance ratio
3. Negative values represent legitimate transfers/sales
4. Zero harmonized area means no fertilizer allocation

## Limitations
- Cannot account for field-specific practices
- Assumes uniform distribution within farms
- Based on reported data
- No consideration of soil types or crop requirements

## Notes
- Negative fertilizer values are legitimate when representing transfers
- Field-level estimates maintain farm totals
- Analysis focuses on nitrogen distribution patterns
- Results should be viewed as estimates based on farm patterns

## Exploration code:
https://colab.research.google.com/drive/1AghsRm8Dsfk3kVR-tUe2BUS3E9v7KY2I?usp=sharing

================
File: src/sources/static/pesticides/parser.py
================
import pandas as pd
from pathlib import Path
from ....base import Source
import os
import re

class Pesticides(Source):
    """Danish Pesticide Data parser"""

    # Using only English column names
    EXPECTED_COLUMNS = [
        'CompanyName',
        'CompanyRegistrationNumber',
        'StreetName',
        'StreetBuildingIdentifier',
        'FloorIdentifier',
        'PostCodeIdentifier',
        'City',
        'AcreageSize',
        'AcreageUnit',
        'Name',
        'Code',
        'PesticideName',
        'PesticideRegistrationNumber',
        'DosageQuantity',
        'DosageUnit',
        'NoPesticides'
    ]

    # Unit mappings based on README
    ACREAGE_UNITS = {
        1: 'm2',
        2: 'hektar'
    }

    DOSAGE_UNITS = {
        1: 'Gram',
        2: 'Kg',
        3: 'Mililiter',
        4: 'Liter',
        5: 'Tablets'
    }

    # Rename columns to match other sources
    COLUMN_RENAMING = {
        'CompanyRegistrationNumber': 'cvr_number',
        'Name': 'crop_type',
        'Code': 'crop_code'
    }

    SNAKE_NAMES = {
        'CompanyName': 'company_name',
        'StreetName': 'street_name',
        'StreetBuildingIdentifier': 'street_building_identifier',
        'FloorIdentifier': 'floor_identifier',
        'PostCodeIdentifier': 'post_code_identifier',
        'City': 'city',
        'AcreageSize': 'acreage_size',
        'AcreageUnit': 'acreage_unit',
        'PesticideName': 'pesticide_name',
        'PesticideRegistrationNumber': 'pesticide_registration_number',
        'DosageQuantity': 'dosage_quantity',
        'DosageUnit': 'dosage_unit',
        'NoPesticides': 'no_pesticides'
    }

    def _extract_plan_year(self, filename: str) -> str:
        """Extract plan-year from the filename"""
        # Assuming the filename contains the plan-year in the format 'YYYY_YYYY'
        match = re.search(r'(\d{4})_(\d{4})', filename)
        if match:
            return f"{match.group(1)}-{match.group(2)}"
        else:
            raise ValueError(f"Plan-year not found in filename: {filename}")

    def _read_excel_files(self) -> list[pd.DataFrame]:
        """Read all Excel files in the current directory"""
        current_dir = Path(__file__).parent

        dfs = []
        for file in current_dir.glob('*.xlsx'):
            try:
                df = pd.read_excel(file, sheet_name=0, engine='openpyxl')
                df['source_file'] = file.name
                df['plan_year'] = self._extract_plan_year(file.name)
                dfs.append(df)
            except Exception as e:
                print(f"Error reading {file}: {e}")

        if not dfs:
            raise FileNotFoundError("No Excel files found in directory")

        return dfs

    def _rename_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Rename columns based on COLUMN_RENAMING and SNAKE_NAMES"""
        combined_renaming = {**self.COLUMN_RENAMING, **self.SNAKE_NAMES}
        return df.rename(columns=combined_renaming)

    def _clean_and_standardize(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize the dataframe"""
        # Ensure all expected columns exist
        missing_cols = set(self.EXPECTED_COLUMNS) - set(df.columns)
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        # Rename columns
        df = self._rename_columns(df)

        # Map unit codes to readable values
        df['AcreageUnit'] = df['AcreageUnit'].map(self.ACREAGE_UNITS)
        df['DosageUnit'] = df['DosageUnit'].map(self.DOSAGE_UNITS)
        df['NoPesticides'] = df['NoPesticides'].map({'SAND': True, 'FALSK': False})

        return df

    async def fetch(self) -> pd.DataFrame:
        # Get list of DataFrames from Excel files
        dfs = self._read_excel_files()

        # Merge all DataFrames into one
        df = pd.concat(dfs, ignore_index=True)

        # Clean data (map units, standardize values)
        df = self._clean_and_standardize(df)
        df = df.fillna('')  # Replace NaN with empty string

        # Optional: Print debug info in test environment
        if 'test_parser' in os.environ.get('ENVIRONMENT', ''):
            print(f"\nFound {len(df)} pesticide entries")
            print("\nColumns:", df.columns.tolist())
            print("\nFirst few entries:")
            print(df.head())

        return df

    async def sync(self):
        """Sync pesticide data"""
        df = await self.fetch()

        # Write to temporary local parquet file
        temp_file = "/tmp/pesticides_current.parquet"
        df.to_parquet(temp_file)

        # Upload to storage
        blob = self.bucket.blob(f'raw/pesticides/current.parquet')
        blob.upload_from_filename(temp_file)

        # Cleanup
        os.remove(temp_file)

        return len(df)

================
File: src/sources/static/pesticides/README.md
================
# Pesticide data

This data was requested from and granted by the The Danish Environmental Protection Agency.

It was downloaded on the 12th of december 2024.

## Note on updating the data

The data for the year 2023/2024 has its deadline on the 31st of march 2025. By then, it should be trivial to request the single sheet with the same columns as in the data already received.

## Column descriptions

With the data we also received a file describing the columns and units in the data.

### Columns

Variable                   |Danish description |English description
---------------------------|-------------------|--------------------
CompanyName                |Virksomhedsnavn
CompanyRegistrationNumber  |CVR nr.
StreetName                 |Vejnavn
StreetBuildingIdentifier   |vej nummer
FloorIdentifier            |etage
PostCodeIdentifier         |Postnummer         |Postal code
City                       |By
AcreageSize                |Behandlet areal    |Acreage size treated
AcreageUnit                |Enhed
Name                       |Afgrdenavn        |Crop name
Code                       |Afgrdekode        |Crop code
PesticideName              |Navn pesticid
PesticideRegistrationNumber|Reg. nr. (pesticid)|Pesticide ID number
DosageQuantity             |Dosis
DosageUnit                 |Enhed dosis
NoPesticides               |Anvender pesticider

### Units

AcreageUnit   | Values
--------------|------------
1             | m2
2             | hektar

DosageUnit    | Values
--------------|------------
1             | Gram
2             | Kg
3             | Mililiter
4             | Liter
5             | Tablets

NoPesticides  | Values
--------------|-----------
FALSK         | Pesticide use
SAND          | NO pesticide use

================
File: src/sources/static/subsidies/readme.me
================
## Documentation to understand subsidies

Kolonne F er sttteordningerne:

MIKO                  Minivdomrder - kompensation (De minimis)
MRDM                Mlrettet regulering (De minimis)
MREG                Mlrettede efterafgrder (De minimis)
MRSB                 Mlrettet regulering - Supplerende betaling (De minimis)
PTK                    Paragraf 3 Kompensation (De minimis)
 

Kolonne I og M hrer tt sammen, og beskriver hvilken slags post det er i vores system. Her om det er en udbetaling fra os, en tilbagebetaling til os, afskrivning grundet konkurs, om der er kommet renter p, osv.



I kolonne I, s er det en blanding af CVR og journalnummer, der viser landbrugeren har lavet en tilbagebetaling til os.

---

Du kan godt gange satsen p det tilskudsberettiget areal og dermed beregne tilskudsbelbet  men det er ikke ndvendigvis det belb, der bliver udbetalt, da der p nogle sager kan ske en reduktion (eks. Sanktion, konditionalitet mv.)

-- 

Subsidy rates for 2023: https://lbst.dk/alle-nyheder/nyheder/2023/dec/se-de-endelige-tilskudssatser-til-grundbetaling-og-bio-ordninger-i-2023

================
File: src/sources/static/wetlands/parser.py
================
import pandas as pd
import geopandas as gpd
from pathlib import Path
from ...base import Source

class Wetlands(Source):
    """Danish Wetlands shapefile parser"""
    
    async def fetch(self) -> pd.DataFrame:
        data_path = Path(__file__).parent / 'data' / f"{self.config['filename']}.shp"
        if not data_path.exists():
            raise FileNotFoundError(f"Wetlands data not found at {data_path}")
            
        gdf = gpd.read_file(data_path)
        gdf = gdf.rename(columns={
            'OBJECTID': 'wetland_id',
            'Kulstof': 'carbon_content',
            'Areal_ha': 'area_ha'
        })
        
        return gdf[['wetland_id', 'carbon_content', 'area_ha', 'geometry']]

================
File: src/sources/utils/geometry_validator.py
================
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry.polygon import orient
import geopandas as gpd
import logging
from shapely.ops import unary_union

logger = logging.getLogger(__name__)

def is_valid_for_bigquery(geom) -> bool:
    """
    Check if geometry meets BigQuery geography requirements:
    - No self-intersections
    - No duplicate vertices
    - No empty rings
    - Edges can't cross
    - Proper ring orientation
    """
    try:
        if not geom.is_valid or not geom.is_simple:
            return False
            
        if isinstance(geom, (Polygon, MultiPolygon)):
            # Check each polygon
            polygons = geom.geoms if isinstance(geom, MultiPolygon) else [geom]
            
            for poly in polygons:
                # Check exterior ring
                ext_coords = list(poly.exterior.coords)
                if len(ext_coords) < 4:  # Need at least 4 points (first = last)
                    return False
                    
                # Check for duplicate consecutive vertices
                for i in range(len(ext_coords)-1):
                    if ext_coords[i] == ext_coords[i+1]:
                        return False
                
                # Check interior rings
                for interior in poly.interiors:
                    int_coords = list(interior.coords)
                    if len(int_coords) < 4:
                        return False
                    
                    # Check for duplicate consecutive vertices in interior
                    for i in range(len(int_coords)-1):
                        if int_coords[i] == int_coords[i+1]:
                            return False
        
        return True
        
    except Exception as e:
        logger.error(f"Error checking BigQuery validity: {str(e)}")
        return False

def validate_and_transform_geometries(gdf: gpd.GeoDataFrame, dataset_name: str) -> gpd.GeoDataFrame:
    """
    Validates and transforms geometries for BigQuery compatibility.
    
    This function performs cleanup operations to ensure geometries are valid
    and meet BigQuery's requirements. All operations are performed in UTM zone 32N (EPSG:25832)
    where possible to maintain geometric precision for Danish data.
    
    The process:
    1. Converts to UTM (EPSG:25832)
    2. Cleans geometries with buffer(0) in UTM
    3. Converts to WGS84 (EPSG:4326) for BigQuery
    4. Final cleanup and validation in WGS84
    
    Args:
        gdf: GeoDataFrame with geometries in any CRS
        dataset_name: Name of dataset for logging
    
    Returns:
        GeoDataFrame with valid geometries in EPSG:4326
        
    Raises:
        ValueError: If geometries cannot be made valid
    """
    try:
        initial_count = len(gdf)
        logger.info(f"{dataset_name}: Starting validation with {initial_count} features")
        logger.info(f"{dataset_name}: Input CRS: {gdf.crs}")
        
        # Convert to UTM
        if gdf.crs != "EPSG:25832":
            logger.info(f"{dataset_name}: Converting to UTM (EPSG:25832) for better precision")
            gdf = gdf.to_crs("EPSG:25832")
        
        # Initial cleanup in UTM
        logger.info(f"{dataset_name}: Performing initial cleanup")
        gdf.geometry = gdf.geometry.apply(lambda g: g.buffer(0))
        
        # Validate in UTM
        invalid_mask = ~gdf.geometry.is_valid
        if invalid_mask.any():
            logger.warning(f"{dataset_name}: Found {invalid_mask.sum()} invalid geometries after cleanup")
            raise ValueError(f"Found {invalid_mask.sum()} invalid geometries after cleanup")
        
        # Convert to WGS84
        logger.info(f"{dataset_name}: Converting to WGS84 (EPSG:4326)")
        gdf = gdf.to_crs("EPSG:4326")
        
        # Final cleanup in WGS84
        gdf.geometry = gdf.geometry.apply(lambda g: g.buffer(0))
        
        # Final validation
        invalid_wgs84 = ~gdf.geometry.is_valid
        if invalid_wgs84.any():
            raise ValueError(f"Found {invalid_wgs84.sum()} invalid geometries after WGS84 conversion")
        
        # Check for self-intersections
        self_intersecting = ~gdf.geometry.is_simple
        if self_intersecting.any():
            logger.warning(f"{dataset_name}: Found {self_intersecting.sum()} self-intersecting geometries in WGS84")
            raise ValueError(f"Found {self_intersecting.sum()} self-intersecting geometries")
        
        # Remove nulls and empty geometries
        gdf = gdf.dropna(subset=['geometry'])
        gdf = gdf[~gdf.geometry.is_empty]
        
        final_count = len(gdf)
        removed_count = initial_count - final_count
        
        logger.info(f"{dataset_name}: Validation complete")
        logger.info(f"{dataset_name}: Initial features: {initial_count}")
        logger.info(f"{dataset_name}: Valid features: {final_count}")
        logger.info(f"{dataset_name}: Removed features: {removed_count}")
        logger.info(f"{dataset_name}: Output CRS: {gdf.crs}")
        
        return gdf
        
    except Exception as e:
        logger.error(f"{dataset_name}: Error in geometry validation: {str(e)}")
        raise

================
File: src/sources/base.py
================
from abc import ABC, abstractmethod
from google.cloud import storage
import geopandas as gpd
from shapely.geometry import shape
import pyarrow as pa
import logging
from typing import Optional, Dict, Any
import time
import os
import pandas as pd
import tempfile
from contextlib import contextmanager
from .utils.geometry_validator import validate_and_transform_geometries

logger = logging.getLogger(__name__)

class BaseSource(ABC):
    """Base class for all data sources that fetch and store raw data"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.storage_client = storage.Client()
        self.bucket = self.storage_client.bucket(config.get('bucket', 'landbrugsdata-raw-data'))
    
    @contextmanager
    def get_temp_file(self):
        """Create a temporary file that's automatically cleaned up"""
        temp = tempfile.NamedTemporaryFile(delete=False)
        try:
            yield temp
        finally:
            temp.close()
            os.unlink(temp.name)
    
    @property
    @abstractmethod
    def source_id(self) -> str:
        """Unique identifier for this source"""
        pass
        
    @abstractmethod
    async def fetch(self) -> pd.DataFrame:
        """Fetch raw data from source"""
        pass
        
    @abstractmethod
    async def sync(self) -> Optional[int]:
        """Full sync process: fetch and store"""
        pass
        
    async def store(self, df: pd.DataFrame) -> bool:
        """Store raw data in GCS"""
        try:
            if df.empty:
                logger.warning(f"Empty DataFrame for {self.source_id}, skipping upload")
                return False
            
            # Save to temp file
            temp_file = f"/tmp/{self.source_id}.parquet"
            df.to_parquet(temp_file, index=False)
            
            # Upload to Cloud Storage
            blob = self.bucket.blob(f'raw/{self.source_id}/current.parquet')
            blob.upload_from_filename(temp_file)
            
            # Clean up
            os.remove(temp_file)
            
            return True
        except Exception as e:
            logger.error(f"Error storing data for {self.source_id}: {str(e)}")
            return False

class GeospatialSource(BaseSource):
    """Base class for geospatial data sources that fetch and store raw data"""
    
    async def sync(self) -> Optional[int]:
        """Default implementation for geospatial sources"""
        try:
            df = await self.fetch()
            if await self.store(df):
                return len(df)
            return None
        except Exception as e:
            logger.error(f"Sync failed for {self.source_id}: {str(e)}")
            return None
    
    async def store(self, df: pd.DataFrame, dataset: str = None) -> bool:
        """Store raw data in GCS"""
        try:
            # Use dataset name if provided, otherwise use source_id
            path = f"raw/{dataset or self.source_id}/current.parquet"
            
            with self.get_temp_file() as temp_file:
                if isinstance(df, gpd.GeoDataFrame):
                    df.to_parquet(temp_file.name)
                else:
                    # Convert to GeoDataFrame if it's not already
                    gdf = gpd.GeoDataFrame(df)
                    gdf = validate_and_transform_geometries(gdf, dataset or self.source_id)
                    gdf.to_parquet(temp_file.name)
                
                self.bucket.blob(path).upload_from_filename(temp_file.name)
                
            return True
        except Exception as e:
            logger.error(f"Error storing data for {self.source_id}: {str(e)}")
            return False

================
File: src/config.py
================
SOURCES = {
    "agricultural_fields": {
        "name": "Danish Agricultural Fields",
        "type": "arcgis",
        "description": "Weekly updated agricultural field data",
        "urls": {
            "fields": "https://kort.vd.dk/server/rest/services/Grunddata/Marker_og_Markblokke/MapServer/12/query",
            "blocks": "https://kort.vd.dk/server/rest/services/Grunddata/Marker_og_Markblokke/MapServer/2/query",
            "fields_2023": "https://kort.vd.dk/server/rest/services/Grunddata/Marker_og_Markblokke/MapServer/13/query",
            "blocks_2023": "https://kort.vd.dk/server/rest/services/Grunddata/Marker_og_Markblokke/MapServer/7/query",
            "blocks_2024": "https://kort.vd.dk/server/rest/services/Grunddata/Marker_og_Markblokke/MapServer/6/query"
        },
        "frequency": "weekly",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data"
    },
    "wetlands": {
        "name": "Danish Wetlands Map",
        "type": "wfs",
        "description": "Wetland areas from Danish EPA",
        "url": "https://wfs2-miljoegis.mim.dk/natur/wfs",
        "layer": "natur:kulstof2022",
        "frequency": "static",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data",
        "create_dissolved": True
    },
    "cadastral": {
        "name": "Danish Cadastral Properties",
        "type": "wfs",
        "description": "Current real estate property boundaries",
        "url": "https://wfs.datafordeler.dk/MATRIKLEN2/MatGaeldendeOgForeloebigWFS/1.0.0/WFS",
        "frequency": "weekly",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data"
    },
    "water_projects": {
        "name": "Danish Water Projects",
        "type": "wfs",
        "description": "Water projects from various Danish programs",
        "url": "https://geodata.fvm.dk/geoserver/wfs",
        "url_mim": "https://wfs2-miljoegis.mim.dk/vandprojekter/wfs",
        "url_nst": "https://gis.nst.dk/server/rest/services/autonom/Klima_lavbund_demarkation___offentlige_projekter/FeatureServer",
        "frequency": "weekly",
        "enabled": True,
        "create_combined": True,
        "combined_timeout": 3600,
        "bucket": "landbrugsdata-raw-data",
        "create_dissolved": True
    },
    "crops": {
        "name": "Danish Agricultural Crop Codes",
        "type": "static",
        "description": "Reference data for crop codes and compensation categories",
        "frequency": "static",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data"
    },
    "property_owners": {
        "name": "Danish Property Owners",
        "frequency": "weekly",
        "enabled": True,
        "type": "sftp",
        "bucket": "landbrugsdata-raw-data",
        "raw_folder": "raw",
        "processed_folder": "processed"
    },
    "chr_data": {
        "name": "Danish CHR Data",
        "type": "soap",
        "description": "Animal data from CHR (Central Husbandry Register)",
        "frequency": "weekly",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data",
        "wsdl_urls": {
            "stamdata": "https://ws.fvst.dk/service/CHR_stamdataWS?WSDL",
            "besaetning": "https://ws.fvst.dk/service/CHR_besaetningWS?wsdl",
            "ejendom": "https://ws.fvst.dk/service/CHR_ejendomWS?wsdl",
            "ejer": "https://ws.fvst.dk/service/CHR_ejerWS?wsdl"
        }
    },
    "bnbo_status": {
        "name": "Danish BNBO Status",
        "type": "wfs",
        "description": "Municipal status for well-near protection areas (BNBO)",
        "url": "https://arealeditering-dist-geo.miljoeportal.dk/geoserver/wfs",
        "layer": "dai:status_bnbo",
        "frequency": "weekly",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data",
        "create_dissolved": True
    },
    "antibiotics": {
        "name": "Danish VetStat Data",
        "type": "soap",
        "description": "Antibiotic usage data from VetStat",
        "frequency": "monthly",
        "enabled": True,
        "bucket": "landbrugsdata-raw-data",
        "url": "https://vetstat.fvst.dk/vetstat/services/external/CHRWS"
    }
}

================
File: src/main.py
================
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
from datetime import datetime, timedelta

from .config import SOURCES
from .sources.parsers import get_source_handler

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Danish Agricultural Data API",
    description="API serving agricultural and environmental data"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/sources")
async def list_sources():
    """List all available data sources"""
    return {
        source_id: {
            "name": config["name"],
            "type": config["type"],
            "description": config["description"],
            "frequency": config["frequency"]
        }
        for source_id, config in SOURCES.items()
        if config["enabled"]
    }

@app.get("/data/{source_id}")
async def get_data(source_id: str):
    """Get data from a specific source"""
    if source_id not in SOURCES:
        raise HTTPException(status_code=404, detail="Source not found")
        
    config = SOURCES[source_id]
    if not config["enabled"]:
        raise HTTPException(status_code=403, detail="Source is disabled")
    
    try:
        # Get appropriate source handler
        source = get_source_handler(source_id, config)
        if not source:
            raise HTTPException(status_code=501, detail="Source not implemented")
        
        # Fetch data
        df = await source.fetch()
        
        # Get next update time for weekly sources
        next_update = None
        if config["frequency"] == "weekly":
            next_update = (datetime.now() + timedelta(days=7)).isoformat()
        
        return JSONResponse(
            content=df.to_dict(orient="records"),
            headers={
                "X-Last-Updated": datetime.now().isoformat(),
                "X-Next-Update": next_update,
                "X-Update-Frequency": config["frequency"]
            }
        )
        
    except Exception as e:
        logger.error(f"Error fetching data from {source_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

================
File: tests/test_cadastral_db.py
================
import asyncio
import logging
import os
from pathlib import Path
import sys
from dotenv import load_dotenv
import asyncpg
from datetime import datetime

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.config import SOURCES
from test_cadastral import TestCadastral  # Import our working test class

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_database_operations():
    """Test database operations with fetched data"""
    load_dotenv()
    
    # Get database credentials
    db_host = os.getenv('DB_HOST')
    db_name = os.getenv('DB_NAME')
    db_user = os.getenv('DB_USER')
    db_password = os.getenv('DB_PASSWORD')
    
    if not all([db_host, db_name, db_user, db_password]):
        raise ValueError("Missing database configuration")
    
    conn = None
    try:
        # Connect to database
        conn = await asyncpg.connect(
            host=db_host,
            database=db_name,
            user=db_user,
            password=db_password
        )
        logger.info("Database connection established")
        
        # Create test table
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS cadastral_properties_test (
                bfe_number INTEGER PRIMARY KEY,
                geometry GEOMETRY(MULTIPOLYGON, 25832)
            )
        """)
        logger.info("Test table created")
        
        # Fetch some test data
        cadastral = TestCadastral(SOURCES["cadastral"])
        features = await cadastral.test_fetch_features()
        
        if not features:
            logger.error("No features to test with")
            return False
            
        # Insert features
        inserted = 0
        for feature in features:
            try:
                await conn.execute("""
                    INSERT INTO cadastral_properties_test (bfe_number, geometry)
                    VALUES ($1, ST_GeomFromText($2, 25832))
                    ON CONFLICT (bfe_number) DO UPDATE 
                    SET geometry = EXCLUDED.geometry
                """, feature['bfe_number'], feature['geometry'])
                inserted += 1
            except Exception as e:
                logger.error(f"Error inserting feature {feature['bfe_number']}: {str(e)}")
                continue
        
        logger.info(f"Successfully inserted {inserted} features")
        
        # Verify data
        count = await conn.fetchval(
            "SELECT COUNT(*) FROM cadastral_properties_test"
        )
        logger.info(f"Total records in test table: {count}")
        
        # Test geometry validity
        valid_geoms = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM cadastral_properties_test 
            WHERE ST_IsValid(geometry)
        """)
        logger.info(f"Valid geometries: {valid_geoms}/{count}")
        
        return True
        
    except Exception as e:
        logger.error(f"Database test failed: {str(e)}", exc_info=True)
        return False
    finally:
        if conn:
            # Clean up test table
            try:
                await conn.execute("DROP TABLE IF EXISTS cadastral_properties_test")
                logger.info("Test table cleaned up")
            except Exception as e:
                logger.error(f"Error cleaning up test table: {str(e)}")
            
            await conn.close()
            logger.info("Database connection closed")

async def main():
    """Run database test"""
    try:
        logger.info("Starting cadastral database test...")
        success = await test_database_operations()
        logger.info(f"Database test {'succeeded' if success else 'failed'}")
        return success
    except Exception as e:
        logger.error(f"Test failed: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

================
File: tests/test_cadastral_sync.py
================
import asyncio
import logging
import os
from pathlib import Path
import sys
from dotenv import load_dotenv
import asyncpg
from datetime import datetime

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.config import SOURCES
from test_cadastral import TestCadastral

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_cadastral_sync():
    """Test full cadastral sync process"""
    load_dotenv()
    
    conn = None
    try:
        # Connect to database
        conn = await asyncpg.connect(
            host='localhost',
            database='landbrugsdata',
            user='landbrugsdata',
            password=os.getenv('DB_PASSWORD')
        )
        logger.info("Database connection established")
        
        # Create cadastral table
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS cadastral_properties (
                bfe_number INTEGER PRIMARY KEY,
                business_event TEXT,
                business_process TEXT,
                latest_case_id TEXT,
                id_local TEXT,
                id_namespace TEXT,
                registration_from TIMESTAMP WITH TIME ZONE,
                effect_from TIMESTAMP WITH TIME ZONE,
                authority TEXT,
                is_worker_housing BOOLEAN,
                is_common_lot BOOLEAN,
                has_owner_apartments BOOLEAN,
                is_separated_road BOOLEAN,
                agricultural_notation TEXT,
                geometry GEOMETRY(MULTIPOLYGON, 25832)
            );
            
            CREATE INDEX IF NOT EXISTS cadastral_properties_geometry_idx 
            ON cadastral_properties USING GIST (geometry);
        """)
        logger.info("Cadastral table created/verified")
        
        # Initialize cadastral parser
        cadastral = TestCadastral(SOURCES["cadastral"])
        cadastral.page_size = 10  # Small batch for testing
        
        # Fetch and process features
        features = await cadastral.test_fetch_features()
        if not features:
            logger.error("No features fetched")
            return False
            
        logger.info(f"Fetched {len(features)} features")
        
        # Insert features
        inserted = 0
        for feature in features:
            try:
                await conn.execute("""
                    INSERT INTO cadastral_properties (
                        bfe_number, geometry
                    ) VALUES ($1, ST_GeomFromText($2, 25832))
                    ON CONFLICT (bfe_number) DO UPDATE 
                    SET geometry = EXCLUDED.geometry
                """, feature['bfe_number'], feature['geometry'])
                inserted += 1
            except Exception as e:
                logger.error(f"Error inserting feature {feature['bfe_number']}: {str(e)}")
                continue
        
        logger.info(f"Successfully inserted {inserted} features")
        
        # Verify data
        count = await conn.fetchval("SELECT COUNT(*) FROM cadastral_properties")
        logger.info(f"Total records in cadastral table: {count}")
        
        # Test geometry validity
        valid_geoms = await conn.fetchval("""
            SELECT COUNT(*) 
            FROM cadastral_properties 
            WHERE ST_IsValid(geometry)
        """)
        logger.info(f"Valid geometries: {valid_geoms}/{count}")
        
        return True
        
    except Exception as e:
        logger.error(f"Sync test failed: {str(e)}", exc_info=True)
        return False
    finally:
        if conn:
            await conn.close()
            logger.info("Database connection closed")

async def main():
    """Run sync test"""
    try:
        logger.info("Starting cadastral sync test...")
        success = await test_cadastral_sync()
        logger.info(f"Sync test {'succeeded' if success else 'failed'}")
        return success
    except Exception as e:
        logger.error(f"Test failed: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

================
File: tests/test_cadastral.py
================
import asyncio
import logging
import os
from pathlib import Path
import sys
from dotenv import load_dotenv
import aiohttp
import xml.etree.ElementTree as ET
from datetime import datetime
from shapely.geometry import Polygon, MultiPolygon
from shapely.wkt import dumps as wkt_dumps

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.config import SOURCES
from src.base import Source, clean_value

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TestCadastral(Source):
    def __init__(self, config):
        super().__init__(config)
        load_dotenv()
        self.username = os.getenv('DATAFORDELER_USERNAME')
        self.password = os.getenv('DATAFORDELER_PASSWORD')
        if not self.username or not self.password:
            raise ValueError("Missing DATAFORDELER_USERNAME or DATAFORDELER_PASSWORD environment variables")
        
        self.page_size = 10  # Small page size for testing
        self.namespaces = {
            'wfs': 'http://www.opengis.net/wfs/2.0',
            'mat': 'http://data.gov.dk/schemas/matrikel/1',
            'gml': 'http://www.opengis.net/gml/3.2'
        }

    def _get_params(self, count=10):
        """Get WFS request parameters"""
        return {
            'username': self.username,
            'password': self.password,
            'SERVICE': 'WFS',
            'REQUEST': 'GetFeature',
            'VERSION': '2.0.0',
            'TYPENAMES': 'mat:SamletFastEjendom_Gaeldende',
            'SRSNAME': 'EPSG:25832',
            'count': str(count)
        }

    def _parse_geometry(self, geom_elem):
        """Parse GML geometry to WKT"""
        try:
            pos_lists = geom_elem.findall('.//gml:posList', self.namespaces)
            if not pos_lists:
                return None

            polygons = []
            for pos_list in pos_lists:
                if not pos_list.text:
                    continue

                # Convert coordinates to pairs
                coords = [float(x) for x in pos_list.text.strip().split()]
                pairs = [(coords[i], coords[i+1]) 
                        for i in range(0, len(coords), 3)]  # Skip Z coordinate

                if len(pairs) < 4:
                    continue

                try:
                    polygon = Polygon(pairs)
                    if polygon.is_valid:
                        polygons.append(polygon)
                except Exception as e:
                    logger.warning(f"Error creating polygon: {str(e)}")
                    continue

            if not polygons:
                return None

            final_geom = MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]
            return wkt_dumps(final_geom)

        except Exception as e:
            logger.error(f"Error parsing geometry: {str(e)}")
            return None

    def _parse_feature(self, feature_elem):
        """Parse a single feature"""
        try:
            result = {}
            
            # Parse BFE number
            bfe_elem = feature_elem.find('.//mat:BFEnummer', self.namespaces)
            if bfe_elem is not None and bfe_elem.text:
                result['bfe_number'] = int(bfe_elem.text)

            # Parse geometry
            geom_elem = feature_elem.find('.//mat:geometri/gml:MultiSurface', self.namespaces)
            if geom_elem is not None:
                geometry_wkt = self._parse_geometry(geom_elem)
                if geometry_wkt:
                    result['geometry'] = geometry_wkt

            return result if result.get('bfe_number') and result.get('geometry') else None

        except Exception as e:
            logger.error(f"Error parsing feature: {str(e)}")
            return None

    async def test_fetch_features(self):
        """Test fetching and parsing features"""
        async with aiohttp.ClientSession() as session:
            params = self._get_params()
            try:
                async with session.get(self.config['url'], params=params) as response:
                    if response.status != 200:
                        logger.error(f"Failed to fetch features. Status: {response.status}")
                        return None

                    content = await response.text()
                    root = ET.fromstring(content)
                    
                    features = []
                    for feature_elem in root.findall('.//mat:SamletFastEjendom_Gaeldende', self.namespaces):
                        feature = self._parse_feature(feature_elem)
                        if feature:
                            features.append(feature)
                    
                    return features

            except Exception as e:
                logger.error(f"Error fetching features: {str(e)}")
                return None

    async def fetch(self):
        """Implement fetch method"""
        return await self.test_fetch_features()

    async def sync(self, client):
        """Implement sync method"""
        return await self.test_fetch_features()

async def main():
    """Run test"""
    try:
        logger.info("Starting cadastral feature test...")
        cadastral = TestCadastral(SOURCES["cadastral"])
        
        features = await cadastral.test_fetch_features()
        if features:
            logger.info(f"Successfully fetched {len(features)} features")
            # Print first feature as example
            if features:
                logger.info("Example of first feature:")
                for key, value in features[0].items():
                    if key == 'geometry':
                        logger.info(f"  geometry: [WKT string of length {len(value)}]")
                    else:
                        logger.info(f"  {key}: {value}")
            return True
        else:
            logger.error("No features fetched")
            return False
            
    except Exception as e:
        logger.error(f"Test failed: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

================
File: tests/test_db_connection.py
================
import asyncio
import logging
import os
from pathlib import Path
import sys
from dotenv import load_dotenv
import asyncpg

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_db_connection():
    """Test database connection"""
    load_dotenv()
    
    # Get database configuration
    db_config = {
        'host': 'localhost',
        'database': 'landbrugsdata',
        'user': 'landbrugsdata',
        'password': os.getenv('DB_PASSWORD'),
        'port': '5432'
    }
    
    logger.info("Attempting database connection with config:")
    for key, value in db_config.items():
        if key != 'password':
            logger.info(f"  {key}: {value}")
    
    try:
        conn = await asyncpg.connect(
            host=db_config['host'],
            database=db_config['database'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )
        
        # Test basic query
        version = await conn.fetchval('SELECT version()')
        logger.info(f"Database version: {version}")
        
        # Test PostGIS
        postgis_version = await conn.fetchval('SELECT PostGIS_Version()')
        logger.info(f"PostGIS version: {postgis_version}")
        
        await conn.close()
        logger.info("Database connection closed")
        return True
        
    except Exception as e:
        logger.error(f"Database connection failed: {str(e)}", exc_info=True)
        return False

async def main():
    """Run database connection test"""
    try:
        logger.info("Starting database connection test...")
        success = await test_db_connection()
        logger.info(f"Database test {'succeeded' if success else 'failed'}")
        return success
    except Exception as e:
        logger.error(f"Test failed: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

================
File: tests/test_herd_data.py
================
import os
import logging
from src.sources.parsers.herd_data import HerdDataParser
from collections import defaultdict
import pandas as pd
from pyproj import Transformer

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logging.getLogger('zeep').setLevel(logging.INFO)

def convert_utm32_to_wgs84(x: float, y: float) -> tuple[float, float]:
    """Convert UTM32 coordinates to WGS84 (latitude, longitude)"""
    if not (x and y):
        return None, None
        
    try:
        # Valid range for Denmark in UTM32
        if not (400000 <= x <= 900000 and 6000000 <= y <= 6500000):
            return None, None
            
        transformer = Transformer.from_crs("EPSG:25832", "EPSG:4326")
        lat, lon = transformer.transform(x, y)
        
        # Validate converted coordinates (Denmark bounds)
        if not (54 <= lat <= 58 and 8 <= lon <= 13):
            return None, None
            
        return lat, lon
    except Exception as e:
        logging.error(f"Error converting coordinates: {e}")
        return None, None

def should_exclude_total(type_str: str) -> bool:
    """Check if this is a total/sum row that should be excluded"""
    lower_type = type_str.lower()
    return any(total in lower_type for total in ['i alt', 'total', 'sum'])

def test_herd_data():
    """Test fetching and processing herd data"""
    # Configuration for the parser
    config = {
        'wsdl_urls': HerdDataParser.WSDL_URLS,
        'bucket': 'landbrugsdata-raw-data'
    }

    # Initialize parser
    parser = HerdDataParser(config)
    
    # Test different species types
    test_combinations = [
        {'species': 11, 'usage': 11},  # Cattle, production herd
        {'species': 15, 'usage': 11},  # Pigs, production herd
    ]
    
    # Data containers for each table
    herds_data = []
    properties_data = []
    owners_data = []
    users_data = []
    practices_data = []
    
    # Track unique entities
    seen_properties = set()
    seen_owners = set()
    seen_users = set()
    seen_practices = set()
    
    for combo in test_combinations:
        species_code = combo['species']
        usage_code = combo['usage']
        
        # Get just 2 herds for this combination
        herds = parser.get_herds_for_combination(species_code, usage_code, limit=2)
        assert herds, f"No herds found for species {species_code}, usage {usage_code}"
        
        for herd_number in herds:
            details = parser.get_herd_details(herd_number, species_code)
            assert details, f"No details found for herd {herd_number}"
            
            chr_number = details.get('chr_number')
            assert chr_number, f"No CHR number found for herd {herd_number}"
            
            # Check herd sizes
            if 'herd_sizes' in details and details['herd_sizes']:
                for size in details['herd_sizes']:
                    if not should_exclude_total(size['type']):  # Skip total/sum rows
                        herd_row = {
                            # Primary key
                            'herd_number': details.get('herd_number'),
                            'chr_number': chr_number,
                            
                            # Herd details
                            'species_code': details.get('species_code'),
                            'species_text': details.get('species_text'),
                            'usage_code': details.get('usage_code'),
                            'usage_text': details.get('usage_text'),
                            'business_type_code': details.get('business_type_code'),
                            'business_type': details.get('business_type'),
                            'herd_type_code': details.get('herd_type_code'),
                            'herd_type': details.get('herd_type'),
                            'trade_code': details.get('trade_code'),
                            'trade_text': details.get('trade_text'),
                            'is_organic': details.get('is_organic'),
                            'herd_status': details.get('herd_status'),
                            
                            # Dates
                            'creation_date': details.get('creation_date'),
                            'last_update': details.get('last_update'),
                            'end_date': details.get('end_date'),
                            'last_size_update': details.get('last_size_update'),
                            'fetched_at': details.get('fetched_at'),
                            
                            # Animal counts
                            'animal_type': size['type'],
                            'count': size['size']
                        }
                        herds_data.append(herd_row)
            
            # Check property details
            if chr_number not in seen_properties:
                # Collect CHR numbers for batch processing
                chr_numbers_batch = []
                for herd_num in herds:
                    herd_details = parser.get_herd_details(herd_num, species_code)
                    if herd_details and herd_details.get('chr_number'):
                        chr_num = herd_details.get('chr_number')
                        if chr_num not in seen_properties:
                            chr_numbers_batch.append(chr_num)
                            seen_properties.add(chr_num)
                
                # Process batch of CHR numbers
                if chr_numbers_batch:
                    property_details_list = parser.get_property_details_batch(chr_numbers_batch, None)
                    assert property_details_list, f"No property details found for CHR numbers {chr_numbers_batch}"
                    
                    for property_details in property_details_list:
                        # Convert coordinates if present
                        if property_details.get('coordinates'):
                            coords = property_details['coordinates']
                            if coords:
                                lat = coords.get('latitude')
                                lon = coords.get('longitude')
                                if lat and lon:
                                    property_details['latitude'] = lat
                                    property_details['longitude'] = lon
                        
                        properties_data.append(property_details)
            
            # Check owner info
            if 'owner' in details and details['owner']:
                owner = details['owner']
                owner_id = owner.get('cvr_number') or owner.get('cpr_number')
                assert owner_id, f"No owner ID found for CHR {chr_number}"
                
                owner_key = f"{owner_id}_{chr_number}"
                if owner_key not in seen_owners:
                    owners_data.append({
                        # Primary keys
                        'cvr_number': owner.get('cvr_number'),
                        'cpr_number': owner.get('cpr_number'),
                        'chr_number': chr_number,  # Link back to property
                        
                        # Contact info
                        'name': owner.get('name'),
                        'address': owner.get('address'),
                        'city': owner.get('city'),
                        'postal_code': owner.get('postal_code'),
                        'postal_district': owner.get('postal_district'),
                        'municipality_code': owner.get('municipality_code'),
                        'municipality_name': owner.get('municipality_name'),
                        'country': owner.get('country'),
                        'phone': owner.get('phone'),
                        'mobile': owner.get('mobile'),
                        'email': owner.get('email'),
                        
                        # Privacy flags
                        'address_protection': owner.get('address_protection', 0),
                        'advertising_protection': owner.get('advertising_protection', 0)
                    })
                    seen_owners.add(owner_key)
            
            # Check user info
            if 'user' in details and details['user']:
                user = details['user']
                user_id = user.get('cvr_number') or user.get('cpr_number')
                assert user_id, f"No user ID found for CHR {chr_number}"
                
                user_key = f"{user_id}_{chr_number}"
                if user_key not in seen_users:
                    users_data.append({
                        # Primary keys
                        'cvr_number': user.get('cvr_number'),
                        'cpr_number': user.get('cpr_number'),
                        'chr_number': chr_number,  # Link back to property
                        
                        # Contact info
                        'name': user.get('name'),
                        'address': user.get('address'),
                        'city': user.get('city'),
                        'postal_code': user.get('postal_code'),
                        'postal_district': user.get('postal_district'),
                        'municipality_code': user.get('municipality_code'),
                        'municipality_name': user.get('municipality_name'),
                        'country': user.get('country'),
                        'phone': user.get('phone'),
                        'mobile': user.get('mobile'),
                        'email': user.get('email'),
                        
                        # Privacy flags
                        'address_protection': user.get('address_protection', 0),
                        'advertising_protection': user.get('advertising_protection', 0)
                    })
                    seen_users.add(user_key)
            
            # Check veterinary practice info
            if 'veterinary_practice' in details and details['veterinary_practice']:
                practice = details['veterinary_practice']
                practice_id = practice.get('number')
                assert practice_id, f"No practice ID found for CHR {chr_number}"
                
                practice_key = f"{practice_id}_{chr_number}"
                if practice_key not in seen_practices:
                    practices_data.append({
                        # Primary key
                        'number': practice_id,
                        'chr_number': chr_number,  # Link back to property
                        
                        # Practice details
                        'name': practice.get('name'),
                        'address': practice.get('address'),
                        'city': practice.get('city'),
                        'postal_code': practice.get('postal_code'),
                        'postal_district': practice.get('postal_district'),
                        'phone': practice.get('phone'),
                        'mobile': practice.get('mobile'),
                        'email': practice.get('email')
                    })
                    seen_practices.add(practice_key)
    
    # Final assertions
    assert herds_data, "No herd data collected"
    assert properties_data, "No property data collected"
    assert owners_data, "No owner data collected"
    assert users_data, "No user data collected"
    assert practices_data, "No practice data collected"
    
    # Print summary
    print("\nData collected:")
    print(f"- Herds: {len(herds_data)}")
    print(f"- Properties: {len(properties_data)}")
    print(f"- Owners: {len(owners_data)}")
    print(f"- Users: {len(users_data)}")
    print(f"- Practices: {len(practices_data)}")

================
File: tests/test_water_projects_parser.py
================
import asyncio
import aiohttp
import logging
import sys
from pathlib import Path
import xml.etree.ElementTree as ET

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.water_projects import WaterProjects
from src.config import SOURCES

# Configure detailed logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_single_layer(parser, layer):
    """Test fetching and parsing a single layer"""
    async with aiohttp.ClientSession(headers=parser.headers) as session:
        params = parser._get_params(layer, 0)
        logger.info(f"\nTesting layer: {layer}")
        logger.debug(f"Request params: {params}")
        
        try:
            async with session.get(parser.config['url'], params=params) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch {layer}. Status: {response.status}")
                    error_text = await response.text()
                    logger.error(f"Error response: {error_text[:500]}")
                    return
                
                text = await response.text()
                logger.debug(f"Response preview: {text[:200]}...")
                
                try:
                    root = ET.fromstring(text)
                    total_features = int(root.get('numberMatched', '0'))
                    logger.info(f"Total features: {total_features}")
                    
                    # Test parsing first feature
                    for member in root.findall('.//{*}member'):
                        for feature in member:
                            logger.debug(f"Raw feature XML: {ET.tostring(feature, encoding='unicode')}")
                            parsed = parser._parse_feature(feature, layer)
                            if parsed:
                                logger.info("Successfully parsed feature:")
                                for key, value in parsed.items():
                                    logger.info(f"  {key}: {value}")
                            else:
                                logger.error("Failed to parse feature")
                            break  # Only test first feature
                        break  # Only test first member
                    
                except ET.ParseError as e:
                    logger.error(f"XML parsing error: {str(e)}")
                    logger.debug(f"Problematic XML: {text[:500]}")
                
        except Exception as e:
            logger.error(f"Error testing layer {layer}: {str(e)}", exc_info=True)

async def main():
    """Test the water projects parser"""
    parser = WaterProjects(SOURCES["water_projects"])
    
    # Test each layer
    for layer in parser.layers:
        await test_single_layer(parser, layer)

if __name__ == "__main__":
    asyncio.run(main())

================
File: tests/test_water_projects.py
================
import asyncio
import aiohttp
import xml.etree.ElementTree as ET
import logging
from pathlib import Path
import sys
from collections import defaultdict
from pprint import pformat

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.append(str(backend_dir))

from src.sources.parsers.water_projects import WaterProjects
from src.config import SOURCES

async def analyze_layer_fields(session, parser, layer_name):
    """Analyze fields and their values for a given layer"""
    try:
        params = parser._get_params(layer_name, 0)
        url = parser.url_mapping.get(layer_name, parser.config['url'])
        
        field_values = defaultdict(set)
        
        async with session.get(url, params=params) as response:
            if response.status != 200:
                logger.error(f"Failed to fetch data. Status: {response.status}")
                return
            
            text = await response.text()
            root = ET.fromstring(text)
            
            # Process first 5 features
            count = 0
            for member in root.findall('.//{*}member'):
                for feature in member:
                    if count >= 5:
                        break
                        
                    # Get all fields except geometry
                    for elem in feature:
                        if not any(geom_name in elem.tag for geom_name in ['the_geom', 'wkb_geometry']):
                            field_name = elem.tag.split('}')[-1]
                            if elem.text:
                                field_values[field_name].add(elem.text)
                    count += 1
                if count >= 5:
                    break
                    
        logger.info(f"\nLayer: {layer_name}")
        logger.info("Fields and sample values:")
        for field, values in field_values.items():
            logger.info(f"\n{field}:")
            for val in values:
                logger.info(f"  - {val} (type: {type(val).__name__})")

    except Exception as e:
        logger.error(f"Error analyzing layer {layer_name}: {str(e)}", exc_info=True)

async def test_all_layers():
    """Test and analyze all water project layers"""
    config = SOURCES['water_projects']
    
    test_layers = [
        'N2000_projekter:Hydrologi_E',
        'N2000_projekter:Hydrologi_F',
        'Ovrige_projekter:Vandloebsrestaurering_E',
        'Ovrige_projekter:Vandloebsrestaurering_F',
        'Vandprojekter:Fosfor_E_samlet',
        'Vandprojekter:Fosfor_F_samlet',
        'Vandprojekter:Lavbund_E_samlet',
        'Vandprojekter:Lavbund_F_samlet',
        'Vandprojekter:Private_vaadomraader',
        'vandprojekter:kla_projektforslag',
        'vandprojekter:kla_projektomraader'
    ]
    
    config['layer_names'] = test_layers
    parser = WaterProjects(config)
    
    async with aiohttp.ClientSession() as session:
        for layer in test_layers:
            logger.info("\n" + "="*80)
            await analyze_layer_fields(session, parser, layer)

if __name__ == "__main__":
    asyncio.run(test_all_layers())

================
File: tests/test_xml_structure.py
================
import asyncio
import aiohttp
import xml.etree.ElementTree as ET
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Original layers from geodata.fvm.dk
FVM_LAYERS = [
    "N2000_projekter:Hydrologi_E",
    "N2000_projekter:Hydrologi_F",
    "Ovrige_projekter:Vandloebsrestaurering_E",
    "Ovrige_projekter:Vandloebsrestaurering_F",
    "Vandprojekter:Fosfor_E_samlet",
    "Vandprojekter:Fosfor_F_samlet",
    "Vandprojekter:Kvaelstof_E_samlet",
    "Vandprojekter:Kvaelstof_F_samlet",
    "Vandprojekter:Lavbund_E_samlet",
    "Vandprojekter:Lavbund_F_samlet",
    "Vandprojekter:Private_vaadomraader",
    "Vandprojekter:Restaurering_af_aadale_2024"
]

# New layers with their full URLs
NEW_URLS = [
    {
        'url': 'https://wfs2-miljoegis.mim.dk/vandprojekter/wfs',
        'layer': 'vandprojekter:kla_projektforslag'
    },
    {
        'url': 'https://wfs2-miljoegis.mim.dk/vandprojekter/wfs',
        'layer': 'vandprojekter:kla_projektomraader'
    }
]

async def analyze_feature(session, layer, base_url="https://geodata.fvm.dk/geoserver/ows"):
    """Fetch and analyze a single feature from a layer"""
    params = {
        'SERVICE': 'WFS',
        'REQUEST': 'GetFeature',
        'VERSION': '2.0.0',
        'TYPENAMES': layer,
        'STARTINDEX': '0',
        'COUNT': '1',
        'SRSNAME': 'urn:ogc:def:crs:EPSG::25832'
    }
    
    try:
        async with session.get(base_url, params=params) as response:
            if response.status != 200:
                print(f"Error: Status {response.status} for {layer}")
                return
                
            text = await response.text()
            root = ET.fromstring(text)
            
            print(f"\n{'='*50}")
            print(f"Layer: {layer}")
            print(f"{'='*50}\n")
            
            # Find the first feature
            feature = None
            for elem in root.iter():
                if elem.tag.split('}')[-1] == layer.split(':')[-1]:
                    feature = elem
                    break
            
            if feature is not None:
                print("=== Feature Tag ===")
                print(f"Full tag: {feature.tag}")
                print(f"Local name: {feature.tag.split('}')[-1]}")
                
                print("\n=== Namespaces Used ===")
                if '}' in feature.tag:
                    ns_url = feature.tag.split('}')[0].strip('{')
                    print(f"Feature namespace: {ns_url}")
                
                print("\n=== Direct Children ===")
                for child in feature:
                    print(f"Full tag: {child.tag}")
                    print(f"Local name: {child.tag.split('}')[-1]}")
                    if child.tag.split('}')[-1].lower() == 'the_geom':
                        print("*** This is the geometry element ***")
                    print(f"Text length: {len(child.text) if child.text else 0}\n")
            else:
                print("No feature found!")
                print("Raw XML response:")
                print(text[:500] + "..." if len(text) > 500 else text)
            
    except Exception as e:
        logger.error(f"Error analyzing layer {layer}: {str(e)}", exc_info=True)

async def main():
    """Test all layers"""
    async with aiohttp.ClientSession() as session:
        # Test original layers
        for layer in FVM_LAYERS:
            await analyze_feature(session, layer)
            
        # Test new layers
        for item in NEW_URLS:
            await analyze_feature(session, item['layer'], item['url'])

if __name__ == "__main__":
    asyncio.run(main())

================
File: .dockerignore
================
__pycache__
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
.env

# Tests
tests/
test_*.py
.coverage
htmlcov/
pytest_cache/
.pytest_cache/

# Logs and debugging
*.log
debug/
logs/

# Version control
.git/
.gitignore

# Development tools
cloud-sql-proxy
test_wfs.py

================
File: cloudbuild.yaml
================
steps:
  # Pull the previous images for layer caching
  - name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args: 
      - '-c'
      - 'docker pull gcr.io/${PROJECT_ID}/${_SERVICE_NAME}:latest || exit 0'

  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build',
      '-t', 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}',
      '-f', '${_DOCKERFILE}',
      '--cache-from', 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}:latest',
      '--build-arg', 'BUILDKIT_INLINE_CACHE=1',
      '.'
    ]
    env:
      - 'DOCKER_BUILDKIT=1'

  - name: 'gcr.io/cloud-builders/docker'
    args: ['tag', 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}', 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}:latest']

images: 
  - 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}'
  - 'gcr.io/${PROJECT_ID}/${_SERVICE_NAME}:latest'

substitutions:
  _SERVICE_NAME: data-sync
  _DOCKERFILE: Dockerfile.sync

options:
  machineType: 'E2_HIGHCPU_8'
  dynamic_substitutions: true

================
File: conftest.py
================
import os
import sys
from pathlib import Path

# Add the project root directory to PYTHONPATH
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

================
File: Dockerfile
================
# Build stage
FROM ghcr.io/osgeo/gdal:ubuntu-small-3.10.0 as builder

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    python3-venv \
    python3-setuptools \
    gcc \
    g++ \
    procps \
    libgdal-dev \
    && rm -rf /var/lib/apt/lists/*

# Create and use virtual environment
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Set GDAL environment variables
ENV GDAL_VERSION=3.10.0
ENV CPLUS_INCLUDE_PATH=/usr/include/gdal
ENV C_INCLUDE_PATH=/usr/include/gdal

# Install dependencies in order of change frequency
WORKDIR /install

# 1. Install build tools (rarely changes)
RUN pip install --no-cache-dir setuptools wheel

# 2. Install core dependencies (changes occasionally)
RUN pip install --no-cache-dir \
    numpy>=1.26.4 \
    pandas>=2.2.0 \
    shapely>=2.0.0 \
    geopandas>=0.14.0 \
    pyproj>=3.0.0 \
    fiona>=1.9.0

# 3. Install API dependencies (changes occasionally)
RUN pip install --no-cache-dir \
    fastapi>=0.100.0 \
    uvicorn>=0.15.0 \
    pydantic>=2.0.0

# 4. Copy requirements and install remaining deps (changes most frequently)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Final stage
FROM ghcr.io/osgeo/gdal:ubuntu-small-3.10.0

RUN apt-get update && apt-get install -y \
    python3 \
    python3-venv \
    procps \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

WORKDIR /app
COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]

================
File: Dockerfile.processing
================
# backend/Dockerfile.processing
FROM apache/beam_python3.11_sdk:2.60.0

# Install system dependencies including GDAL
RUN apt-get update && apt-get install -y \
    gdal-bin \
    libgdal-dev \
    build-essential \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

# Set GDAL environment variables
ENV CPLUS_INCLUDE_PATH=/usr/include/gdal
ENV C_INCLUDE_PATH=/usr/include/gdal

# Copy requirements first for better caching
COPY requirements.txt /app/requirements.txt
RUN pip install -r /app/requirements.txt

# Install additional dependencies needed for validation
RUN pip install \
    geopandas \
    pandas \
    shapely \
    fsspec>=2024.10.0 \
    gcsfs>=2024.10.0 \
    pyarrow

# Copy the dataflow directory (which includes setup.py)
COPY dataflow /app/dataflow

# Install the package in development mode
WORKDIR /app/dataflow
RUN pip install -e .

# Set the working directory
WORKDIR /app

================
File: Dockerfile.sync
================
# Build stage
FROM ghcr.io/osgeo/gdal:ubuntu-small-3.10.0 as builder

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    python3-venv \
    python3-setuptools \
    gcc \
    g++ \
    procps \
    libgdal-dev \
    && rm -rf /var/lib/apt/lists/*

# Create and use virtual environment
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Set GDAL environment variables
ENV GDAL_VERSION=3.10.0
ENV CPLUS_INCLUDE_PATH=/usr/include/gdal
ENV C_INCLUDE_PATH=/usr/include/gdal

# Install dependencies in order of change frequency
WORKDIR /install

# 1. Install build tools (rarely changes)
RUN pip install --no-cache-dir setuptools wheel

# 2. Install core dependencies (changes occasionally)
RUN pip install --no-cache-dir \
    numpy>=1.26.4 \
    pandas>=2.2.0 \
    shapely>=2.0.0 \
    geopandas>=0.14.0 \
    pyproj>=3.0.0 \
    fiona>=1.9.0 \
    dask>=2024.1.0 \
    dask-geopandas>=0.3.1

# 3. Install sync-specific dependencies (changes occasionally)
RUN pip install --no-cache-dir \
    tqdm>=4.62.0 \
    psutil==5.9.7 \
    pyarrow>=12.0.0

# 4. Copy requirements and install remaining deps (changes most frequently)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Final stage
FROM ghcr.io/osgeo/gdal:ubuntu-small-3.10.0

RUN apt-get update && apt-get install -y \
    python3 \
    python3-venv \
    procps \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

WORKDIR /app
COPY scripts ./scripts
COPY src ./src
COPY sync_app.py .

ENTRYPOINT ["python", "sync_app.py"]

================
File: pyproject.toml
================


================
File: README.md
================
# Agricultural Data API

Backend service for Danish agricultural and environmental data.

## Architecture

.
 backend/
    src/
        sources/
           base.py
           parsers/          # API/WFS sources
              agricultural_fields/
           static/           # Static file sources
               animal_welfare/
               biogas/
               fertilizer/
               herd_data/
               pesticides/
               pig_movements/
               subsidies/
               visa/
               wetlands/
        main.py
        config.py
 frontend/
     src/
         components/
         api/
        
## API Endpoints

- `GET /health` - Health check
- `GET /sources` - List available data sources
- `GET /sources/{source_id}` - Get data for specific source

## Development Setup

### Requirements
- Python 3.9+
- GDAL library
- Virtual environment

### Installation Steps
1. Create and activate virtual environment
2. Install dependencies: `pip install -r requirements.txt`
3. Run development server: `uvicorn src.main:app --reload`

## Adding New Data Sources


1. Choose location:
   - `sources/parsers/` for API/WFS sources
   - `sources/static/` for static file sources

2. Implement your parser:

from ...base import Source
class YourSource(Source):
async def fetch(self) -> pd.DataFrame:
# Implement data fetching
pass

3. Add to `config.py`:
python
SOURCES = {
"your_source": {
"name": "Your Source Name",
"type": "wfs", # or "static"
"enabled": True,
# Add source-specific config
}
}
## Environment Variables
Required in `.env`:
- GOOGLE_CLOUD_PROJECT: Your GCP project ID
- GCS_BUCKET: Your GCS bucket name

## Deployment
Automatic deployment to Google Cloud Run:
- On push to main branch
- Weekly on Mondays at 2 AM UTC

================
File: requirements.txt
================
# Core Data Processing
pandas~=2.2.3
numpy~=2.2.2
pyarrow<17.0.0,>=3.0.0

# Geospatial Processing
geopandas~=1.0.1
shapely~=2.0.6
pyproj~=3.7.0
fiona~=1.10.1

# Cloud Services
google-cloud-storage~=2.19.0
google-cloud-bigquery~=3.29.0
google-cloud-secret-manager~=2.22.1
fsspec~=2024.12.0
gcsfs~=2024.12.0

# Web Framework
fastapi~=0.115.7
uvicorn~=0.34.0
pydantic~=2.10.6

# HTTP and API Clients
aiohttp~=3.11.11
aiohttp[speedups]~=3.11.11
requests~=2.32.3
zeep~=4.3.1
paramiko~=3.5.0
certifi~=2024.12.14
pyOpenSSL~=25.0.0

# Data Pipeline and Validation
apache-beam~=2.62.0
backoff~=2.2.1

# PDF Processing
pdfplumber~=0.11.5

# Utilities
python-dotenv~=1.0.1
tqdm~=4.67.1
psutil~=6.1.1
cryptography~=44.0.0
lxml~=5.3.0
ijson~=3.3.0

================
File: sync_app.py
================
import os
import asyncio
import logging
import signal
import sys
from src.sources.parsers import get_source_handler
from src.config import SOURCES

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global flag for graceful shutdown
shutdown = asyncio.Event()

def handle_shutdown(signum, frame):
    """Handle shutdown signals gracefully"""
    sig_name = signal.Signals(signum).name
    logger.info(f"Received {sig_name}. Starting graceful shutdown...")
    shutdown.set()

# Register signal handlers
signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)

async def run_sync() -> bool:
    """Run the sync process based on environment variable"""
    sync_type = os.getenv('SYNC_TYPE', 'all')
    logger.info(f"Starting sync process for: {sync_type}")
    
    try:
        if sync_type == 'all':
            success = True
            for source_id, config in SOURCES.items():
                if shutdown.is_set():
                    logger.info("Shutdown requested, stopping sync process")
                    return False
                    
                if config['enabled']:
                    source = get_source_handler(source_id, config)
                    if source:
                        try:
                            total_synced = await source.sync()
                            if total_synced is not None:
                                logger.info(f"{source_id} sync completed. Total records: {total_synced:,}")
                            else:
                                logger.error(f"{source_id} sync failed")
                                success = False
                        except Exception as e:
                            logger.error(f"Error syncing {source_id}: {str(e)}", exc_info=True)
                            success = False
            return success
        else:
            if sync_type not in SOURCES:
                logger.error(f"Unknown sync type: {sync_type}")
                return False
                
            config = SOURCES[sync_type]
            if not config['enabled']:
                logger.error(f"Sync type {sync_type} is disabled")
                return False
                
            source = get_source_handler(sync_type, config)
            if not source:
                logger.error(f"No handler for sync type: {sync_type}")
                return False
                
            total_synced = await source.sync()
            if total_synced is not None:
                logger.info(f"{sync_type} sync completed. Total records: {total_synced:,}")
                return True
            logger.error(f"{sync_type} sync failed")
            return False
        
    except Exception as e:
        logger.error(f"Error during sync: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    try:
        success = asyncio.run(run_sync())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt. Shutting down...")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unhandled exception: {str(e)}", exc_info=True)
        sys.exit(1)



================================================================
End of Codebase
================================================================
